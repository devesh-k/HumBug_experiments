{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b7bacb",
   "metadata": {},
   "source": [
    "### New install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e005e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyterlab in /opt/conda/lib/python3.8/site-packages (2.3.2)\n",
      "Collecting jupyterlab\n",
      "  Downloading jupyterlab-4.0.8-py3-none-any.whl (9.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2 MB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (6.6.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (21.3)\n",
      "Collecting jupyter-server<3,>=2.4.0\n",
      "  Downloading jupyter_server-2.10.0-py3-none-any.whl (377 kB)\n",
      "\u001b[K     |████████████████████████████████| 377 kB 120.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (5.1.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (5.4.0)\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Downloading jupyter_lsp-2.2.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 103.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-server<3,>=2.19.0\n",
      "  Downloading jupyterlab_server-2.25.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 77.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jupyter-core in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (4.9.1)\n",
      "Collecting tornado>=6.2.0\n",
      "  Downloading tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
      "\u001b[K     |████████████████████████████████| 427 kB 106.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata>=4.8.3\n",
      "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (3.0.3)\n",
      "Requirement already satisfied: tomli in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (1.2.2)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Collecting notebook-shim>=0.2\n",
      "  Downloading notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from async-lru>=1.0.0->jupyterlab) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyterlab) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=3.0.3->jupyterlab) (2.0.1)\n",
      "Collecting jupyter-core\n",
      "  Downloading jupyter_core-5.5.0-py3-none-any.whl (28 kB)\n",
      "Collecting anyio>=3.1.0\n",
      "  Downloading anyio-4.0.0-py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 91.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nbconvert>=6.4.4\n",
      "  Downloading nbconvert-7.11.0-py3-none-any.whl (256 kB)\n",
      "\u001b[K     |████████████████████████████████| 256 kB 155.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyzmq>=24\n",
      "  Downloading pyzmq-25.1.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 102.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting send2trash>=1.8.2\n",
      "  Downloading Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Collecting traitlets\n",
      "  Downloading traitlets-5.13.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 104.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-server-terminals\n",
      "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.12.0)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Downloading nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 115.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting websocket-client\n",
      "  Downloading websocket_client-1.6.4-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 109.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.12.1)\n",
      "Collecting jupyter-client>=7.4.4\n",
      "  Downloading jupyter_client-8.6.0-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 130.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-events>=0.6.0\n",
      "  Downloading jupyter_events-0.9.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (21.1.0)\n",
      "Collecting overrides\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.8.2)\n",
      "Collecting platformdirs>=2.5\n",
      "  Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Collecting referencing\n",
      "  Downloading referencing-0.30.2-py3-none-any.whl (25 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting jsonschema[format-nongpl]>=4.18.0\n",
      "  Downloading jsonschema-4.19.2-py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 103.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 151.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=22.2.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 111.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pkgutil-resolve-name>=1.3.10\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Collecting isoduration\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting jsonpointer>1.13\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting webcolors>=1.11\n",
      "  Downloading webcolors-1.13-py3-none-any.whl (14 kB)\n",
      "Collecting uri-template\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting fqdn\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests>=2.31\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 85.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (0.9.6)\n",
      "Collecting babel>=2.10\n",
      "  Downloading Babel-2.13.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 141.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2015.7 in /opt/conda/lib/python3.8/site-packages (from babel>=2.10->jupyterlab-server<3,>=2.19.0->jupyterlab) (2021.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.10.0)\n",
      "Collecting tinycss2\n",
      "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.10.0)\n",
      "Collecting mistune<4,>=2.0.3\n",
      "  Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 82.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.0)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.1.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.9)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.16.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from nbclient>=0.5.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.4)\n",
      "Collecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.18.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->jupyterlab) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab) (2.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab) (2021.10.8)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.8/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (2.21)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.3)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyterlab) (0.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyterlab) (1.5.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyterlab) (7.30.0)\n",
      "Collecting jupyter-client>=7.4.4\n",
      "  Downloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 142.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (3.0.22)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (59.4.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyterlab) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyterlab) (0.2.5)\n",
      "Collecting arrow>=0.15.0\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 107.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting types-python-dateutil>=2.8.10\n",
      "  Downloading types_python_dateutil-2.8.19.14-py3-none-any.whl (9.4 kB)\n",
      "Installing collected packages: rpds-py, attrs, referencing, types-python-dateutil, traitlets, platformdirs, pkgutil-resolve-name, jsonschema-specifications, tornado, pyzmq, jupyter-core, jsonschema, fastjsonschema, arrow, webcolors, uri-template, rfc3986-validator, rfc3339-validator, nbformat, jupyter-client, jsonpointer, isoduration, fqdn, tinycss2, sniffio, python-json-logger, mistune, importlib-metadata, exceptiongroup, websocket-client, send2trash, overrides, nbconvert, jupyter-server-terminals, jupyter-events, anyio, requests, jupyter-server, babel, notebook-shim, jupyterlab-server, jupyter-lsp, async-lru, jupyterlab\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.1.1\n",
      "    Uninstalling traitlets-5.1.1:\n",
      "      Successfully uninstalled traitlets-5.1.1\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.1\n",
      "    Uninstalling tornado-6.1:\n",
      "      Successfully uninstalled tornado-6.1\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 22.3.0\n",
      "    Uninstalling pyzmq-22.3.0:\n",
      "      Successfully uninstalled pyzmq-22.3.0\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter-core 4.9.1\n",
      "    Uninstalling jupyter-core-4.9.1:\n",
      "      Successfully uninstalled jupyter-core-4.9.1\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.2.1\n",
      "    Uninstalling jsonschema-4.2.1:\n",
      "      Successfully uninstalled jsonschema-4.2.1\n",
      "  Attempting uninstall: nbformat\n",
      "    Found existing installation: nbformat 5.1.3\n",
      "    Uninstalling nbformat-5.1.3:\n",
      "      Successfully uninstalled nbformat-5.1.3\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter-client 7.1.0\n",
      "    Uninstalling jupyter-client-7.1.0:\n",
      "      Successfully uninstalled jupyter-client-7.1.0\n",
      "  Attempting uninstall: mistune\n",
      "    Found existing installation: mistune 0.8.4\n",
      "    Uninstalling mistune-0.8.4:\n",
      "      Successfully uninstalled mistune-0.8.4\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.8.2\n",
      "    Uninstalling importlib-metadata-4.8.2:\n",
      "      Successfully uninstalled importlib-metadata-4.8.2\n",
      "  Attempting uninstall: send2trash\n",
      "    Found existing installation: Send2Trash 1.8.0\n",
      "    Uninstalling Send2Trash-1.8.0:\n",
      "      Successfully uninstalled Send2Trash-1.8.0\n",
      "  Attempting uninstall: nbconvert\n",
      "    Found existing installation: nbconvert 6.3.0\n",
      "    Uninstalling nbconvert-6.3.0:\n",
      "      Successfully uninstalled nbconvert-6.3.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "  Attempting uninstall: babel\n",
      "    Found existing installation: Babel 2.9.1\n",
      "    Uninstalling Babel-2.9.1:\n",
      "      Successfully uninstalled Babel-2.9.1\n",
      "  Attempting uninstall: jupyterlab-server\n",
      "    Found existing installation: jupyterlab-server 1.2.0\n",
      "    Uninstalling jupyterlab-server-1.2.0:\n",
      "      Successfully uninstalled jupyterlab-server-1.2.0\n",
      "  Attempting uninstall: jupyterlab\n",
      "    Found existing installation: jupyterlab 2.3.2\n",
      "    Uninstalling jupyterlab-2.3.2:\n",
      "      Successfully uninstalled jupyterlab-2.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+b6df043, but you have torch 2.1.0 which is incompatible.\n",
      "markdown-it-py 1.1.0 requires attrs<22,>=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\n",
      "Successfully installed anyio-4.0.0 arrow-1.3.0 async-lru-2.0.4 attrs-23.1.0 babel-2.13.1 exceptiongroup-1.1.3 fastjsonschema-2.18.1 fqdn-1.5.1 importlib-metadata-6.8.0 isoduration-20.11.0 jsonpointer-2.4 jsonschema-4.19.2 jsonschema-specifications-2023.7.1 jupyter-client-7.4.9 jupyter-core-5.5.0 jupyter-events-0.9.0 jupyter-lsp-2.2.0 jupyter-server-2.10.0 jupyter-server-terminals-0.4.4 jupyterlab-4.0.8 jupyterlab-server-2.25.1 mistune-3.0.2 nbconvert-7.11.0 nbformat-5.9.2 notebook-shim-0.2.3 overrides-7.4.0 pkgutil-resolve-name-1.3.10 platformdirs-3.11.0 python-json-logger-2.0.7 pyzmq-25.1.1 referencing-0.30.2 requests-2.31.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.12.0 send2trash-1.8.2 sniffio-1.3.0 tinycss2-1.2.1 tornado-6.3.3 traitlets-5.13.0 types-python-dateutil-2.8.19.14 uri-template-1.3.0 webcolors-1.13 websocket-client-1.6.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting jupyter-console\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Collecting qtconsole\n",
      "  Downloading qtconsole-5.5.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (7.11.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.0)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 65.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.3.3)\n",
      "Requirement already satisfied: traitlets<6.0,>=5.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.13.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.4.9)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (0.1.3)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.30.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (2.10.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (59.4.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (3.0.22)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (5.5.0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (25.1.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core>=4.9.2->jupyter-client<8.0->ipykernel->jupyter) (3.11.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyter) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter-client<8.0->ipykernel->jupyter) (1.16.0)\n",
      "Collecting comm>=0.1.3\n",
      "  Downloading comm-0.2.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting widgetsnbextension~=4.0.9\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 140.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-widgets~=3.0.9\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 36.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipykernel\n",
      "  Downloading ipykernel-6.26.0-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 114.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.39-py3-none-any.whl (385 kB)\n",
      "\u001b[K     |████████████████████████████████| 385 kB 104.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Collecting debugpy<2.0,>=1.0.0\n",
      "  Downloading debugpy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 106.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (6.8.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.9.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.10.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=3.6->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert->jupyter) (4.19.2)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.18.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (2023.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (23.1.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (0.12.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (0.30.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.3)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.2)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.2.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Collecting qtpy>=2.4.0\n",
      "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 103.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: prompt-toolkit, debugpy, comm, widgetsnbextension, qtpy, jupyterlab-widgets, ipykernel, qtconsole, jupyter-console, ipywidgets, jupyter\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.22\n",
      "    Uninstalling prompt-toolkit-3.0.22:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.22\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.5.1\n",
      "    Uninstalling debugpy-1.5.1:\n",
      "      Successfully uninstalled debugpy-1.5.1\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.6.0\n",
      "    Uninstalling ipykernel-6.6.0:\n",
      "      Successfully uninstalled ipykernel-6.6.0\n",
      "Successfully installed comm-0.2.0 debugpy-1.8.0 ipykernel-6.26.0 ipywidgets-8.1.1 jupyter-1.0.0 jupyter-console-6.6.3 jupyterlab-widgets-3.0.9 prompt-toolkit-3.0.39 qtconsole-5.5.0 qtpy-2.4.1 widgetsnbextension-4.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.13.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jupyterlab\n",
    "!pip install --upgrade jupyter\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f698717",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3740babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /dli/data/plots\n",
      "Created directory: /dli/data/features/MED\n",
      "Created directory: /dli/models\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as AT\n",
    "import torchvision.transforms as VT\n",
    "from nnAudio import features\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e50d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_prep(model_ft):\n",
    "    model_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n",
    "# Step 3\n",
    "    model_ft = torch.quantization.prepare_qat(model_ft, inplace=True)\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = True\n",
    "    return(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cc7a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models.quantization as models\n",
    "model = models.resnet18(pretrained=True, progress=True, quantize=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.train()\n",
    "model.fuse_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e0657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def create_combined_model(model_fe):\n",
    "    # Step 1. Isolate the feature extractor.\n",
    "    model_fe_features = nn.Sequential(\n",
    "    model_fe.quant,  # Quantize the input\n",
    "    model_fe.conv1,\n",
    "    model_fe.bn1,\n",
    "    model_fe.relu,\n",
    "    model_fe.maxpool,\n",
    "    model_fe.layer1,\n",
    "    model_fe.layer2,\n",
    "    model_fe.layer3,\n",
    "    model_fe.layer4,\n",
    "    model_fe.avgpool,\n",
    "    model_fe.dequant,  # Dequantize the output\n",
    "  )\n",
    "\n",
    "    # Step 2. Create a new \"head\"\n",
    "    new_head = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(num_ftrs, 2),\n",
    "   )\n",
    "\n",
    "  # Step 3. Combine, and don't forget the quant stubs.\n",
    "    new_model = nn.Sequential(\n",
    "    model_fe_features,\n",
    "    nn.Flatten(1),\n",
    "    new_head,)\n",
    "    \n",
    "    model_ft  = quant_prep(new_model)\n",
    "    \n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58531045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7f8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e81c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_file,model = create_combined_model(model)):\n",
    "    model =model \n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9588560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5fe36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a94aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a815c02",
   "metadata": {},
   "source": [
    "## Load the base fp-32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788527fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda71a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02495a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): QuantStub(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7838]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11287054419517517, max_val=99.4275131225586)\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBnReLU2d(\n",
       "      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5303207635879517, max_val=0.6030262112617493)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7911]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=100.46710968017578)\n",
       "      )\n",
       "    )\n",
       "    (2): Identity()\n",
       "    (3): Identity()\n",
       "    (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (5): Sequential(\n",
       "      (0): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4112796187400818, max_val=0.48046109080314636)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3346]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=42.48940658569336)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0061]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7800601124763489, max_val=0.5947202444076538)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1264]), zero_point=tensor([72], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-81.41188049316406, max_val=61.64603805541992)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7353]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=93.37883758544922)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0028]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2955160439014435, max_val=0.35802650451660156)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3909]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=49.64764404296875)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0080]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0192744731903076, max_val=0.7874662280082703)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0924]), zero_point=tensor([81], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-88.6553726196289, max_val=50.079898834228516)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8575]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=108.90803527832031)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0019]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1944296658039093, max_val=0.24689579010009766)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3062]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=38.88494873046875)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0060]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5522620677947998, max_val=0.7629640102386475)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7314]), zero_point=tensor([59], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-43.27009582519531, max_val=49.61182403564453)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059]), zero_point=tensor([0], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7176772356033325, max_val=0.7501586675643921)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6722]), zero_point=tensor([66], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-44.4168701171875, max_val=40.947322845458984)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4122]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=52.34981918334961)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0025]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30802762508392334, max_val=0.3226325809955597)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2312]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=29.361980438232422)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8376755714416504, max_val=0.6912389397621155)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6668]), zero_point=tensor([76], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-50.95977020263672, max_val=33.726200103759766)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5316]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=67.51696014404297)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0022]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20750701427459717, max_val=0.28205040097236633)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2451]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=31.125097274780273)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4269964396953583, max_val=0.5966797471046448)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5350]), zero_point=tensor([52], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-27.77962875366211, max_val=40.16476821899414)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032]), zero_point=tensor([0], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.402071088552475, max_val=0.30672407150268555)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2064]), zero_point=tensor([67], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.84971809387207, max_val=12.368556022644043)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3183]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=40.42247772216797)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0020]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21809768676757812, max_val=0.2558903992176056)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1945]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=24.702301025390625)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9112961292266846, max_val=0.8250895142555237)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4429]), zero_point=tensor([76], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-33.824737548828125, max_val=22.429607391357422)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2523]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=32.04408264160156)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1566593497991562, max_val=0.28749120235443115)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1355]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.209970474243164)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0106]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7447466850280762, max_val=1.357274055480957)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4579]), zero_point=tensor([65], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-29.668500900268555, max_val=28.487041473388672)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0075]), zero_point=tensor([0], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7893481850624084, max_val=0.9507298469543457)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3429]), zero_point=tensor([61], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-20.955944061279297, max_val=22.59812355041504)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2364]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=30.028310775756836)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15437749028205872, max_val=0.2966344952583313)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0417]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.300099849700928)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0389]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9657297134399414, max_val=4.955899715423584)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7547]), zero_point=tensor([65], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-49.16975402832031, max_val=46.6772346496582)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4129]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=52.43907165527344)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (10): DeQuantStub()\n",
       "  )\n",
       "  (1): Flatten(start_dim=1, end_dim=-1)\n",
       "  (2): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "filename = os.path.join(\"models\",\"model_med0_2023_11_08_22_44_10.pth\")\n",
    "\n",
    "#state_dict = torch.load(filename)\n",
    "model_ft_tuned  = load_model(filename)\n",
    "model_ft_tuned.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d9d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d500e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to save the float model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b15882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf6165",
   "metadata": {},
   "source": [
    "## Quantization starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee57440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import convert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a537de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): QuantStub(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7838]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11287054419517517, max_val=99.4275131225586)\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBnReLU2d(\n",
       "      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5303207635879517, max_val=0.6030262112617493)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7911]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=100.46710968017578)\n",
       "      )\n",
       "    )\n",
       "    (2): Identity()\n",
       "    (3): Identity()\n",
       "    (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (5): Sequential(\n",
       "      (0): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4112796187400818, max_val=0.48046109080314636)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3346]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=42.48940658569336)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0061]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7800601124763489, max_val=0.5947202444076538)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1264]), zero_point=tensor([72], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-81.41188049316406, max_val=61.64603805541992)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7353]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=93.37883758544922)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0028]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2955160439014435, max_val=0.35802650451660156)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3909]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=49.64764404296875)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0080]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0192744731903076, max_val=0.7874662280082703)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0924]), zero_point=tensor([81], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-88.6553726196289, max_val=50.079898834228516)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8575]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=108.90803527832031)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0019]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1944296658039093, max_val=0.24689579010009766)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3062]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=38.88494873046875)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0060]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5522620677947998, max_val=0.7629640102386475)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7314]), zero_point=tensor([59], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-43.27009582519531, max_val=49.61182403564453)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059]), zero_point=tensor([0], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7176772356033325, max_val=0.7501586675643921)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6722]), zero_point=tensor([66], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-44.4168701171875, max_val=40.947322845458984)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4122]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=52.34981918334961)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0025]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30802762508392334, max_val=0.3226325809955597)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2312]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=29.361980438232422)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8376755714416504, max_val=0.6912389397621155)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6668]), zero_point=tensor([76], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-50.95977020263672, max_val=33.726200103759766)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5316]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=67.51696014404297)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0022]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20750701427459717, max_val=0.28205040097236633)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2451]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=31.125097274780273)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4269964396953583, max_val=0.5966797471046448)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5350]), zero_point=tensor([52], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-27.77962875366211, max_val=40.16476821899414)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032]), zero_point=tensor([0], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.402071088552475, max_val=0.30672407150268555)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2064]), zero_point=tensor([67], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.84971809387207, max_val=12.368556022644043)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3183]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=40.42247772216797)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0020]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21809768676757812, max_val=0.2558903992176056)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1945]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=24.702301025390625)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9112961292266846, max_val=0.8250895142555237)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4429]), zero_point=tensor([76], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-33.824737548828125, max_val=22.429607391357422)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2523]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=32.04408264160156)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1566593497991562, max_val=0.28749120235443115)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1355]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.209970474243164)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0106]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7447466850280762, max_val=1.357274055480957)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4579]), zero_point=tensor([65], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-29.668500900268555, max_val=28.487041473388672)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): ConvBn2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0075]), zero_point=tensor([0], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7893481850624084, max_val=0.9507298469543457)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3429]), zero_point=tensor([61], dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-20.955944061279297, max_val=22.59812355041504)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2364]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=30.028310775756836)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): QuantizableBasicBlock(\n",
       "        (conv1): ConvBnReLU2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15437749028205872, max_val=0.2966344952583313)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0417]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.300099849700928)\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu): Identity()\n",
       "        (conv2): ConvBn2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0389]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9657297134399414, max_val=4.955899715423584)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7547]), zero_point=tensor([65], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-49.16975402832031, max_val=46.6772346496582)\n",
       "          )\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (add_relu): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4129]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=52.43907165527344)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (10): DeQuantStub()\n",
       "  )\n",
       "  (1): Flatten(start_dim=1, end_dim=-1)\n",
       "  (2): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft_tuned.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e35322e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_quantized_and_trained = convert(model_ft_tuned, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d1db0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/jit/_script.py:1277: UserWarning: `optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Sequential\n",
       "  (0): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(original_name=Quantize)\n",
       "    (1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "    (2): RecursiveScriptModule(original_name=Identity)\n",
       "    (3): RecursiveScriptModule(original_name=Identity)\n",
       "    (4): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "    (5): RecursiveScriptModule(\n",
       "      original_name=Sequential\n",
       "      (0): RecursiveScriptModule(\n",
       "        original_name=QuantizableBasicBlock\n",
       "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
       "        (relu): RecursiveScriptModule(original_name=Identity)\n",
       "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
       "        (add_relu): RecursiveScriptModule(\n",
       "          original_name=QFunctional\n",
       "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "      )\n",
       "      (1): RecursiveScriptModule(\n",
       "        original_name=QuantizableBasicBlock\n",
       "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
       "        (relu): RecursiveScriptModule(original_name=Identity)\n",
       "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
       "        (add_relu): RecursiveScriptModule(\n",
       "          original_name=QFunctional\n",
       "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): RecursiveScriptModule(\n",
       "      original_name=Sequential\n",
       "      (0): RecursiveScriptModule(\n",
       "        original_name=QuantizableBasicBlock\n",
       "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
       "        (relu): RecursiveScriptModule(original_name=Identity)\n",
       "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
       "        (downsample): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (1): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "        (add_relu): RecursiveScriptModule(\n",
       "          original_name=QFunctional\n",
       "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "      )\n",
       "      (1): RecursiveScriptModule(\n",
       "        original_name=QuantizableBasicBlock\n",
       "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
       "        (relu): RecursiveScriptModule(original_name=Identity)\n",
       "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
       "        (add_relu): RecursiveScriptModule(\n",
       "          original_name=QFunctional\n",
       "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): RecursiveScriptModule(\n",
       "      original_name=Sequential\n",
       "      (0): RecursiveScriptModule(\n",
       "        original_name=QuantizableBasicBlock\n",
       "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
       "        (relu): RecursiveScriptModule(original_name=Identity)\n",
       "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
       "        (downsample): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (1): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "        (add_relu): RecursiveScriptModule(\n",
       "          original_name=QFunctional\n",
       "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "      )\n",
       "      (1): RecursiveScriptModule(\n",
       "        original_name=QuantizableBasicBlock\n",
       "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
       "        (relu): RecursiveScriptModule(original_name=Identity)\n",
       "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
       "        (add_relu): RecursiveScriptModule(\n",
       "          original_name=QFunctional\n",
       "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): RecursiveScriptModule(\n",
       "      original_name=Sequential\n",
       "      (0): RecursiveScriptModule(\n",
       "        original_name=QuantizableBasicBlock\n",
       "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
       "        (relu): RecursiveScriptModule(original_name=Identity)\n",
       "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
       "        (downsample): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "          (1): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "        (add_relu): RecursiveScriptModule(\n",
       "          original_name=QFunctional\n",
       "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "      )\n",
       "      (1): RecursiveScriptModule(\n",
       "        original_name=QuantizableBasicBlock\n",
       "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
       "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
       "        (relu): RecursiveScriptModule(original_name=Identity)\n",
       "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
       "        (add_relu): RecursiveScriptModule(\n",
       "          original_name=QFunctional\n",
       "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): RecursiveScriptModule(original_name=AdaptiveAvgPool2d)\n",
       "    (10): RecursiveScriptModule(original_name=DeQuantize)\n",
       "  )\n",
       "  (1): RecursiveScriptModule(original_name=Flatten)\n",
       "  (2): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(original_name=Dropout)\n",
       "    (1): RecursiveScriptModule(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.script(model_quantized_and_trained ,\"resnet_ft.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5d4935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/utils.py:317: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#model_int8_new_t_no_qnt_layer = torch.quantization.convert(model_fp32_prepared.to('cpu').eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9666101",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
