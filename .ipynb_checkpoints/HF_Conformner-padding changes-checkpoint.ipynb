{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fba8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.28.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (6.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib>=2.2->seaborn) (1.2.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib>=2.2->seaborn) (59.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c681f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n",
    "from transformers import AutoProcessor, Wav2Vec2ConformerForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8417dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922add56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler = GradScaler()\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbafbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c402ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hugging face\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC\n",
    "from transformers import Wav2Vec2ConformerForSequenceClassification, Wav2Vec2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0781d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b756eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aef09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee37e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "import torch.multiprocessing as mp\n",
    "#\n",
    "\n",
    "#pool.map(worker_fn, range(4))\n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers= 0\n",
    "pin_memory=False\n",
    "#train_size = 100\n",
    "batch_size = 1\n",
    "test_batch_size = 1\n",
    "num_epochs = 200\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    num_workers=0\n",
    "    num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496aac1",
   "metadata": {},
   "source": [
    "### Let's get the data in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd1d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    stride = step_frac*min_length\n",
    "#     print(\"min_length = \" +str(min_length))\n",
    "#     print(\"step_frac = \" +str(step_frac))\n",
    "#     print(\"stride = \" +str(stride))\n",
    "    for _,row in df.iterrows():\n",
    "        #processed_data keeps track of the tensor_values processed thus far\n",
    "        if row['length'] > min_length:\n",
    "            processed_data = 0\n",
    "            #total_data is the total tensor present in the audio\n",
    "            total_data = config.rate*row['length']\n",
    "            #print(\"********\")\n",
    "            count = 0\n",
    "#             print(\"count = \" +str(count))\n",
    "#             print(\"id = \" + str(row['id']) + \" duration = \" +str(row['length']) + \"total x vals = \" + str(total_data))\n",
    "            inner_loop_flag = False\n",
    "            #print(\"going into the inner loop to offset....\")\n",
    "            while(processed_data < total_data):\n",
    "                #print(\"inside inner loop.....\")\n",
    "                start = count*stride*config.rate\n",
    "                #now find out the row_len\n",
    "                if total_data - (start + min_length*config.rate) >= 0:\n",
    "                    #print(\"full chunk \")\n",
    "                    row_len = min_length\n",
    "                    end = start + row_len*config.rate\n",
    "                    audio_offsets.append({'id':row['id'], 'offset':count, 'length': row_len,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "                    #print(\"count = \" +str(count) + \"offset = \" +str(count) + \"start = \" +str(start) + \"end = \" +str(end))\n",
    "                    #print(\"for count.... = \" + str(count) + \"processed data = \" +str(processed_data))\n",
    "                    count+=1\n",
    "                    processed_data = (count*stride)*config.rate\n",
    "                    \n",
    "                else:\n",
    "                    inner_loop_flag = True\n",
    "                    break\n",
    "                    \n",
    "                                                       \n",
    "            #for processing residual data\n",
    "            if(inner_loop_flag):\n",
    "                #print(\"processing residual ....processed \" +str(processed_data) + \" of \" + str(total_data))\n",
    "                start = count*stride*config.rate\n",
    "                resid_durn = round((total_data - processed_data)/config.rate,2)\n",
    "                end = total_data\n",
    "                #print(\"for...\" + str(row['id']) + \" adding the residual data in the data frame with duration = \" + str(resid_durn))\n",
    "                audio_offsets.append({'id':row['id'], 'offset':count, 'length':resid_durn ,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "            \n",
    "        elif short_audio:\n",
    "            start = 0\n",
    "            end = row['length']*config.rate\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind'],'start':0 , 'end':end})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e24a9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23844f97",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cfa3800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81</td>\n",
       "      <td>0.302665</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>0.226999</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86</td>\n",
       "      <td>0.264832</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>71</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id    length                             name  sample_rate  \\\n",
       "1   53  0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2   57  0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3   61  0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4   69  0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5   56  0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "6   81  0.302665  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "7   70  0.226999  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "8   86  0.264832  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "9   62  0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "10  71  0.690455  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "\n",
       "   record_datetime sound_type     species gender  fed plurality  age method  \\\n",
       "1    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN   \n",
       "2    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN   \n",
       "3    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN   \n",
       "4    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN   \n",
       "5    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Plural  NaN    NaN   \n",
       "6    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Plural  NaN    NaN   \n",
       "7    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN   \n",
       "8    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Plural  NaN    NaN   \n",
       "9    8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN   \n",
       "10   8/9/2016 8:00   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN   \n",
       "\n",
       "   mic_type    device_type country district province  \\\n",
       "1     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "2     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "3     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "4     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "5     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "6     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "7     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "8     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "9     phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "10    phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "\n",
       "                           place location_type  \n",
       "1   CDC insect cultures, Atlanta       culture  \n",
       "2   CDC insect cultures, Atlanta       culture  \n",
       "3   CDC insect cultures, Atlanta       culture  \n",
       "4   CDC insect cultures, Atlanta       culture  \n",
       "5   CDC insect cultures, Atlanta       culture  \n",
       "6   CDC insect cultures, Atlanta       culture  \n",
       "7   CDC insect cultures, Atlanta       culture  \n",
       "8   CDC insect cultures, Atlanta       culture  \n",
       "9   CDC insect cultures, Atlanta       culture  \n",
       "10  CDC insect cultures, Atlanta       culture  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if DEBUG:\n",
    "#     df = pd.read_csv(config.data_df_msc_test)\n",
    "# else:\n",
    "df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958f72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ca853a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "807d5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad412d",
   "metadata": {},
   "source": [
    "At this stage we have all extracted the data with specie information and have encoded the specie encoding in a col = 'specie_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7721fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06d3b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fd099db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data- this is as per the humbug paper\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f02a2c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "import seaborn as sns\n",
    "sns.countplot(x = 'species', data = df_all , ax = ax , hue = 'gender',palette='dark')\n",
    "#ax.bar_label(ax.containers[0])\n",
    "#ax.bar_label(ax.containers[-1], fmt='Count:\\n%.2f', label_type='center')\n",
    "plt.xticks(rotation=90 )\n",
    "plt.title(\"Distribution of Species \")\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('figure', titlesize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d3d24",
   "metadata": {},
   "source": [
    "### Train-Test split( avoiding sklearn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "387165af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acee651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70595e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed7afde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>221144</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_28_668.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>culex pipiens complex</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>221140</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_30_670.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>221136</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_30_670.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>221139</td>\n",
       "      <td>5.12</td>\n",
       "      <td>IFA_17_30_670.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>221134</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_30_670.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>221102</td>\n",
       "      <td>12.80</td>\n",
       "      <td>IFA_17_31_671.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>culex pipiens complex</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  length               name  sample_rate record_datetime  \\\n",
       "1879  221103    2.56  IFA_17_24_664.wav        44100  30-01-20 00:00   \n",
       "1880  221111    2.56  IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "1881  221110    2.56  IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "1882  221149    2.56  IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "1890  221144    2.56  IFA_17_28_668.wav        44100  30-01-20 00:00   \n",
       "1891  221140    2.56  IFA_17_30_670.wav        44100  30-01-20 00:00   \n",
       "1892  221136    2.56  IFA_17_30_670.wav        44100  30-01-20 00:00   \n",
       "1896  221139    5.12  IFA_17_30_670.wav        44100  30-01-20 00:00   \n",
       "1897  221134    2.56  IFA_17_30_670.wav        44100  30-01-20 00:00   \n",
       "1898  221102   12.80  IFA_17_31_671.wav        44100  30-01-20 00:00   \n",
       "\n",
       "     sound_type                species  gender fed plurality  age method  \\\n",
       "1879   mosquito           ma africanus  Female   f    Single  NaN    HBN   \n",
       "1880   mosquito           ma africanus  Female   f    Single  NaN    HBN   \n",
       "1881   mosquito           ma africanus  Female   f    Single  NaN    HBN   \n",
       "1882   mosquito          an arabiensis  Female   f    Single  NaN    HBN   \n",
       "1890   mosquito  culex pipiens complex  Female   f    Single  NaN    HBN   \n",
       "1891   mosquito           ma africanus  Female   f    Single  NaN    HBN   \n",
       "1892   mosquito           ma africanus  Female   f    Single  NaN    HBN   \n",
       "1896   mosquito           ma africanus  Female   f    Single  NaN    HBN   \n",
       "1897   mosquito           ma africanus  Female   f    Single  NaN    HBN   \n",
       "1898   mosquito  culex pipiens complex  Female   f    Single  NaN    HBN   \n",
       "\n",
       "     mic_type device_type   country            district  province    place  \\\n",
       "1879  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1880  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1881  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1882  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1890  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1891  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1892  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1896  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1897  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1898  telinga      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "\n",
       "     location_type specie_ind  \n",
       "1879           cup          7  \n",
       "1880           cup          7  \n",
       "1881           cup          7  \n",
       "1882           cup          0  \n",
       "1890           cup          1  \n",
       "1891           cup          7  \n",
       "1892           cup          7  \n",
       "1896           cup          7  \n",
       "1897           cup          7  \n",
       "1898           cup          1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f24cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe9b695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221144</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_28_668.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>culex pipiens complex</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  length               name  sample_rate record_datetime sound_type  \\\n",
       "0  221103    2.56  IFA_17_24_664.wav        44100  30-01-20 00:00   mosquito   \n",
       "1  221111    2.56  IFA_17_25_665.wav        44100  30-01-20 00:00   mosquito   \n",
       "2  221110    2.56  IFA_17_25_665.wav        44100  30-01-20 00:00   mosquito   \n",
       "3  221149    2.56  IFA_17_26_666.wav        44100  30-01-20 00:00   mosquito   \n",
       "4  221144    2.56  IFA_17_28_668.wav        44100  30-01-20 00:00   mosquito   \n",
       "\n",
       "                 species  gender fed plurality  age method mic_type  \\\n",
       "0           ma africanus  Female   f    Single  NaN    HBN  telinga   \n",
       "1           ma africanus  Female   f    Single  NaN    HBN  telinga   \n",
       "2           ma africanus  Female   f    Single  NaN    HBN  telinga   \n",
       "3          an arabiensis  Female   f    Single  NaN    HBN  telinga   \n",
       "4  culex pipiens complex  Female   f    Single  NaN    HBN  telinga   \n",
       "\n",
       "  device_type   country            district  province    place location_type  \\\n",
       "0      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "1      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "2      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "3      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "4      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "\n",
       "  specie_ind  \n",
       "0          7  \n",
       "1          7  \n",
       "2          7  \n",
       "3          0  \n",
       "4          1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "083d04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.reset_index(inplace= True , drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72d5b9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221122</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_27_667.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma uniformis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221124</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_27_667.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma uniformis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221145</td>\n",
       "      <td>38.40</td>\n",
       "      <td>IFA_17_28_668.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>culex pipiens complex</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221138</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_30_670.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221101</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_31_671.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>culex pipiens complex</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  length               name  sample_rate record_datetime sound_type  \\\n",
       "0  221122    2.56  IFA_17_27_667.wav        44100  30-01-20 00:00   mosquito   \n",
       "1  221124    2.56  IFA_17_27_667.wav        44100  30-01-20 00:00   mosquito   \n",
       "2  221145   38.40  IFA_17_28_668.wav        44100  30-01-20 00:00   mosquito   \n",
       "3  221138    2.56  IFA_17_30_670.wav        44100  30-01-20 00:00   mosquito   \n",
       "4  221101    2.56  IFA_17_31_671.wav        44100  30-01-20 00:00   mosquito   \n",
       "\n",
       "                 species  gender fed plurality  age method mic_type  \\\n",
       "0           ma uniformis  Female   f    Single  NaN    HBN  telinga   \n",
       "1           ma uniformis  Female   f    Single  NaN    HBN  telinga   \n",
       "2  culex pipiens complex  Female   f    Single  NaN    HBN  telinga   \n",
       "3           ma africanus  Female   f    Single  NaN    HBN  telinga   \n",
       "4  culex pipiens complex  Female   f    Single  NaN    HBN  telinga   \n",
       "\n",
       "  device_type   country            district  province    place location_type  \\\n",
       "0      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "1      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "2      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "3      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "4      tascam  Tanzania  Kilombero District  Morogoro  Ifakara           cup   \n",
       "\n",
       "  specie_ind  \n",
       "0          6  \n",
       "1          6  \n",
       "2          1  \n",
       "3          7  \n",
       "4          1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b765c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.reset_index(inplace= True , drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12d8a9",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b18c9d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>plurality_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 39 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15a36e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>plurality_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 39 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b39f52a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>plurality_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 39 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8338cc4",
   "metadata": {},
   "source": [
    "We've confirmed that there is no recording that is common in Train,Test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ede4d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "204681b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35354291 0.51942815 3.61479592 0.73802083 1.86447368 3.10745614\n",
      " 2.24208861 3.22045455]\n"
     ]
    }
   ],
   "source": [
    "#Class imbalance \n",
    "np.array(df_train.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train.specie_ind)),y=np.array(np.array(df_train.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a815953",
   "metadata": {},
   "source": [
    "### Now we need to populate the lists of file_names and label_names for use in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebc9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32329c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eec10c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/audio'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"..\",\"..\",\"data\",\"audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd7b9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "    #parent_dir = os.path.join(\"~\",\"ComParE2022_VecNet\",\"data\",\"audio\")\n",
    "    parent_dir = os.path.join(\"..\",\"..\",\"data\",\"audio\")\n",
    "    file_list = []\n",
    "    label_list = []\n",
    "    #print(\"inside get_lists\")\n",
    "    for ind,row in tqdm(df.iterrows()):\n",
    "        #print(row)\n",
    "        #print(row['id'])\n",
    "        wav = str(int(row['id']))+\".wav\"\n",
    "        final_path = parent_dir + \"/\"+wav\n",
    "        #print(\"wav file path = \",final_path)\n",
    "        #start_offset = int(round((row['start'])))\n",
    "        #end_offset = int(round((row['end'])))\n",
    "                                  \n",
    "        file_list.append(final_path)\n",
    "        label_list.append(torch.tensor(row['specie_ind']))\n",
    "    return file_list, label_list\n",
    "        \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16ce05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cb0c8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b31cd851ee4064a9b650764e193182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36883329fb8b48f1b421e53e5212c519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf0123f8c4d47d89ecfe5c3b0d20e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_list_train , label_list_train = get_lists(df_train)\n",
    "file_list_val , label_list_val = get_lists(df_val)\n",
    "file_list_test , label_list_test = get_lists(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e32178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list_train len =  1417\n",
      "label_list_train len =  1417\n",
      "file_list_val len =  389\n",
      "label_list_val len =  389\n",
      "file_list_test len =  482\n",
      "label_list_test len =  482\n"
     ]
    }
   ],
   "source": [
    "print(\"file_list_train len = \", len(file_list_train))\n",
    "print(\"label_list_train len = \", len(label_list_train))\n",
    "print(\"file_list_val len = \", len(file_list_val))\n",
    "print(\"label_list_val len = \", len(label_list_val))\n",
    "print(\"file_list_test len = \", len(file_list_test))\n",
    "print(\"label_list_test len = \", len(label_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cff682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c901cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82e28f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model(x)['prediction']\n",
    "#             y_pred_smax = softmax(y_pred)\n",
    "#             preds = torch.argmax(y_pred_smax, axis = 1)\n",
    "#             y_pred_cpu = y_pred.cpu().detach()\n",
    "#             if DEBUG:\n",
    "#                 print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "#             #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "#             if DEBUG:\n",
    "#                 print(\"preds = \" +str(preds))\n",
    "#             all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "#             loss = criterion(y_pred, y)\n",
    "#             test_loss += loss.item()\n",
    "#             all_y.append(y.cpu().detach())\n",
    "#             #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "#             del x\n",
    "#             del y\n",
    "#             del y_pred\n",
    "#         all_y = torch.cat(all_y)\n",
    "#         all_y_pred = torch.cat(all_y_pred)\n",
    "#         if DEBUG:\n",
    "#             print(\"inside test....\")\n",
    "#             print(\"y = \" + str(all_y))\n",
    "#             print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "#         test_loss = test_loss/len(test_loader)\n",
    "#         test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6eab8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    softmax = nn.Softmax()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        for i, (inputs, labels) in tqdm(enumerate(loader), desc = \"inside test\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_pred = model(inputs).logits\n",
    "            y_pred_smax = softmax(y_pred)\n",
    "            preds = torch.argmax(y_pred_smax, axis = 1)\n",
    "            #_, preds = torch.max(outputs, 1)\n",
    "            #preds = torch.argmax(y_pred_smax, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, labels)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(labels.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            if DEBUG:\n",
    "                print(\"inside test....\" + \"calling for \" + str(call))\n",
    "                print(\"inputs shape  = \" + str(inputs.shape))\n",
    "                print(\"labels shape  = \" + str(labels.shape))\n",
    "                print(\"y_pred  = \" + str(y_pred))\n",
    "                #print(\"y_pred_smax = \" +str(y_pred_smax))\n",
    "                print(\"preds = \" +str(preds))\n",
    "                            \n",
    "            \n",
    "            \n",
    "            del inputs\n",
    "            del labels\n",
    "            del y_pred,preds\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        if DEBUG:\n",
    "            print(\"all_y_pred = \", all_y_pred)\n",
    "            print(\"all_y = \", all_y)\n",
    "            print(\"test_f1 = \", test_f1)\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c93aaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the function for training the model\n",
    "# def train_model(model, train_loader, optimizer, loss_fn, device):\n",
    "#     model.train()\n",
    "#     for i, (inputs, labels) in enumerate(train_loader):\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         with autocast():\n",
    "#             outputs = model(inputs).logits\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Define the function for evaluating the model\n",
    "# def eval_model(model, eval_loader, device):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in eval_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(inputs).logits\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#     accuracy = 100 * correct / total\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c634a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for loading the dataset\n",
    "def load_dataset(file_list, label_list, batch_size=batch_size):\n",
    "    dataset = AudioDataset(file_list, label_list)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers = num_workers, collate_fn = collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d84073f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a597559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    \"\"\"\n",
    "    Collate function to handle variable length audio sequences.\n",
    "    :param batch: List of tuples (audio feature tensor, label)\n",
    "    :return: Tuple of padded audio feature tensor and label tensor\n",
    "    \"\"\"\n",
    "    # Sort the batch by sequence length in descending order\n",
    "    #print(\"inside collate...\")\n",
    "    #print(\"batch   = \", batch )\n",
    "#     for b,l in batch:\n",
    "#         print(\"b shape = \",b.shape)\n",
    "    max_len = max([x.shape[0] for x,_ in batch])\n",
    "    #t_list_updated= [(torch.cat((x,(torch.zeros(max_len-x.shape[0]))),dim = -1),label) for (x,label) in t_list]\n",
    "    tensor_updated = [(torch.cat((x,(torch.zeros(max_len-x.shape[0]))),dim = -1)) for x,_ in batch]\n",
    "    tensor_updated_new = [x.unsqueeze(dim = 0 ) for x in tensor_updated]\n",
    "    label_list = [label for _,label in batch]\n",
    "    #final_bat = list(zip(tensor_updated,label_list))\n",
    "    #print(\"len (tensor_updated ) = \", len(tensor_updated_new))\n",
    "    tensor_concat = torch.cat(tensor_updated_new , dim = 0)\n",
    "    if DEBUG:\n",
    "        print(\"^^^ INSIDE COLLATE\" + \"returning final_bat of tensor of shape = \",(tensor_concat.shape))\n",
    "        print(\"type of tensor = \",tensor_concat.dtype)\n",
    "        #print(\"lean of list holding tensor  = \",len(tensor_updated))\n",
    "        #print(\"label list len = \",len(label_list))\n",
    "\n",
    "    return tensor_concat.squeeze(dim = 1) , torch.tensor(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ba7a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_list,label_list):\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "        \n",
    "        #self.labels = labels\n",
    "        #processor = processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\",sampling_rate = 16000,return_tensors=\"pt\",padding= \"longest\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav2_vec_rate = 16000\n",
    "        file_path = self.file_list[index]\n",
    "        label = self.label_list[index]\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"file_path = \", file_path)\n",
    "            #print(\"start = \", start)\n",
    "            #print(\"end = \", end)\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        waveform.to('cuda')\n",
    "        #print(\"after loading . Wavform shape = \" , waveform.shape)\n",
    "              \n",
    "        \n",
    "        \n",
    "        if sample_rate != wav2_vec_rate:\n",
    "            if DEBUG:\n",
    "                print(\"file_path = \" + str(file_path) + \" Original sample rate = \" +str(sample_rate)+ \" resampling ...\")\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(sample_rate, wav2_vec_rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "            if DEBUG:\n",
    "                print(\"waveform shape post resampling = \", waveform.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"final waveform shape being returned = \", waveform.shape)\n",
    "        input_values = self.processor(waveform, sampling_rate=wav2_vec_rate, return_tensors=\"pt\",padding= \"longest\").input_values\n",
    "        if DEBUG:\n",
    "            print(\"input_values shape = \",input_values.shape)\n",
    "        label = self.label_list[index]\n",
    "        return input_values.squeeze(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "772c2ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1417\n",
      "355\n",
      "355\n"
     ]
    }
   ],
   "source": [
    "print(len(file_list_train))\n",
    "test_dataset = AudioDataset(file_list_train, label_list_train)\n",
    "# f_data = test_dataset[0]\n",
    "# feat,lab = f_data\n",
    "# print(feat)\n",
    "# print(lab)\n",
    "demo_loader_col = DataLoader(dataset = test_dataset , batch_size = batch_size,collate_fn = collate_fn )\n",
    "demo_loader = DataLoader(dataset = test_dataset , batch_size = batch_size)\n",
    "print(len(demo_loader))\n",
    "print(len(demo_loader_col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02cc3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_iter = iter(demo_loader)\n",
    "# data_no_col = demo_iter.next()\n",
    "# x, y = data_no_col\n",
    "# print(((x.shape)))\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ac98a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_iter_col = iter(demo_loader_col)\n",
    "# data_col = demo_iter_col.next()\n",
    "# x, y = data_col\n",
    "# print(type(x))\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9d6d1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside DEBUG for dataset creation.. len of train_loader = 3\n",
      "inside DEBUG for dataset creation.. len of val_loader = 2\n",
      "inside DEBUG for dataset creation.. len of test_loader = 2\n"
     ]
    }
   ],
   "source": [
    "train_loader = load_dataset(file_list_train,label_list_train, batch_size=batch_size)\n",
    "val_loader = load_dataset(file_list_val,label_list_val ,batch_size=batch_size)\n",
    "test_loader = load_dataset(file_list_test,label_list_test, batch_size=batch_size)\n",
    "\n",
    "if DEBUG:\n",
    "    file_list_train = file_list_train[:10]\n",
    "    label_list_train = label_list_train[:10]\n",
    "    file_list_val = file_list_val[:6]\n",
    "    label_list_val = label_list_val[:6]\n",
    "    file_list_test = file_list_test[:7]\n",
    "    label_list_test = label_list_test[:7]\n",
    "    train_loader = load_dataset(file_list_train,label_list_train,batch_size=batch_size)\n",
    "    val_loader = load_dataset(file_list_val,label_list_val, batch_size=batch_size)\n",
    "    test_loader = load_dataset(file_list_test,label_list_test ,batch_size=batch_size)\n",
    "    print(\"inside DEBUG for dataset creation..\" + \" len of train_loader = \" + str(len(train_loader)))\n",
    "    print(\"inside DEBUG for dataset creation..\" + \" len of val_loader = \" + str(len(val_loader)))\n",
    "    print(\"inside DEBUG for dataset creation..\" + \" len of test_loader = \" + str(len(test_loader)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d549116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_offset[df_train_offset['id']== 220895]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "272baf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#facebook/wav2vec2-conformer-rel-pos-large-960h-ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d7ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ea676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ee745a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        for i,(inputs,labels) in enumerate(loader):\n",
    "            #inputs,labels = tup[i]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_pred = model(inputs).logits\n",
    "            _, preds = torch.max(y_pred, dim=1)\n",
    "            #_, preds = torch.max(outputs, 1)\n",
    "            #preds = torch.argmax(y_pred_smax, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, labels)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(labels.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            if DEBUG:\n",
    "                print(\"inside test....\" + \"calling for \" + str(call))\n",
    "                print(\"inputs shape  = \" + str(inputs.shape))\n",
    "                print(\"labels shape  = \" + str(labels.shape))\n",
    "                print(\"y_pred  = \" + str(y_pred))\n",
    "                #print(\"y_pred_smax = \" +str(y_pred_smax))\n",
    "                print(\"preds = \" +str(preds))\n",
    "                            \n",
    "            \n",
    "            \n",
    "            del inputs\n",
    "            del labels\n",
    "            del y_pred,preds\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        if DEBUG:\n",
    "            print(\"all_y_pred = \", all_y_pred)\n",
    "            print(\"all_y = \", all_y)\n",
    "            print(\"test_f1 = \", test_f1)\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b433ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09d8a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,test_loader,model ,classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    torch.manual_seed(0)\n",
    "    lr = 1e-4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Training on {device}')    \n",
    "    model = model.to(device)\n",
    "    class_weights = class_weights\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    #sigmoid = nn.Sigmoid()\n",
    "    softmax = nn.Softmax()\n",
    "    all_train_f1 = []\n",
    "    all_val_f1 = []\n",
    "    lr_log = []\n",
    "    for e in tqdm(range(num_epochs), desc = \"epoc loop\"):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        print(train_loader)\n",
    "        for i, (inputs,labels) in enumerate(train_loader):\n",
    "            print(\"^^^^^^^^^i^^^^^^^^^ = \",  i)\n",
    "                  \n",
    "            if i % 200 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            \n",
    "            #inputs,labels = tup[i]\n",
    "            memory_size = inputs.element_size() * inputs.numel()\n",
    "            memory_size_lab = labels.element_size() * labels.numel()\n",
    "            \n",
    "            print(\"inputs memory size in MB= \",memory_size/1e+6)\n",
    "            print(\"labels memory size in MB= \",memory_size_lab/1e+6)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if DEBUG:\n",
    "                    print(\"inputs shape = \", inputs.shape)\n",
    "                    print(\"labels  = \", labels)\n",
    "            with autocast():\n",
    "                y_pred = model(inputs).logits\n",
    "                y_pred_memory_size = y_pred.cpu().element_size() * y_pred.cpu().numel()\n",
    "                print(\"y_pred_memory_size memory size in MB= \",y_pred_memory_size/1e+6)\n",
    "                \n",
    "                _, preds = torch.max(y_pred, dim=1)\n",
    "                #print(\"outputs = \",preds)\n",
    "                if DEBUG:\n",
    "                    print(\"y_pred = \", y_pred)\n",
    "                    print(\"y_pred shape = \", y_pred.shape)\n",
    "                    \n",
    "                    #print(\"y_pred_smax = \", y_pred_smax)\n",
    "                    print(\"preds shape = \", preds.shape)\n",
    "                    print(\"preds  = \", preds)\n",
    "                    \n",
    "                    \n",
    "                loss = loss_fn(y_pred, labels)\n",
    "                                    \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            #train_loss += loss.item()\n",
    "            all_y.append(labels.cpu().detach())\n",
    "            y_pred_cpu = preds.cpu().detach()\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            del inputs\n",
    "            del labels\n",
    "            del y_pred,preds\n",
    "            \n",
    "        #lr_log.append(lr)\n",
    "        train_loss += loss.item()\n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "                 \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        if DEBUG:\n",
    "            print(\"all_y = \", all_y.numpy())\n",
    "            print(\"all_y_pred.numpy() = \", all_y_pred.numpy())\n",
    "            print(\"train f1  = \", train_f1)\n",
    "        all_train_f1.append(train_f1)\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"val\")\n",
    "        all_val_f1.append(val_f1)\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir,  checkpoint_name))\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.model_dir,  checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"********************************\")\n",
    "            from sklearn.metrics import classification_report\n",
    "            _, _ , all_y_test,all_y_pred_test = test_model(model, test_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"test\")\n",
    "            # at times output is not getting printed. Could be due to multi threading and hence adding sleep\n",
    "            time.sleep(2)\n",
    "            sys.stdout.flush()\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            time.sleep(2)\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , classes)\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee75e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_loader:\n",
    "#     print(i)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1fb2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,\n",
    "# max_split_size_mb:128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf1d6f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft were not used when initializing Wav2Vec2ConformerForSequenceClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ConformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ConformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ConformerForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft and are newly initialized: ['projector.weight', 'classifier.weight', 'projector.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de7341d42f44618916532c97c928bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoc loop:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f621631ea00>\n",
      "file_path =  ../../data/audio/221136.wav\n",
      "file_path = ../../data/audio/221136.wav Original sample rate = 44100 resampling ...\n",
      "waveform shape post resampling =  torch.Size([1, 40960])\n",
      "input_values shape =  torch.Size([1, 1, 40960])\n",
      "file_path =  ../../data/audio/221139.wav\n",
      "file_path = ../../data/audio/221139.wav Original sample rate = 44100 resampling ...\n",
      "waveform shape post resampling =  torch.Size([1, 81920])\n",
      "input_values shape =  torch.Size([1, 1, 81920])\n",
      "file_path =  ../../data/audio/221111.wav\n",
      "file_path = ../../data/audio/221111.wav Original sample rate = 44100 resampling ...\n",
      "waveform shape post resampling =  torch.Size([1, 40960])\n",
      "input_values shape =  torch.Size([1, 1, 40960])\n",
      "file_path =  ../../data/audio/221144.wav\n",
      "file_path = ../../data/audio/221144.wav Original sample rate = 44100 resampling ...\n",
      "waveform shape post resampling =  torch.Size([1, 40960])\n",
      "input_values shape =  torch.Size([1, 1, 40960])\n",
      "^^^ INSIDE COLLATEreturning final_bat of tensor of shape =  torch.Size([4, 81920])\n",
      "type of tensor =  torch.float32\n",
      "^^^^^^^^^i^^^^^^^^^ =  0\n",
      "epoch = 0batch = 0 of 3duraation = 0.0010865092277526855\n",
      "inputs memory size in MB=  1.31072\n",
      "labels memory size in MB=  3.2e-05\n",
      "inputs shape =  torch.Size([4, 81920])\n",
      "labels  =  tensor([7, 7, 7, 1], device='cuda:0')\n",
      "y_pred_memory_size memory size in MB=  6.4e-05\n",
      "y_pred =  tensor([[-0.0033,  0.0067,  0.0454, -0.0079, -0.0170,  0.0088,  0.0202, -0.0089],\n",
      "        [ 0.0029,  0.0250,  0.0188,  0.0068,  0.0021, -0.0023,  0.0093, -0.0184],\n",
      "        [ 0.0035,  0.0186,  0.0368, -0.0057, -0.0302, -0.0003,  0.0291, -0.0152],\n",
      "        [ 0.0167,  0.0352,  0.0547, -0.0091, -0.0074,  0.0102,  0.0151, -0.0059]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "y_pred shape =  torch.Size([4, 8])\n",
      "preds shape =  torch.Size([4])\n",
      "preds  =  tensor([2, 1, 2, 2], device='cuda:0')\n",
      "file_path =  ../../data/audio/221110.wav\n",
      "file_path = ../../data/audio/221110.wav Original sample rate = 44100 resampling ...\n",
      "waveform shape post resampling =  torch.Size([1, 40960])\n",
      "input_values shape =  torch.Size([1, 1, 40960])\n",
      "file_path =  ../../data/audio/221103.wav\n",
      "file_path = ../../data/audio/221103.wav Original sample rate = 44100 resampling ...\n",
      "waveform shape post resampling =  torch.Size([1, 40960])\n",
      "input_values shape =  torch.Size([1, 1, 40960])\n",
      "file_path =  ../../data/audio/221102.wav\n",
      "file_path = ../../data/audio/221102.wav Original sample rate = 44100 resampling ...\n",
      "waveform shape post resampling =  torch.Size([1, 204800])\n",
      "input_values shape =  torch.Size([1, 1, 204800])\n",
      "file_path =  ../../data/audio/221134.wav\n",
      "file_path = ../../data/audio/221134.wav Original sample rate = 44100 resampling ...\n",
      "waveform shape post resampling =  torch.Size([1, 40960])\n",
      "input_values shape =  torch.Size([1, 1, 40960])\n",
      "^^^ INSIDE COLLATEreturning final_bat of tensor of shape =  torch.Size([4, 204800])\n",
      "type of tensor =  torch.float32\n",
      "^^^^^^^^^i^^^^^^^^^ =  1\n",
      "inputs memory size in MB=  3.2768\n",
      "labels memory size in MB=  3.2e-05\n",
      "inputs shape =  torch.Size([4, 204800])\n",
      "labels  =  tensor([7, 7, 1, 7], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 22.20 GiB total capacity; 20.11 GiB already allocated; 30.06 MiB free; 20.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44761/3121004971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWav2Vec2ConformerForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'facebook/wav2vec2-conformer-rope-large-960h-ft'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44761/749464261.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, test_loader, model, classes, class_weights, num_epochs, n_channels)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels  = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0my_pred_memory_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_pred_memory_size memory size in MB= \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_memory_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e+6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0moutput_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_weighted_layer_sum\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1800\u001b[0;31m         outputs = self.wav2vec2_conformer(\n\u001b[0m\u001b[1;32m   1801\u001b[0m             \u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1362\u001b[0m         )\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1365\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    936\u001b[0m                     )\n\u001b[1;32m    937\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                     layer_outputs = layer(\n\u001b[0m\u001b[1;32m    939\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, relative_position_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# 2. Self-Attention layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         hidden_states, attn_weigts = self.self_attn(\n\u001b[0m\u001b[1;32m    841\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, relative_position_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    717\u001b[0m             )\n\u001b[1;32m    718\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;31m# apply attention_mask if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 22.20 GiB total capacity; 20.11 GiB already allocated; 30.06 MiB free; 20.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Wav2Vec2ConformerForSequenceClassification.from_pretrained('facebook/wav2vec2-conformer-rope-large-960h-ft' , num_labels = len(classes),output_hidden_states=False ,output_attentions=False,return_dict=True )\n",
    "tr_model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1 = train_model(train_loader,val_loader,test_loader,model ,classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec672c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e780b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "x = data_iter.next()\n",
    "print(len(x))\n",
    "print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "#for batch_idx, (inputs, targets) in enumerate(dataloader)\n",
    "for bat_ind, tup in enumerate(data_iter):\n",
    "    print(\"bat_ind = \",bat_ind)\n",
    "    print(\"tup = \", tup)\n",
    "    for i in range(len(tup)):\n",
    "        x,y = tup[i]\n",
    "        print(\"***********************\")\n",
    "        print(\"x shape = \",x)\n",
    "        print(\"y = \",y)\n",
    "#     x , y = tup[i]\n",
    "#     print(\"x = \",x)\n",
    "#     print(\"y = \",y)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc43130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ConformerForSequenceClassification\n",
    "model = Wav2Vec2ConformerForSequenceClassification.from_pretrained('facebook/wav2vec2-conformer-rope-large-960h-ft' , num_labels = 8,output_hidden_states=False ,output_attentions=False,return_dict=True )\n",
    "model.to('cuda')\n",
    "x_temp = torch.rand((4,204800)).to('cuda')\n",
    "print(x_temp)\n",
    "print(\"type of x_temp = \",(x_temp.dtype))\n",
    "out = model(x_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_log= out.logits\n",
    "_, preds = torch.max(out_log, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3414c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_log.cpu().element_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957411f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_log.cpu().element_size() * out_log.cpu().numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1262cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191d86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421454a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9db36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4559d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdce03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4919a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b99aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66bb617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48718c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb399f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed37767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ee53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e99a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff821917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e9cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f1ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
