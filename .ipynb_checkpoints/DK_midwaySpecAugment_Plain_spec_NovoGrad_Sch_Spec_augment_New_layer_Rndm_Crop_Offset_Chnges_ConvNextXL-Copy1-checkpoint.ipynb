{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f2f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0006d7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbbd65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463c8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_1.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_2.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_3.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_4.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04555681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip /content/humbugdb_neurips_2021_1.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_2.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_3.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_4.zip?download=1 -d '/content/HumBugDB/data/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e700e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch_optimizer in /opt/conda/lib/python3.8/site-packages (0.3.0)\n",
      "Requirement already satisfied: pytorch-ranger>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from torch_optimizer) (0.1.1)\n",
      "Requirement already satisfied: torch>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from torch_optimizer) (2.0.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (3.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.4.91)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.101)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (2.14.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch_optimizer) (59.4.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch_optimizer) (0.37.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.5.0->torch_optimizer) (16.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.5.0->torch_optimizer) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (6.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.28.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib!=3.6.1,>=3.1->seaborn) (59.4.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib!=3.6.1,>=3.1->seaborn) (1.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_optimizer\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2793e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525a1859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5cad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3006d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f28a9d5",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d2fc131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8de97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6df48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f12607a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch_optimizer as optim \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "345817fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7130bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "933d7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c92c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02715e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers= 4\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 64\n",
    "test_batch_size = 32\n",
    "DEBUG = False\n",
    "num_epochs = 50\n",
    "if DEBUG:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    num_workers=0\n",
    "    num_epochs = 4\n",
    "    \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f0cf3",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "929647d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    stride = step_frac*min_length\n",
    "#     print(\"min_length = \" +str(min_length))\n",
    "#     print(\"step_frac = \" +str(step_frac))\n",
    "#     print(\"stride = \" +str(stride))\n",
    "    for _,row in df.iterrows():\n",
    "        #processed_data keeps track of the tensor_values processed thus far\n",
    "        if row['length'] > min_length:\n",
    "            processed_data = 0\n",
    "            #total_data is the total tensor present in the audio\n",
    "            total_data = config.rate*row['length']\n",
    "            #print(\"********\")\n",
    "            count = 0\n",
    "#             print(\"count = \" +str(count))\n",
    "#             print(\"id = \" + str(row['id']) + \" duration = \" +str(row['length']) + \"total x vals = \" + str(total_data))\n",
    "            inner_loop_flag = False\n",
    "            #print(\"going into the inner loop to offset....\")\n",
    "            while(processed_data < total_data):\n",
    "                #print(\"inside inner loop.....\")\n",
    "                start = count*stride*config.rate\n",
    "                #now find out the row_len\n",
    "                if total_data - (start + min_length*config.rate) >= 0:\n",
    "                    #print(\"full chunk \")\n",
    "                    row_len = min_length\n",
    "                    end = start + row_len*config.rate\n",
    "                    audio_offsets.append({'id':row['id'], 'offset':count, 'length': row_len,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "#                     print(\"count = \" +str(count) + \"offset = \" +str(count) + \"start = \" +str(start) + \"end = \" +str(end))\n",
    "#                     print(\"for count.... = \" + str(count) + \"processed data = \" +str(processed_data))\n",
    "                    count+=1\n",
    "                    processed_data = (count*stride)*config.rate\n",
    "                    \n",
    "                else:\n",
    "                    inner_loop_flag = True\n",
    "                    break\n",
    "                    \n",
    "                                                       \n",
    "            #for processing residual data\n",
    "            if(inner_loop_flag):\n",
    "                #print(\"processing residual ....processed \" +str(processed_data) + \" of \" + str(total_data))\n",
    "                start = count*stride*config.rate\n",
    "                resid_durn = round((total_data - processed_data)/config.rate,2)\n",
    "                end = total_data\n",
    "                #print(\"for...\" + str(row['id']) + \" adding the residual data in the data frame with duration = \" + str(resid_durn))\n",
    "                audio_offsets.append({'id':row['id'], 'offset':count, 'length':resid_durn ,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "            \n",
    "        elif short_audio:\n",
    "            start = 0\n",
    "            end = row['length']*config.rate\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind'],'start':0 , 'end':end})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf37526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935ee0d",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2f8bc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "...                     ...                              ...           ...  \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "\n",
       "[6008 rows x 19 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if DEBUG:\n",
    "#     df = pd.read_csv(config.data_df_msc_test)\n",
    "# else:\n",
    "df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6c68eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4701013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8ddd76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68c185",
   "metadata": {},
   "source": [
    "At this stage we have all extracted the data with specie information and have encoded the specie encoding in a col = 'specie_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba7fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28c2a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data- this is as per the humbug paper\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d34fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "becf3095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879</td>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881</td>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1882</td>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1883</td>\n",
       "      <td>221150</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>4546</td>\n",
       "      <td>222615</td>\n",
       "      <td>30.72</td>\n",
       "      <td>IFA_86_39_3439.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>4547</td>\n",
       "      <td>222585</td>\n",
       "      <td>25.60</td>\n",
       "      <td>IFA_86_40_3440.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>4548</td>\n",
       "      <td>222586</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_10_3450.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>4549</td>\n",
       "      <td>222596</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_11_3451.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>4550</td>\n",
       "      <td>222614</td>\n",
       "      <td>38.40</td>\n",
       "      <td>IFA_87_12_3452.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2288 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      id  length                name  sample_rate record_datetime  \\\n",
       "0      1879  221103    2.56   IFA_17_24_664.wav        44100  30-01-20 00:00   \n",
       "1      1880  221111    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "2      1881  221110    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "3      1882  221149    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "4      1883  221150    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "...     ...     ...     ...                 ...          ...             ...   \n",
       "2283   4546  222615   30.72  IFA_86_39_3439.wav        44100  23-08-20 00:00   \n",
       "2284   4547  222585   25.60  IFA_86_40_3440.wav        44100  23-08-20 00:00   \n",
       "2285   4548  222586   40.90  IFA_87_10_3450.wav        44100  23-08-20 00:00   \n",
       "2286   4549  222596   40.90  IFA_87_11_3451.wav        44100  23-08-20 00:00   \n",
       "2287   4550  222614   38.40  IFA_87_12_3452.wav        44100  23-08-20 00:00   \n",
       "\n",
       "     sound_type         species  gender fed  ... age  method mic_type  \\\n",
       "0      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "1      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "2      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "3      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "4      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "...         ...             ...     ...  ..  ...  ..     ...      ...   \n",
       "2283   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2284   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2285   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2286   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2287   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "\n",
       "     device_type   country            district  province    place  \\\n",
       "0         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "3         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "4         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "...          ...       ...                 ...       ...      ...   \n",
       "2283      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2284      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2285      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2286      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2287      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "\n",
       "     location_type specie_ind  \n",
       "0              cup          7  \n",
       "1              cup          7  \n",
       "2              cup          7  \n",
       "3              cup          0  \n",
       "4              cup          0  \n",
       "...            ...        ...  \n",
       "2283           cup          3  \n",
       "2284           cup          3  \n",
       "2285           cup          3  \n",
       "2286           cup          3  \n",
       "2287           cup          3  \n",
       "\n",
       "[2288 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eafe1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "import seaborn as sns\n",
    "sns.countplot(x = 'species', data = df_all , ax = ax , hue = 'gender',palette='dark')\n",
    "#ax.bar_label(ax.containers[0])\n",
    "#ax.bar_label(ax.containers[-1], fmt='Count:\\n%.2f', label_type='center')\n",
    "plt.xticks(rotation=90 )\n",
    "plt.title(\"Distribution of Species \")\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('figure', titlesize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ec462",
   "metadata": {},
   "source": [
    "### Train-Test split( avoiding sklearn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebbd611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4245)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9b10203",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4245)\n",
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff82048",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f992101e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a11fc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8ac8b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5da9a",
   "metadata": {},
   "source": [
    "We've confirmed that there is no recording that is common in Train,Test,val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6ef5b",
   "metadata": {},
   "source": [
    "### Next, we perform \"offsets\", spliting each(long) recording into multiple 1.92 secs chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cd245f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7376b941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train offset = 36491\n",
      "length of test offset = 10328\n",
      "length of val offset = 8733\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8d1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3ae03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff8c490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995883c",
   "metadata": {},
   "source": [
    "### Let's check for data leakage in offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0cff3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_test_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "714be1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "353dda83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471ef25",
   "metadata": {},
   "source": [
    "### At this stage we've a dataframe of recordin ids and each row corresponds to a 1.92 secs recording or shorter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43687c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb65c14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32016389 0.5471899  3.45820697 0.64847526 2.21748906 3.68447092\n",
      " 2.96578349 6.31769391]\n"
     ]
    }
   ],
   "source": [
    "#Class imbalance \n",
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402ca6e",
   "metadata": {},
   "source": [
    "Let us now get the class distribution for each of the dataframes- train,test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4e5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b975235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = train\n",
      "i = 0\n",
      "14247\n",
      "DF type = train\n",
      "i = 1\n",
      "8336\n",
      "DF type = train\n",
      "i = 2\n",
      "1319\n",
      "DF type = train\n",
      "i = 3\n",
      "7034\n",
      "DF type = train\n",
      "i = 4\n",
      "2057\n",
      "DF type = train\n",
      "i = 5\n",
      "1238\n",
      "DF type = train\n",
      "i = 6\n",
      "1538\n",
      "DF type = train\n",
      "i = 7\n",
      "722\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9f54ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = Val\n",
      "i = 0\n",
      "3701\n",
      "DF type = Val\n",
      "i = 1\n",
      "1960\n",
      "DF type = Val\n",
      "i = 2\n",
      "229\n",
      "DF type = Val\n",
      "i = 3\n",
      "1851\n",
      "DF type = Val\n",
      "i = 4\n",
      "289\n",
      "DF type = Val\n",
      "i = 5\n",
      "228\n",
      "DF type = Val\n",
      "i = 6\n",
      "312\n",
      "DF type = Val\n",
      "i = 7\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78560239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = test\n",
      "i = 0\n",
      "4145\n",
      "DF type = test\n",
      "i = 1\n",
      "1775\n",
      "DF type = test\n",
      "i = 2\n",
      "413\n",
      "DF type = test\n",
      "i = 3\n",
      "2210\n",
      "DF type = test\n",
      "i = 4\n",
      "747\n",
      "DF type = test\n",
      "i = 5\n",
      "207\n",
      "DF type = test\n",
      "i = 6\n",
      "578\n",
      "DF type = test\n",
      "i = 7\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca01b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a4ae6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2569f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64aba647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2ff21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35cc895d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c33c65",
   "metadata": {},
   "source": [
    "### Class Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff722257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f90b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc2a663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d320df8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221103</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221111</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id  offset  length  specie_ind    start      end\n",
       "0      0  221103       0    1.92           7      0.0  15360.0\n",
       "1      1  221103       1    1.92           7   5120.0  20480.0\n",
       "2      2  221103       2    1.28           7  10240.0  20480.0\n",
       "3      3  221111       0    1.92           7      0.0  15360.0\n",
       "4      4  221111       1    1.92           7   5120.0  20480.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bcbb4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_df(loader , trained_model, DEBUG = False):\n",
    "    err_dict = {'id': None,\n",
    "               'label': None,\n",
    "               'offset':None,\n",
    "               'y_hat':None}\n",
    "    model = trained_model\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        all_wav_id = []\n",
    "        all_offset = []\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y,offset,wav_id) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                print(\"y = \" + str(y))\n",
    "                print(\"offset = \" + str(offset))\n",
    "                print(\"wav_id = \" + str(wav_id))\n",
    "                \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            y_pred = model(x)['prediction']\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_wav_id.append(wav_id.cpu().detach())\n",
    "            all_offset.append(offset.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y).numpy()\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        all_wav_id = torch.cat(all_wav_id)\n",
    "        all_offset = torch.cat(all_offset)\n",
    "        \n",
    "        err_dict['id'] = all_wav_id\n",
    "        err_dict['label'] = all_y\n",
    "        err_dict['offset'] = all_offset\n",
    "        err_dict['y_hat'] = all_y_pred\n",
    "        df_err = pd.DataFrame.from_dict(err_dict)\n",
    "        df_err_uniq = df_err[df_err['label']!= df_err['y_hat']]\n",
    "        df_err_uniq.sort_values(by=['id','offset'])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"inside error ....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        #test_loss = test_loss/len(test_loader)\n",
    "        #test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return df_err_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cfb2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    softmax = nn.Softmax()\n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\" $$$$$$$$inside test$$$$$$$$....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            output = model(x,train = False)\n",
    "            y_pred = output['probs']\n",
    "            #y_pred_smax = softmax(y_pred)\n",
    "            preds = output['preds']\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "            print(\" $$$$$$$$ exiting test$$$$$$$$....\")\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c77c4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )\n",
    "def train_model(train_loader, val_loader,test_loader, model = None,  classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    \n",
    "    #from torch.optim import Novograd\n",
    "    #from torchcontrib.optim import Lookahead\n",
    "    #base_optimizer = \n",
    "    optimiser = optim.NovoGrad(model.parameters(),lr= .01,betas=(0.9, 0.999),eps=1e-8,weight_decay=0,grad_averaging=True, amsgrad=False,)\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    scheduler = CosineAnnealingLR(optimiser, T_max=num_epochs, eta_min= 1e-5)\n",
    "    \n",
    "    \n",
    "    #optimiser = timm.optim.create_optimizer_v2(model.parameters(), lr= 1e-4,opt = 'lookahead_adam')\n",
    "    #optimiser = timm.optim.AdamW(model.parameters(), lr=config_pytorchlr)\n",
    "    #timm.optim.Lookahead(optimiser, alpha=0.5, k=6)\n",
    "    \n",
    "    #optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    softmax = nn.Softmax()\n",
    "    all_train_f1 = []\n",
    "    all_val_f1 = []\n",
    "    lr_log = []\n",
    "    for e in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        #tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(train_loader):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 500 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. x device = \" +str(x.device))\n",
    "                \n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            with autocast():\n",
    "                output = model(x,train = True)\n",
    "                y_pred = output['probs']\n",
    "                #y_pred_smax = softmax(y_pred)\n",
    "                preds = output['preds']\n",
    "                loss = criterion(y_pred, y)\n",
    "            \n",
    "            if DEBUG:\n",
    "                    print(\"y_pred  = \" +str(y_pred))\n",
    "                    print(\"preds = \" +str(preds))\n",
    "                \n",
    "                \n",
    "            #loss_scaler(loss, optimiser,parameters=model_parameters(model))\n",
    "            if loss.item() > 10000:\n",
    "                print(\"^^^^^^^^^^^^^^^^^ EXPLOSION^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                print(\"x sum = \" + str(torch.sum(x)))\n",
    "                print(\"current loss = \" + str(loss.item()))\n",
    "            train_loss += loss.item()\n",
    "            if torch.isnan(loss):\n",
    "                print(\"NAN encountered ..\")\n",
    "                print(\"current learning rate = \",scheduler.get_last_lr()[0])\n",
    "                break\n",
    "                       \n",
    "            \n",
    "            all_y.append(y.cpu().detach())\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "                \n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),error_if_nonfinite=False ,max_norm = 5.0 )\n",
    "            optimiser.step()\n",
    "            del x\n",
    "            del y\n",
    "            del y_pred,preds\n",
    "        \n",
    "        #lr_log.append(lr)\n",
    "#        optimiser.sync_lookahead()\n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        all_train_f1.append(train_f1)\n",
    "        if DEBUG:\n",
    "            print(\"train_f1 = \" +str(train_f1))\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"val\")\n",
    "        all_val_f1.append(val_f1)\n",
    "        all_val_loss.append(val_loss)\n",
    "        lr_log.append(scheduler.get_last_lr()[0])\n",
    "        if DEBUG:\n",
    "            print(\"val F1 = \" + str(val_f1))\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        scheduler.step()\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir,  checkpoint_name))\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.model_dir,  checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"current LR = \" , scheduler.get_last_lr()[0])\n",
    "            print(\"********************************\")\n",
    "            from sklearn.metrics import classification_report\n",
    "            test_loss, test_f1 , all_y_test,all_y_pred_test = test_model(model, test_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"test\")\n",
    "            # at times output is not getting printed. Could be due to multi threading and hence adding sleep\n",
    "            time.sleep(2)\n",
    "            sys.stdout.flush()\n",
    "            print(\"test_loss = \",str(test_loss/len(test_loader)))\n",
    "            print(\"test_f1 = \",str(test_f1))\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            \n",
    "            time.sleep(2)\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , classes)\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4ddec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])\n",
    "\n",
    "\n",
    "# apply_augmentation = Compose(transforms=[AddColoredNoise(p = 1) ,TimeInversion( p = 1) ,PolarityInversion(p = 1)])\n",
    "\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3da9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozErrAnalysisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        \n",
    "               \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'],offset, self.audio_df.loc[idx]['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c887aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        temp_id = int(self.audio_df.loc[idx]['id'])\n",
    "        path_var = self.data_dir +\"/\" +str(temp_id)+ str(\".wav\")\n",
    "        entire_aud, inp_rate = torchaudio.load(path_var)\n",
    "        if inp_rate != config.rate:\n",
    "            #print(\" Original sample rate = \" +str(inp_rate)+ \" resampling ...\")\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=entire_aud.dtype)\n",
    "            entire_aud = resampler(entire_aud)\n",
    "            #print(\"processsing file on \" +str(path_var) + \"Post resample shape =  \" + str(entire_aud.shape))\n",
    "        \n",
    "        aud_len = self.audio_df.loc[idx]['length']\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        #print(\"sliced val = \" +str(int((offset+config.min_duration)*config.rate)))\n",
    "        start_pos = int(round(self.audio_df.loc[idx]['start']))\n",
    "        #print(\"start_pos = \" +str(start_pos))\n",
    "        end_pos =  int(round(self.audio_df.loc[idx]['end']))\n",
    "        #print(\"end_pos = \" +str(end_pos))\n",
    "        x = entire_aud[:,start_pos:end_pos]\n",
    "        #print(\"extracted x = \" +str(x))\n",
    "        #print(\"x shape = \" +str(x.shape))\n",
    "        if aud_len < config.min_duration:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            #print(\"padding on \" +str(path_var))\n",
    "            f_out = pad_mean(x)\n",
    "            #print(\"returning from padding  SHape = \" +str(f_out.shape))\n",
    "        else:\n",
    "            f_out = x[0]\n",
    "            f_out = f_out.unsqueeze(0)\n",
    "            \n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            #print(\"offset = \" + str(offset))\n",
    "            #print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(f_out.shape))\n",
    "        \n",
    "        #x_val = x[:,start:end]\n",
    "        #now that we have final x- let's create specgram and add augmentations.\n",
    "                 \n",
    "        return (f_out,self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e67bb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ApplyAug(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.aug_flag_y = transforms.Compose([\n",
    "            transforms.GaussianBlur(3),\n",
    "            transforms.RandomErasing(),\n",
    "            transforms.Normalize(mean=2.7360104e-05, std=.0061507192)\n",
    "        ])\n",
    "        self.aug_flag_n = transforms.Compose([\n",
    "            transforms.Normalize(mean=2.7360104e-05, std=.0061507192)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, spec_gram, aug_flag):\n",
    "        if aug_flag == \"Y\":\n",
    "            rgb_img_auto_aug = self.aug_flag_y(spec_gram)\n",
    "            return rgb_img_auto_aug\n",
    "        else:\n",
    "            img_tensor = self.aug_flag_n(spec_gram)\n",
    "            return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9195a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name ,input_size = 4, hidden_size = 768 , num_classes = 8 , image_size = 224 , batch_size = batch_size):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.25)\n",
    "        self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=True,verbose = False,fmin = 100,fmax = 2000)\n",
    "        #self.linear = nn.Linear(hidden_size , 1024)\n",
    "        self.output = nn.Linear(1000, num_classes)\n",
    "        self.sizer = VT.Resize((image_size,image_size),antialias=True)\n",
    "        self.transform_layer = nn.Sequential(T.FrequencyMasking(freq_mask_param=30), T.TimeMasking(time_mask_param=100))\n",
    "        self.softmax = nn.Softmax( dim = 1)\n",
    "\n",
    "    def forward(self, input_ids,train = True ,attention_mask = False):\n",
    "        #this will hold the output\n",
    "        output_dict = {'probs':None , 'preds':None}\n",
    "        spec_gram = self.spec_layer(input_ids)\n",
    "        #print(\"post NN Audio  shape of spec_gram = \" , spec_gram.shape)\n",
    "        if train== True:\n",
    "            #apply_aug\n",
    "            rand_aug_choice = torch.randint(low=0, high=100, size=(1,1),device = 'cuda',dtype=torch.int32)\n",
    "            #print(\"rand_aug_choice = \",rand_aug_choice)\n",
    "            if rand_aug_choice %3 == 0 :\n",
    "                spec_gram = self.transform_layer(spec_gram)\n",
    "                #print(\"called aug with flag Y and shape post application = \" , spec_gram.shape)\n",
    "            \n",
    "#         # now reshape to image_size\n",
    "        spec_gram = spec_gram.view(batch_size,1,-1)\n",
    "        spec_gram = self.sizer(spec_gram)\n",
    "        spec_gram = spec_gram.unsqueeze(dim = 1)\n",
    "        \n",
    "        #print(\"post sizer shape of spec_gram = \" , spec_gram.shape)\n",
    "        backbone_op = self.backbone(spec_gram)\n",
    "        #print(\"backbone_op shape \",backbone_op.shape)\n",
    "        #linear_output = self.linear(backbone_op_reshp)\n",
    "        #print(\"backbone_op = \", backbone_op)\n",
    "        out_smax = self.softmax(backbone_op)\n",
    "        #print(\"out_smax = \",out_smax)\n",
    "        #print(\"out_smax shape = \" , out_smax.shape)\n",
    "        #out_smax = self.softmax(output)\n",
    "        out = torch.argmax(out_smax , dim = 1)\n",
    "        #print(\"out = \",out)\n",
    "        \n",
    "        output_dict['probs'] = out_smax\n",
    "        output_dict['preds'] = out\n",
    "        #print(\"^^^^^ inside forward^^^^^^^\")\n",
    "        #print(\"output_dict = \", output_dict)\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6be30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(num_values ,df ,classes = classes):\n",
    "    new_df = pd.DataFrame()\n",
    "    for ind in range(len(classes)):\n",
    "        #print(\"ind = \", ind)\n",
    "        op = df[df['specie_ind'] == ind]\n",
    "        #print(\"len op = \", len(op))\n",
    "        op_new = op.sample(n = 1)\n",
    "        #print(\"rand_ind = \" , rand_ind)\n",
    "        #([df1, df2], axis=1)\n",
    "        new_df = pd.concat([op_new,new_df],axis = 0)\n",
    "        #print(\"elem = \" , elem)\n",
    "        #new_list.append(elem)\n",
    "    if len(new_df) < num_values:\n",
    "        diff =  num_values - len(new_df)\n",
    "        #print(\"diff = \", diff)\n",
    "        remaining_elems= df.sample(n = diff)\n",
    "        #print(\"len of remaining elems = \", len(remaining_elems))\n",
    "        new_df = pd.concat([remaining_elems,new_df],axis = 0)\n",
    "        \n",
    "    #print(\"new_df = \", new_df)    \n",
    "    new_df_1 = new_df.reset_index(drop = True)\n",
    "    return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e3850cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221103</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221111</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36486</th>\n",
       "      <td>36486</td>\n",
       "      <td>222614</td>\n",
       "      <td>54</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3</td>\n",
       "      <td>276480.0</td>\n",
       "      <td>291840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36487</th>\n",
       "      <td>36487</td>\n",
       "      <td>222614</td>\n",
       "      <td>55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3</td>\n",
       "      <td>281600.0</td>\n",
       "      <td>296960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36488</th>\n",
       "      <td>36488</td>\n",
       "      <td>222614</td>\n",
       "      <td>56</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3</td>\n",
       "      <td>286720.0</td>\n",
       "      <td>302080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36489</th>\n",
       "      <td>36489</td>\n",
       "      <td>222614</td>\n",
       "      <td>57</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3</td>\n",
       "      <td>291840.0</td>\n",
       "      <td>307200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36490</th>\n",
       "      <td>36490</td>\n",
       "      <td>222614</td>\n",
       "      <td>58</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3</td>\n",
       "      <td>296960.0</td>\n",
       "      <td>307200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36491 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index      id  offset  length  specie_ind     start       end\n",
       "0          0  221103       0    1.92           7       0.0   15360.0\n",
       "1          1  221103       1    1.92           7    5120.0   20480.0\n",
       "2          2  221103       2    1.28           7   10240.0   20480.0\n",
       "3          3  221111       0    1.92           7       0.0   15360.0\n",
       "4          4  221111       1    1.92           7    5120.0   20480.0\n",
       "...      ...     ...     ...     ...         ...       ...       ...\n",
       "36486  36486  222614      54    1.92           3  276480.0  291840.0\n",
       "36487  36487  222614      55    1.92           3  281600.0  296960.0\n",
       "36488  36488  222614      56    1.92           3  286720.0  302080.0\n",
       "36489  36489  222614      57    1.92           3  291840.0  307200.0\n",
       "36490  36490  222614      58    1.28           3  296960.0  307200.0\n",
       "\n",
       "[36491 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3d440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c44d82e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/audio\n"
     ]
    }
   ],
   "source": [
    "print(config.data_dir)\n",
    "if DEBUG:\n",
    "    print(DEBUG)\n",
    "    df_train_offset_db = get_indices(12,df_train_offset)\n",
    "    df_val_offset_db = get_indices(8,df_val_offset)\n",
    "    df_test_offset_db = get_indices(8,df_test_offset)\n",
    "    \n",
    "    train_dataset_db = MozDataset(df_train_offset_db,  config.data_dir, min_length)\n",
    "    val_dataset_db = MozDataset(df_val_offset_db,  config.data_dir, min_length)\n",
    "    test_dataset_db = MozDataset(df_test_offset_db,  config.data_dir, min_length)\n",
    "    #error_dataset = MozErrAnalysisDataset(df_val_offset,  config.data_dir, min_length = config.min_duration)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset_db, num_workers=num_workers,batch_size = batch_size,shuffle = True, pin_memory=True,drop_last = True )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset_db, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory,drop_last = True )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset_db, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,drop_last = True)\n",
    "    #error_loader = torch.utils.data.DataLoader(error_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,drop_last = True)\n",
    "    print(\"Length of train dataset = \" +str(len(train_dataset_db)))\n",
    "    print(\"Length of train loader = \" +str(len(train_loader)))\n",
    "    print(\"Length of val dataset = \" +str(len(val_dataset_db)))\n",
    "    print(\"Length of val loader = \" +str(len(val_loader)))\n",
    "    print(\"Length of test dataset = \" +str(len(test_dataset_db)))\n",
    "    print(\"Length of test loader = \" +str(len(test_loader)))\n",
    "\n",
    "else:\n",
    "    train_dataset = MozDataset(df_train_offset,  config.data_dir, min_length)\n",
    "    val_dataset = MozDataset(df_val_offset,  config.data_dir, min_length)\n",
    "    test_dataset = MozDataset(df_test_offset,  config.data_dir, min_length)\n",
    "    error_dataset = MozErrAnalysisDataset(df_val_offset,  config.data_dir, min_length = config.min_duration)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True, pin_memory=True,drop_last = True )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory,drop_last = True )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,drop_last = True)\n",
    "    error_loader = torch.utils.data.DataLoader(error_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,drop_last = True)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1293b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2db6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b967290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_itr = iter(train_loader)\n",
    "# a,b = train_itr.next()\n",
    "# print(a.shape)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9881b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "#                               window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "#                            sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "# x = spec_layer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7347544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_mod = Model('convnext_small',224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "325e308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mod(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c139c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "547b599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model=MyModel('convnext_xlarge_in22k',224)):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ce104a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath = ../../models/model_e79_2023_04_06_03_52_18.pth\n",
      "model = MyModel(\n",
      "  (backbone): ConvNeXt(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(1, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): ConvNeXtStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.006)\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "            (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.011)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.017)\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.023)\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.029)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.034)\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.040)\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.046)\n",
      "          )\n",
      "          (3): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.051)\n",
      "          )\n",
      "          (4): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.057)\n",
      "          )\n",
      "          (5): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.063)\n",
      "          )\n",
      "          (6): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.069)\n",
      "          )\n",
      "          (7): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.074)\n",
      "          )\n",
      "          (8): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.080)\n",
      "          )\n",
      "          (9): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.086)\n",
      "          )\n",
      "          (10): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "          )\n",
      "          (11): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.097)\n",
      "          )\n",
      "          (12): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.103)\n",
      "          )\n",
      "          (13): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.109)\n",
      "          )\n",
      "          (14): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.114)\n",
      "          )\n",
      "          (15): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.120)\n",
      "          )\n",
      "          (16): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.126)\n",
      "          )\n",
      "          (17): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.131)\n",
      "          )\n",
      "          (18): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.137)\n",
      "          )\n",
      "          (19): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.143)\n",
      "          )\n",
      "          (20): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.149)\n",
      "          )\n",
      "          (21): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.154)\n",
      "          )\n",
      "          (22): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.160)\n",
      "          )\n",
      "          (23): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.166)\n",
      "          )\n",
      "          (24): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.171)\n",
      "          )\n",
      "          (25): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.177)\n",
      "          )\n",
      "          (26): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.183)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)\n",
      "            (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.189)\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)\n",
      "            (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.194)\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)\n",
      "            (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.200)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_pre): Identity()\n",
      "    (head): Sequential(\n",
      "      (global_pool): SelectAdaptivePool2d (pool_type=max, flatten=Identity())\n",
      "      (norm): LayerNorm2d((2048,), eps=1e-06, elementwise_affine=True)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (drop): Dropout(p=0.25, inplace=False)\n",
      "      (fc): Linear(in_features=2048, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (spec_layer): STFT(n_fft=2048, Fourier Kernel size=(1025, 1, 2048), iSTFT=False, trainable=True)\n",
      "  (output): Linear(in_features=1000, out_features=8, bias=True)\n",
      "  (sizer): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "  (transform_layer): Sequential(\n",
      "    (0): FrequencyMasking()\n",
      "    (1): TimeMasking()\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training on cuda:0\n",
      "Training on cuda:0\n",
      "epoch = 0batch = 0 of 570duraation = 0.0321237325668335\n",
      "epoch = 0batch = 500 of 570duraation = 7.188078773021698\n",
      "Epoch: 0, Train Loss: 1.62439352, Train f1: 0.64368310, Val Loss: 0.01025445, Val f1: 0.63130182, overrun_counter -1\n",
      "Saving model to: ../../models/model_e0_2023_04_06_04_55_23.pth\n",
      "Now printing classification rport... \n",
      "current LR =  0.0009990232305719944\n",
      "********************************\n",
      "test_loss =  0.010547788242219264\n",
      "test_f1 =  0.5817682022056316\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.74      0.60      0.66      4145\n",
      "culex pipiens complex       0.45      0.52      0.49      1775\n",
      "           ae aegypti       0.62      0.85      0.72       413\n",
      "       an funestus ss       0.65      0.62      0.64      2186\n",
      "         an squamosus       0.40      0.38      0.39       747\n",
      "          an coustani       0.30      0.46      0.37       207\n",
      "         ma uniformis       0.41      0.58      0.48       578\n",
      "         ma africanus       0.17      0.25      0.20       253\n",
      "\n",
      "             accuracy                           0.57     10304\n",
      "            macro avg       0.47      0.53      0.49     10304\n",
      "         weighted avg       0.60      0.57      0.58     10304\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1batch = 0 of 570duraation = 0.02834237813949585\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_588/2780788782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../models/model_e79_2023_04_06_03_52_18.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_epcoh_99\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_epcoh_99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mclass_weights\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_588/2375298278.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, test_loader, model, classes, class_weights, num_epochs, n_channels)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5.0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforeach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_has_foreach_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'foreach=True was passed, but can\\'t use the foreach API on {device.type} tensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model =MyModel('convnext_xlarge_in22k')\n",
    "model =MyModel('convnext_xlarge_in22k')\n",
    "filepath = \"../../models/model_e79_2023_04_06_03_52_18.pth\"\n",
    "model_epcoh_99 = load_model(filepath,model)\n",
    "model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1 = train_model(train_loader, val_loader, test_loader,model_epcoh_99, classes ,class_weights ,num_epochs = num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141927e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len of all_train_f1 = \"+str(len(all_train_f1)))\n",
    "print(\"len of all_val_f1 = \"+str(len(all_val_f1)))\n",
    "print(\"len of all_val_loss = \"+str(len(all_val_loss)))\n",
    "print(\"len of all_train_loss = \"+str(len(all_train_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7c501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa574808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54869c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359f2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_f1_final =  [v for i, v in enumerate(all_val_f1) if i % 2 == 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fb0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_loss_final =  [v for i, v in enumerate(all_val_loss) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e491b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_f1_final =  [v for i, v in enumerate(all_train_f1) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_train_f1_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({'train_loss':all_train_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5316ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['val_f1'] = all_val_f1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab78a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['train_f1'] = all_train_f1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee13d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['val_loss'] = all_val_loss_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.fillna(0)\n",
    "plot_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.iloc[129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b652df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(plot_df['train_f1','val_f1']);\n",
    "plt.figure(figsize=(8,6)) \n",
    "sns.lineplot(plot_df[['train_f1','val_f1']])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score - ConvNext Small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) \n",
    "sns.lineplot(plot_df[['train_loss','val_loss']])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss - ConvNext Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('file1.csv')\n",
    "plot_df.to_csv(\"plot_df_convNext_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2322d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e70b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b250b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_new = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "val_loader_new = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=2,\n",
    "        num_workers=0, pin_memory=pin_memory  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = df_val_offset\n",
    "model = model_epcoh_10\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "for idx,(x,y) in enumerate(val_dataset):\n",
    "    print(idx)\n",
    "    print(y)\n",
    "    x = x.to('cuda').float()\n",
    "    print(\"x shape = \" +str(x.shape))\n",
    "    #x_new = x.unsqueeze(dim = 1)\n",
    "    print(\"x_new shape = \" +str(x_new.shape))\n",
    "    x_new = x.to('cuda')\n",
    "    y_pred = model(x_new)['prediction']\n",
    "    y_pred_cpu = y_pred.cpu().detach()\n",
    "    preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "    df_erroriloc[idx]['y_hat'] = preds\n",
    "    del x_new\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,15360)\n",
    "x = x.unsqueeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_offset.head()\n",
    "path_temp = \"../data/audio/\"\n",
    "for i,row in df_val_offset.iterrows():\n",
    "    print(\"i = \" +str(i))\n",
    "    print(\"id = \" + str(int(row['id'])))\n",
    "    file = str(int(row['id']))+\".wav\"\n",
    "    print(file)\n",
    "    path = path_temp + file\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "    if inp_rate != config.rate:\n",
    "        import torchaudio.transforms as T\n",
    "        resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[1] < config.rate*min_length:\n",
    "        #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "        f_out = pad_mean(waveform)\n",
    "    else:\n",
    "        f = waveform[0]\n",
    "        f_out = f.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12abce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(df):\n",
    "    \n",
    "    path_name = \"../data/audio/\"\n",
    "    file = df.loc[idx]['id'])}.wav\")\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"\")\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            \n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"returning x of shape ...\" + str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the model checkpoint as a parameter as input\n",
    "# read the val df\n",
    "#get the tensor rep for the offset.\n",
    "#pass it to the model get add get the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    label.append(np.random.rand(9))\n",
    "    pred.append(np.random.rand(9))\n",
    "print(label)\n",
    "print(pred)\n",
    "print(classification_report(label, pred, target_names= classes, labels= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b755c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.randn(4,9)\n",
    "y_pred.shape\n",
    "#y_pred_np = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_np\n",
    "# y_pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c676103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a317e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209aafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(x,y) in enumerate(test_loader):\n",
    "    print(\"idx = \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46400b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa6520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c76aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
