{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4d2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c259812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler = GradScaler()\n",
    "from glob import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC ,AutoProcessor , AutoProcessor,Wav2Vec2Tokenizer\n",
    "from transformers import Wav2Vec2ConformerConfig, Wav2Vec2ConformerModel  \n",
    "# general Py and SK learn\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os , gc\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "import numpy as np\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "## Hugging face\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC ,AutoProcessor , AutoProcessor,Wav2Vec2Tokenizer\n",
    "from transformers import Wav2Vec2ConformerConfig, Wav2Vec2ConformerModel \n",
    "#from transformers import AutoImageProcessor, ConvNextV2ForImageClassification\n",
    "\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb5866",
   "metadata": {},
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, pre_tr_model ,input_size = 4, hidden_size = 768 , num_classes = 8):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.backbone = pre_tr_model\n",
    "        #self.linear = nn.Linear(hidden_size , 1024)\n",
    "        self.output = nn.Linear(1000, num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask = False):\n",
    "        batch_size = 4\n",
    "        output_dict = {'probs':None , 'preds':None}\n",
    "        \n",
    "        backbone_op_reshp = self.backbone(input_ids)\n",
    "        print(backbone_op_reshp.logits.shape)\n",
    "        #linear_output = self.linear(backbone_op_reshp)\n",
    "        #print(linear_output.shape)\n",
    "        output = self.output(backbone_op_reshp['logits'])\n",
    "        #print(output)\n",
    "        #print(\"output shape = \" , output.shape)\n",
    "        out_smax = self.softmax(output)\n",
    "        out = torch.argmax(out_smax , dim = 1)\n",
    "        #print(\"out = \",out)\n",
    "        \n",
    "        output_dict['probs'] = out_smax\n",
    "        output_dict['preds'] = out\n",
    "        #print(\"^^^^^ inside forward^^^^^^^\")\n",
    "        #print(\"output_dict = \", output_dict)\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745e2541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.5\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "print(timm.__version__)\n",
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44a32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f1302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, ConvNextV2ForImageClassification,ASTModel\n",
    "# model = model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e290d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40fbb51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.5\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "print(timm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0898610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.models.list_models('convnext*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38936692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.aug_flag_y = transforms.Compose([\n",
    "            #transforms.GaussianBlur(3),\n",
    "            #transforms.RandomErasing(),\n",
    "            transforms.Normalize(mean=2.7360104e-05, std=.0061507192)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def forward(self, spec_gram):\n",
    "        rgb_img_auto_aug = self.aug_flag_y(spec_gram)\n",
    "        return rgb_img_auto_aug\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4279e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "os.environ['TORCH_HOME'] = '../../pretrained_models'\n",
    "import timm\n",
    "from timm.models.layers import to_2tuple,trunc_normal_\n",
    "\n",
    "# override the timm package to relax the input shape constraint.\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=1, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class ASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The AST model.\n",
    "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
    "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
    "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
    "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
    "    :param input_tdim: the number of time frames of the input spectrogram\n",
    "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
    "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
    "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_dim= 8, fstride=10, tstride=10, input_fdim=128, input_tdim=31, imagenet_pretrain=True, audioset_pretrain=True, model_size='base384', verbose=True):\n",
    "\n",
    "        super(ASTModel, self).__init__()\n",
    "        #assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n",
    "\n",
    "        if verbose == True:\n",
    "            print('---------------AST Model Summary---------------')\n",
    "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
    "        # override timm input shape restriction\n",
    "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
    "\n",
    "        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n",
    "        if audioset_pretrain == False:\n",
    "            if model_size == 'tiny224':\n",
    "                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'small224':\n",
    "                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base224':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base384':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n",
    "            else:\n",
    "                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n",
    "            self.original_num_patches = self.v.patch_embed.num_patches\n",
    "            self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            # automatcially get the intermediate shape\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            # the linear projection layer\n",
    "            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "            if imagenet_pretrain == True:\n",
    "                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n",
    "                new_proj.bias = self.v.patch_embed.proj.bias\n",
    "            self.v.patch_embed.proj = new_proj\n",
    "\n",
    "            # the positional embedding\n",
    "            if imagenet_pretrain == True:\n",
    "                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n",
    "                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n",
    "                # cut (from middle) or interpolate the second dimension of the positional embedding\n",
    "                if t_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n",
    "                # cut (from middle) or interpolate the first dimension of the positional embedding\n",
    "                if f_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
    "                # flatten the positional embedding\n",
    "                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n",
    "                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n",
    "                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "            else:\n",
    "                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
    "                # TODO can use sinusoidal positional embedding instead\n",
    "                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
    "                self.v.pos_embed = new_pos_embed\n",
    "                trunc_normal_(self.v.pos_embed, std=.02)\n",
    "\n",
    "        # now load a model that is pretrained on both ImageNet and AudioSet\n",
    "        elif audioset_pretrain == True:\n",
    "            if audioset_pretrain == True and imagenet_pretrain == False:\n",
    "                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n",
    "            if model_size != 'base384':\n",
    "                raise ValueError('currently only has base384 AudioSet pretrained model.')\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            #if os.path.exists('../../pretrained_models/audioset_10_10_0.4593.pth') == False:\n",
    "                #print(\"path doesn't exist..creating dir..\")\n",
    "                #os.makedirs(\"../../pretrained_model\")\n",
    "                # this model performs 0.4593 mAP on the audioset eval set\n",
    "                #audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
    "                #wget.download(audioset_mdl_url, out='../../pretrained_models/audioset_10_10_0.4593.pth')\n",
    "            sd = torch.load('../../pretrained_models/audioset_0.4593.pth', map_location=device)\n",
    "            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n",
    "            audio_model = torch.nn.DataParallel(audio_model)\n",
    "            audio_model.load_state_dict(sd, strict=False)\n",
    "            self.v = audio_model.module.v\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n",
    "            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n",
    "            if t_dim < 101:\n",
    "                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n",
    "            # otherwise interpolate\n",
    "            else:\n",
    "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n",
    "            if f_dim < 12:\n",
    "                new_pos_embed = new_pos_embed[:, :, 6 - int(f_dim/2): 6 - int(f_dim/2) + f_dim, :]\n",
    "            # otherwise interpolate\n",
    "            elif f_dim > 12:\n",
    "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
    "            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n",
    "            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "\n",
    "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
    "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
    "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        test_out = test_proj(test_input)\n",
    "        f_dim = test_out.shape[2]\n",
    "        t_dim = test_out.shape[3]\n",
    "        return f_dim, t_dim\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        B = x.shape[0]\n",
    "        x = self.v.patch_embed(x)\n",
    "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        x = x + self.v.pos_embed\n",
    "        x = self.v.pos_drop(x)\n",
    "        for blk in self.v.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.v.norm(x)\n",
    "        x = (x[:, 0] + x[:, 1]) / 2\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     input_tdim = 100\n",
    "#     ast_mdl = ASTModel(input_tdim=input_tdim)\n",
    "#     # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n",
    "#     test_input = torch.rand([10, input_tdim, 128])\n",
    "#     test_output = ast_mdl(test_input)\n",
    "#     # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n",
    "#     print(test_output.shape)\n",
    "\n",
    "#     input_tdim = 256\n",
    "#     ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=50, audioset_pretrain=True)\n",
    "#     # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n",
    "#     test_input = torch.rand([10, input_tdim, 128])\n",
    "#     test_output = ast_mdl(test_input)\n",
    "#     # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n",
    "#     print(test_output.shape)\n",
    "# Footer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69bfc238",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d33b6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast_mdl = ASTModel(input_tdim=31,label_dim=8, audioset_pretrain=True)\n",
    "# ast_mdl.to('cuda')\n",
    "# # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n",
    "\n",
    "# spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=128, hop_length=int(config.n_hop),\n",
    "#                               window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "#                            sr=16000, output_format=\"Magnitude\", trainable=True,verbose = False).to('cuda')\n",
    "\n",
    "# spec_gram = spec_layer(x)\n",
    "# print(spec_gram.shape)\n",
    "# #test_input = torch.rand([10, 1025, 31]).to('cuda')\n",
    "# test_output = ast_mdl(spec_gram)\n",
    "# # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n",
    "# print(test_output.shape)\n",
    "# print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1b6d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "585ebf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel_dupe(nn.Module):\n",
    "    def __init__(self, input_size = batch_size, hidden_size = 768 , num_classes = 8 , image_size = 224 , batch_size = batch_size):\n",
    "        super(MyModel_dupe, self).__init__()\n",
    "\n",
    "        self.backbone = ASTModel(input_tdim=31,label_dim=8, audioset_pretrain=True)\n",
    "        #self.mfcc_layer = features.mel.MFCC(sr = 8000)\n",
    "        self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=128, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=16000, output_format=\"Magnitude\", trainable=True,verbose = False)\n",
    "        #self.linear = nn.Linear(hidden_size , 1024)\n",
    "        self.output = nn.Linear(1000, num_classes)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.norm_layer = Normalize()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.transform_layer = nn.Sequential(T.FrequencyMasking(freq_mask_param=30), T.TimeMasking(time_mask_param=100))\n",
    "\n",
    "    def forward(self, input_ids,train = True ,attention_mask = False):\n",
    "        #this will hold the output\n",
    "        output_dict = {'probs':None , 'preds':None}\n",
    "        spec_gram = self.spec_layer(input_ids)\n",
    "        print(\"Input spec_gram = \",spec_gram.shape)\n",
    "        spec_gram = self.norm_layer(spec_gram)\n",
    "        print(\"Post Normalize spec_gram = \",spec_gram.shape)\n",
    "        \n",
    "        if train== True:\n",
    "            #apply_aug\n",
    "            rand_aug_choice = torch.randint(low=0, high=100, size=(1,1),device = 'cuda',dtype=torch.int32)\n",
    "            print(\"rand_aug_choice = \",rand_aug_choice)\n",
    "            if rand_aug_choice %3 == 0 :\n",
    "                spec_gram = self.transform_layer(spec_gram)\n",
    "                print(\"called aug with flag Y and shape post application = \" , spec_gram.shape)\n",
    "        \n",
    "        # now reshape to image_size\n",
    "        #spec_gram = spec_gram.view(batch_size,1,-1)\n",
    "        #spec_gram = spec_gram.unsqueeze(dim = 1)\n",
    "        #print(\"post Unsqueeze shape of spec_gram = \" , spec_gram.shape)\n",
    "        #now make it 3 channel \n",
    "                       \n",
    "        backbone_op = self.backbone(spec_gram)\n",
    "        print(\"backbone_op shape \",backbone_op.shape)\n",
    "        #linear_output = self.linear(backbone_op_reshp)\n",
    "        print(\"backbone_op = \", backbone_op)\n",
    "        out_smax = self.softmax(backbone_op)\n",
    "        print(\"out_smax = \",out_smax)\n",
    "        print(\"out_smax shape = \" , out_smax.shape)\n",
    "        #out_smax = self.softmax(output)\n",
    "        out = torch.argmax(out_smax , dim = 1)\n",
    "        print(\"out = \",out)\n",
    "        \n",
    "        output_dict['probs'] = out_smax\n",
    "        output_dict['preds'] = out\n",
    "        print(\"^^^^^ inside forward^^^^^^^\")\n",
    "        print(\"output_dict = \", output_dict)\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11ec35dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(4,1,15360,device = 'cuda')\n",
    "#convnext_xlarge_in22k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a2269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1a84c23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: True\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyModel_dupe(\n",
       "  (backbone): ASTModel(\n",
       "    (v): DistilledVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (pre_logits): Identity()\n",
       "      (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "      (head_dist): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    )\n",
       "    (mlp_head): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (spec_layer): STFT(n_fft=2048, Fourier Kernel size=(128, 1, 2048), iSTFT=False, trainable=True)\n",
       "  (output): Linear(in_features=1000, out_features=8, bias=True)\n",
       "  (sizer): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "  (norm_layer): Normalize()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (transform_layer): Sequential(\n",
       "    (0): FrequencyMasking()\n",
       "    (1): TimeMasking()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new = MyModel_dupe()\n",
    "model_new.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525839eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b32e479",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Novograd' from 'torch.optim' (/opt/conda/lib/python3.8/site-packages/torch/optim/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3c1ea8009f9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNovograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Novograd' from 'torch.optim' (/opt/conda/lib/python3.8/site-packages/torch/optim/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56c459a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input spec_gram =  torch.Size([4, 128, 31])\n",
      "Post Normalize spec_gram =  torch.Size([4, 128, 31])\n",
      "post Unsqueeze shape of spec_gram =  torch.Size([4, 1, 1, 3968])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 1, 1, 1, 3968]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/977404857.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_70/1605136344.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, train, attention_mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#now make it 3 channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mbackbone_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_gram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"backbone_op shape \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbackbone_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#linear_output = self.linear(backbone_op_reshp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'@autocast() decorator is not supported in script mode'\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_70/3489600532.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mcls_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mdist_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_70/3489600532.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 443\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 1, 1, 1, 3968]"
     ]
    }
   ],
   "source": [
    "model_new(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07959ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f3f7c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 2787])\n"
     ]
    }
   ],
   "source": [
    "x_t = torch.rand(4,1,2787)\n",
    "\n",
    "print(x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f32d5fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4478, 0.0691, 0.7442,  ..., 0.8182, 0.8111, 0.7679]],\n",
       "\n",
       "        [[0.4227, 0.1689, 0.2496,  ..., 0.9209, 0.3005, 0.0188]],\n",
       "\n",
       "        [[0.8573, 0.4481, 0.3623,  ..., 0.4724, 0.1481, 0.2535]],\n",
       "\n",
       "        [[0.7805, 0.0820, 0.5871,  ..., 0.1496, 0.7302, 0.4926]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac578bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input spec gram =  torch.Size([4, 1025, 6])\n",
      "rand_aug_choice =  tensor([[4]], device='cuda:0', dtype=torch.int32)\n",
      "called aug with flag Y and shape post application =  torch.Size([4, 1025, 6])\n",
      "post sizer shape of spec_gram =  torch.Size([4, 1, 224, 224])\n",
      "post 3 channel shape of spec_gram =  torch.Size([4, 3, 224, 224])\n",
      "backbone_op shape  torch.Size([4, 8])\n",
      "backbone_op =  tensor([[-0.1840, -0.0506, -0.1204,  0.2701,  0.3115,  0.1568, -0.1851,  0.2270],\n",
      "        [ 0.1548, -0.1121, -0.1502,  0.1132,  0.1729, -0.0780, -0.6140, -0.1699],\n",
      "        [-0.0498,  0.0370,  0.0282,  0.1059,  0.0224, -0.0708, -0.2141, -0.1978],\n",
      "        [ 0.0329, -0.1281, -0.2116,  0.1523,  0.1960, -0.0830, -0.1006, -0.0238]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "out_smax =  tensor([[0.0967, 0.1105, 0.1031, 0.1523, 0.1588, 0.1360, 0.0966, 0.1459],\n",
      "        [0.1548, 0.1186, 0.1141, 0.1485, 0.1576, 0.1227, 0.0718, 0.1119],\n",
      "        [0.1234, 0.1346, 0.1334, 0.1442, 0.1326, 0.1208, 0.1047, 0.1064],\n",
      "        [0.1307, 0.1113, 0.1024, 0.1473, 0.1539, 0.1164, 0.1144, 0.1235]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "out_smax shape =  torch.Size([4, 8])\n",
      "out =  tensor([4, 4, 3, 4], device='cuda:0')\n",
      "^^^^^ inside forward^^^^^^^\n",
      "output_dict =  {'probs': tensor([[0.0967, 0.1105, 0.1031, 0.1523, 0.1588, 0.1360, 0.0966, 0.1459],\n",
      "        [0.1548, 0.1186, 0.1141, 0.1485, 0.1576, 0.1227, 0.0718, 0.1119],\n",
      "        [0.1234, 0.1346, 0.1334, 0.1442, 0.1326, 0.1208, 0.1047, 0.1064],\n",
      "        [0.1307, 0.1113, 0.1024, 0.1473, 0.1539, 0.1164, 0.1144, 0.1235]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>), 'preds': tensor([4, 4, 3, 4], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "o = model_new(x_t.to('cuda') , train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5cc6724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probs': tensor([[0.0967, 0.1105, 0.1031, 0.1523, 0.1588, 0.1360, 0.0966, 0.1459],\n",
      "        [0.1548, 0.1186, 0.1141, 0.1485, 0.1576, 0.1227, 0.0718, 0.1119],\n",
      "        [0.1234, 0.1346, 0.1334, 0.1442, 0.1326, 0.1208, 0.1047, 0.1064],\n",
      "        [0.1307, 0.1113, 0.1024, 0.1473, 0.1539, 0.1164, 0.1144, 0.1235]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>), 'preds': tensor([4, 4, 3, 4], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77ed8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427c221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1223192",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_tr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-db9a21ba75cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm_o\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpre_tr_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_tr_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "m_o= pre_tr_model(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82990f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a76204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load a grayscale image (shape: [1, height, width])\n",
    "gray_img = torch.rand(1, 256, 256)\n",
    "gray_img = (gray_img - gray_img.min()) / (gray_img.max() - gray_img.min())\n",
    "\n",
    "# Convert the grayscale image to an RGB image (shape: [1, 3, height, width])\n",
    "rgb_img = torch.cat((gray_img, gray_img, gray_img), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "rgb_img.shape\n",
    "img_t= T.ToPILImage(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b38211",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-large-22k-384\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbccf88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "o = processor(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec931727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(o['pixel_values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d16312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8927ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = timm.create_model('convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3226f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
