{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4435fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854dfb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8471b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea22d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_1.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_2.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_3.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_4.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81cca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip /content/humbugdb_neurips_2021_1.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_2.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_3.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_4.zip?download=1 -d '/content/HumBugDB/data/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d97a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch_audiomentations in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.2.2)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.2.6)\n",
      "Requirement already satisfied: librosa>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.8.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.11.0+cu113)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.11.0+cu113)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (21.3)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.24.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.6.3)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.10.3.post1)\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.53.1)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (5.1.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.21.4)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (2.1.9)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (59.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa>=0.6.0->torch_audiomentations) (3.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.26.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in /opt/conda/lib/python3.8/site-packages (from resampy>=0.2.2->librosa>=0.6.0->torch_audiomentations) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torch_audiomentations) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (2.21)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (4.0.1)\n",
      "Requirement already satisfied: primePy>=1.3 in /opt/conda/lib/python3.8/site-packages (from torch-pitch-shift>=1.2.2->torch_audiomentations) (1.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.21.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (6.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.28.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib>=2.2->seaborn) (1.2.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib>=2.2->seaborn) (59.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_audiomentations\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70df488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose,AddBackgroundNoise , AddColoredNoise , ApplyImpulseResponse,PeakNormalization,TimeInversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d59100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db380e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f821cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f49c0f",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01b61c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7faaf95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea453a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "759400ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2413334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa878c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a34304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49236a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dc9aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers=8\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    num_workers=1\n",
    "    \n",
    "     \n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90350d9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f902b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    for _,row in df.iterrows():\n",
    "        if row['length'] > min_length:\n",
    "            step_size = step_frac*min_length\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0, 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "            for i in range(1, int((row['length']-min_length)//step_size)):\n",
    "                audio_offsets.append({'id': row['id'], 'offset':int(min_length+(i*step_size)*config.rate), 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "        elif short_audio:\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f740a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5a33b",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceb38451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "...                     ...                              ...           ...  \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "\n",
       "[6008 rows x 19 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(config.data_df_msc_test)\n",
    "else:\n",
    "    df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ac5f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ae96902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7afd7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5285a1b",
   "metadata": {},
   "source": [
    "At this stage we have all extracted the data with specie information and have encoded the specie encoding in a col = 'specie_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e91d07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data- this is as per the humbug paper\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c809ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "232e4851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879</td>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881</td>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1882</td>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1883</td>\n",
       "      <td>221150</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>4546</td>\n",
       "      <td>222615</td>\n",
       "      <td>30.72</td>\n",
       "      <td>IFA_86_39_3439.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>4547</td>\n",
       "      <td>222585</td>\n",
       "      <td>25.60</td>\n",
       "      <td>IFA_86_40_3440.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>4548</td>\n",
       "      <td>222586</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_10_3450.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>4549</td>\n",
       "      <td>222596</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_11_3451.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>4550</td>\n",
       "      <td>222614</td>\n",
       "      <td>38.40</td>\n",
       "      <td>IFA_87_12_3452.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2288 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      id  length                name  sample_rate record_datetime  \\\n",
       "0      1879  221103    2.56   IFA_17_24_664.wav        44100  30-01-20 00:00   \n",
       "1      1880  221111    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "2      1881  221110    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "3      1882  221149    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "4      1883  221150    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "...     ...     ...     ...                 ...          ...             ...   \n",
       "2283   4546  222615   30.72  IFA_86_39_3439.wav        44100  23-08-20 00:00   \n",
       "2284   4547  222585   25.60  IFA_86_40_3440.wav        44100  23-08-20 00:00   \n",
       "2285   4548  222586   40.90  IFA_87_10_3450.wav        44100  23-08-20 00:00   \n",
       "2286   4549  222596   40.90  IFA_87_11_3451.wav        44100  23-08-20 00:00   \n",
       "2287   4550  222614   38.40  IFA_87_12_3452.wav        44100  23-08-20 00:00   \n",
       "\n",
       "     sound_type         species  gender fed  ... age  method mic_type  \\\n",
       "0      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "1      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "2      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "3      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "4      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "...         ...             ...     ...  ..  ...  ..     ...      ...   \n",
       "2283   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2284   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2285   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2286   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2287   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "\n",
       "     device_type   country            district  province    place  \\\n",
       "0         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "3         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "4         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "...          ...       ...                 ...       ...      ...   \n",
       "2283      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2284      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2285      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2286      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2287      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "\n",
       "     location_type specie_ind  \n",
       "0              cup          7  \n",
       "1              cup          7  \n",
       "2              cup          7  \n",
       "3              cup          0  \n",
       "4              cup          0  \n",
       "...            ...        ...  \n",
       "2283           cup          3  \n",
       "2284           cup          3  \n",
       "2285           cup          3  \n",
       "2286           cup          3  \n",
       "2287           cup          3  \n",
       "\n",
       "[2288 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55cbca9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75b93cf4",
   "metadata": {},
   "source": [
    "### Train-Test split( avoiding sklearn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "713d02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a439514",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841f392",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a9d42e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db8c5c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "565d045f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f364b01",
   "metadata": {},
   "source": [
    "We've confirmed that there is no recording that is common in Train,Test,val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a1904",
   "metadata": {},
   "source": [
    "### Next, we perform \"offsets\", spliting each(long) recording into multiple 1.92 secs chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff5a4d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4912c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train offset = 32239\n",
      "length of test offset = 10087\n",
      "length of val offset = 8692\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1348cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88514e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b233ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e01e9",
   "metadata": {},
   "source": [
    "### Let's check for data leakage in offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b701e762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_test_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44af6f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa1fe87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879e1759",
   "metadata": {},
   "source": [
    "### At this stage we've a dataframe of recordin ids and each row corresponds to a 1.92 secs recording or shorter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab7fea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be01926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32298429 0.56615271 3.6109991  0.61779473 1.98809817 4.24197368\n",
      " 3.08802682 5.57382434]\n"
     ]
    }
   ],
   "source": [
    "#Class imbalance \n",
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48972c8",
   "metadata": {},
   "source": [
    "Let us now get the class distribution for each of the dataframes- train,test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73b02809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = train\n",
      "i = 0\n",
      "12477\n",
      "DF type = train\n",
      "i = 1\n",
      "7118\n",
      "DF type = train\n",
      "i = 2\n",
      "1116\n",
      "DF type = train\n",
      "i = 3\n",
      "6523\n",
      "DF type = train\n",
      "i = 4\n",
      "2027\n",
      "DF type = train\n",
      "i = 5\n",
      "950\n",
      "DF type = train\n",
      "i = 6\n",
      "1305\n",
      "DF type = train\n",
      "i = 7\n",
      "723\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d75ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = Val\n",
      "i = 0\n",
      "3613\n",
      "DF type = Val\n",
      "i = 1\n",
      "1994\n",
      "DF type = Val\n",
      "i = 2\n",
      "230\n",
      "DF type = Val\n",
      "i = 3\n",
      "1855\n",
      "DF type = Val\n",
      "i = 4\n",
      "280\n",
      "DF type = Val\n",
      "i = 5\n",
      "228\n",
      "DF type = Val\n",
      "i = 6\n",
      "426\n",
      "DF type = Val\n",
      "i = 7\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fc70127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = test\n",
      "i = 0\n",
      "4356\n",
      "DF type = test\n",
      "i = 1\n",
      "1879\n",
      "DF type = test\n",
      "i = 2\n",
      "439\n",
      "DF type = test\n",
      "i = 3\n",
      "1959\n",
      "DF type = test\n",
      "i = 4\n",
      "507\n",
      "DF type = test\n",
      "i = 5\n",
      "312\n",
      "DF type = test\n",
      "i = 6\n",
      "441\n",
      "DF type = test\n",
      "i = 7\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790852f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49a4dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7639ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d33b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee84c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9865d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030f505",
   "metadata": {},
   "source": [
    "### Class Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e00eebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce4a007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78155f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221110</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221149</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221144</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id  offset  length  specie_ind\n",
       "0      0  221103       0    2.56           7\n",
       "1      1  221111       0    2.56           7\n",
       "2      2  221110       0    2.56           7\n",
       "3      3  221149       0    2.56           0\n",
       "4      4  221144       0    2.56           1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4987f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_df(loader , trained_model, DEBUG = False):\n",
    "    err_dict = {'id': None,\n",
    "               'label': None,\n",
    "               'offset':None,\n",
    "               'y_hat':None}\n",
    "    model = trained_model\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        all_wav_id = []\n",
    "        all_offset = []\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y,offset,wav_id) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                print(\"y = \" + str(y))\n",
    "                print(\"offset = \" + str(offset))\n",
    "                print(\"wav_id = \" + str(wav_id))\n",
    "                \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            y_pred = model(x)['prediction']\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_wav_id.append(wav_id.cpu().detach())\n",
    "            all_offset.append(offset.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y).numpy()\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        all_wav_id = torch.cat(all_wav_id)\n",
    "        all_offset = torch.cat(all_offset)\n",
    "        \n",
    "        err_dict['id'] = all_wav_id\n",
    "        err_dict['label'] = all_y\n",
    "        err_dict['offset'] = all_offset\n",
    "        err_dict['y_hat'] = all_y_pred\n",
    "        df_err = pd.DataFrame.from_dict(err_dict)\n",
    "        df_err_uniq = df_err[df_err['label']!= df_err['y_hat']]\n",
    "        df_err_uniq.sort_values(by=['id','offset'])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"inside error ....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        #test_loss = test_loss/len(test_loader)\n",
    "        #test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return df_err_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d0526fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            y_pred = model(x)['prediction']\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a99a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )\n",
    "def train_model(train_loader, val_loader,test_loader, model = None,  classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    lr_log = []\n",
    "    for e in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(tk0):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 200 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            global_step += 1\n",
    "            optimiser.zero_grad()\n",
    "            # AMP\n",
    "            with autocast():\n",
    "                y_pred = model(x)['prediction']\n",
    "                preds = torch.argmax(y_pred, axis = 1)\n",
    "                if DEBUG:\n",
    "                    print(\"y_pred  = \" +str(y_pred))\n",
    "                    print(\"preds = \" +str(preds))\n",
    "                loss = criterion(y_pred, y)\n",
    "            loss_scaler(loss, optimiser,parameters=model_parameters(model))\n",
    "            train_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "                \n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            lr_log.append(optimiser.param_groups[0]['lr'])\n",
    "            tk0.set_postfix(training_loss=(train_loss / (batch_i+1)), lr=optimiser.param_groups[0]['lr'])\n",
    "            del x\n",
    "            del y\n",
    "            del y_pred,preds\n",
    "        \n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        if DEBUG:\n",
    "            print(\"train acc = \" +str(train_acc))\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"val\")\n",
    "        if DEBUG:\n",
    "            print(\"val F1 = \" + str(val_f1))\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir,  checkpoint_name))\n",
    "            print('Saving model to:', os.path.join(config.model_dir,  checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"********************************\")\n",
    "            from sklearn.metrics import classification_report\n",
    "            _, _ , all_y_test,all_y_pred_test = test_model(model, test_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"test\")\n",
    "            # at times output is not getting printed. Could be due to multi threading and hence adding sleep\n",
    "            time.sleep(2)\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            time.sleep(2)\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , classes)\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc12835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])\n",
    "\n",
    "\n",
    "# apply_augmentation = Compose(transforms=[AddColoredNoise(p = 1) ,TimeInversion( p = 1) ,PolarityInversion(p = 1)])\n",
    "\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b09209f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "                   \n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "       # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            print(\"offset = \" + str(offset))\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "940244b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozErrAnalysisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'],offset, self.audio_df.loc[idx]['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2c4adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"offset = \" + str(offset))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b677a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aug(x,rate):\n",
    "        apply_augmentation = Compose(transforms=[AddColoredNoise(p = .85) ,TimeInversion( p = .75) ,PolarityInversion(p = .25)])\n",
    "        aug_audio = apply_augmentation(x,sample_rate = rate)\n",
    "        return(aug_audio)\n",
    "    \n",
    "\n",
    "class augment_audio(nn.Module):\n",
    "    \"\"\"This is a class to introduce randomness in the data.\n",
    "    We implement it as a layer in the NN to ensure that it learns from the propertis of the data\"\"\"\n",
    "    def __init__(self , trainable = True, sample_rate = config.rate):\n",
    "        super().__init__()\n",
    "        self.trainable = trainable\n",
    "        self.rate = sample_rate\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "           \n",
    "        if self.trainable:\n",
    "            x = apply_aug(x , self.rate)\n",
    "        else:\n",
    "            x = x\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x.squeeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84ffc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.25)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "                #### end layer freezing\n",
    "        self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.timeMasking = T.TimeMasking(time_mask_param=int(config.win_size*0.4), iid_masks=True)\n",
    "        self.freqMasking = T.FrequencyMasking(freq_mask_param=int((config.NFFT//4)*0.15), iid_masks=True)\n",
    "        self.norm_layer = Normalization(mode='framewise')\n",
    "        self.pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "        self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first compute spectrogram\n",
    "        if DEBUG:\n",
    "            print(\"input shape that goes for augmentation = \" + str(x.squeeze().shape))\n",
    "        spec = self.augment_layer(x.squeeze())\n",
    "        if DEBUG:\n",
    "            print(\"Out put of augment and input shape that goes for STFT = \" + str(spec.shape))\n",
    "        spec = self.spec_layer(x)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        if DEBUG:\n",
    "            print(\"Out put of STFT and input shape that goes for PCEN = \" + str(spec.shape))\n",
    "        spec = self.pcen_layer(spec)\n",
    "        if DEBUG:\n",
    "            print(\"Out put of PCEN and input shape that goes for NORM = \" + str(spec.shape))\n",
    "        spec = self.norm_layer(spec)\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"Out put of NORM and input shape that goes for time mask = \" + str(spec.shape))\n",
    "        spec = self.timeMasking(spec)\n",
    "        if DEBUG:\n",
    "            print(\"Out put of timemask and input shape that goes for freq mask = \" + str(spec.shape))\n",
    "        spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec = self.sizer(spec)\n",
    "        x = spec.unsqueeze(1)\n",
    "        # then repeat channels\n",
    "        if DEBUG:\n",
    "            print(\"Final shape that goes to backbone = \" + str(x.shape))\n",
    "        \n",
    "        x = self.backbone(x)\n",
    "        #print(\"x shape = \" + str(x.shape))\n",
    "        #print(\"x = \" +str(x))\n",
    "        #pred = nn.Softmax(x)\n",
    "        pred = x\n",
    "        #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "        #print(pred)\n",
    "        output = {\"prediction\": pred,\n",
    "                  \"spectrogram\": spec}\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46dda367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/audio\n"
     ]
    }
   ],
   "source": [
    "print(config.data_dir)\n",
    "train_dataset = MozTrainDataset(df_train_offset,  config.data_dir, min_length , transform = None)\n",
    "val_dataset = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "test_dataset = MozTestDataset(df_test_offset,  config.data_dir, min_length)\n",
    "error_dataset = MozErrAnalysisDataset(df_val_offset,  config.data_dir, min_length = config.min_duration)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True\n",
    "    , pin_memory=True )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        num_workers= num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "error_loader = torch.utils.data.DataLoader(\n",
    "        error_dataset, batch_size=batch_size,\n",
    "        num_workers= num_workers, pin_memory=pin_memory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2c6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10c14562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 32239\n",
      "Length of train loader = 504\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train dataset = \" +str(len(train_dataset)))\n",
    "print(\"Length of train loader = \" +str(len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8be8ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_itr = iter(train_loader)\n",
    "# a,b = train_itr.next()\n",
    "# print(a.shape)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "315cfecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "#                               window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "#                            sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "# x = spec_layer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "486e916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_mod = Model('convnext_small',224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1972edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mod(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c268f76",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75c342bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0852 seconds\n"
     ]
    }
   ],
   "source": [
    "def load_model(filepath, model=Model('convnext_small',224)):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be2e333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0764 seconds\n",
      "Filepath = ../../models/model_e0_2022_09_30_04_47_29.pth\n",
      "model = Model(\n",
      "  (backbone): ConvNeXt(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): ConvNeXtStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (3): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (4): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (5): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (6): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (7): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (8): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (9): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (10): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (11): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (12): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (13): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (14): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (15): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (16): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (17): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (18): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (19): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (20): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (21): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (22): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (23): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (24): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (25): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (26): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_pre): Identity()\n",
      "    (head): Sequential(\n",
      "      (global_pool): SelectAdaptivePool2d (pool_type=max, flatten=Identity())\n",
      "      (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Linear(in_features=768, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (spec_layer): STFT(n_fft=2048, Fourier Kernel size=(1025, 1, 2048), iSTFT=False, trainable=True)\n",
      "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (sizer): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "  (timeMasking): TimeMasking()\n",
      "  (freqMasking): FrequencyMasking()\n",
      "  (pcen_layer): PCENTransform()\n",
      "  (augment_layer): augment_audio()\n",
      ")\n",
      "Training on cuda:0\n",
      "Training on cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af82849de9ae425da6e04cbd82f97e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0batch = 0 of 504duraation = 0.06146449247996012\n",
      "epoch = 0batch = 200 of 504duraation = 1.9101621389389039\n",
      "epoch = 0batch = 400 of 504duraation = 3.724101452032725\n",
      "Saving model to: ../../models/model_e0_2022_09_30_05_35_32.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.69      0.79      0.74      4356\n",
      "culex pipiens complex       0.50      0.61      0.55      1879\n",
      "           ae aegypti       0.62      0.38      0.47       439\n",
      "       an funestus ss       0.63      0.50      0.55      1959\n",
      "         an squamosus       0.26      0.13      0.17       507\n",
      "          an coustani       0.39      0.35      0.37       312\n",
      "         ma uniformis       0.33      0.27      0.30       441\n",
      "         ma africanus       0.37      0.29      0.32       194\n",
      "\n",
      "             accuracy                           0.60     10087\n",
      "            macro avg       0.47      0.41      0.43     10087\n",
      "         weighted avg       0.59      0.60      0.59     10087\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.02104289, Train f1: 0.98811897, Val Loss: 0.01367943, Val f1: 0.61646936, overrun_counter -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5538d1441f4dbf860ea2a94c3cf5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1batch = 0 of 504duraation = 0.05213433901468913\n",
      "epoch = 1batch = 200 of 504duraation = 1.9056747436523438\n",
      "epoch = 1batch = 400 of 504duraation = 3.7181677500406902\n",
      "..Overrun....no improvement\n",
      "Epoch: 1, Train Loss: 0.10260711, Train f1: 0.94216048, Val Loss: 0.01177266, Val f1: 0.60687057, overrun_counter 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bc0193995b42339ebcbb7591239109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2batch = 0 of 504duraation = 0.05928674141565959\n",
      "epoch = 2batch = 200 of 504duraation = 1.9004414240519205\n",
      "epoch = 2batch = 400 of 504duraation = 3.704732016722361\n",
      "..Overrun....no improvement\n",
      "Epoch: 2, Train Loss: 0.12401439, Train f1: 0.94048323, Val Loss: 0.01092126, Val f1: 0.60189527, overrun_counter 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0011f74a39654a4f89a567137b57053e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3batch = 0 of 504duraation = 0.06452706654866537\n",
      "epoch = 3batch = 200 of 504duraation = 1.8995152552922567\n",
      "epoch = 3batch = 400 of 504duraation = 3.7137028336524964\n",
      "..Overrun....no improvement\n",
      "Epoch: 3, Train Loss: 0.12573465, Train f1: 0.93725914, Val Loss: 0.01140675, Val f1: 0.59602797, overrun_counter 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aef0a9d062c4d5699837aa56e38cc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4batch = 0 of 504duraation = 0.05569841464360555\n",
      "epoch = 4batch = 200 of 504duraation = 1.9035823106765748\n",
      "epoch = 4batch = 400 of 504duraation = 3.7102176109949747\n",
      "..Overrun....no improvement\n",
      "Epoch: 4, Train Loss: 0.12209116, Train f1: 0.93944734, Val Loss: 0.01098175, Val f1: 0.60088792, overrun_counter 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dc83716d764dbb82f4cacbd57bd079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5batch = 0 of 504duraation = 0.05966392755508423\n",
      "epoch = 5batch = 200 of 504duraation = 1.9038097659746807\n",
      "epoch = 5batch = 400 of 504duraation = 3.713060394922892\n",
      "..Overrun....no improvement\n",
      "Epoch: 5, Train Loss: 0.11981012, Train f1: 0.94078956, Val Loss: 0.01163530, Val f1: 0.59695131, overrun_counter 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b457dff4ed48199ad3994cf2859d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6batch = 0 of 504duraation = 0.061366486549377444\n",
      "epoch = 6batch = 200 of 504duraation = 1.9055846850077311\n",
      "epoch = 6batch = 400 of 504duraation = 3.707613941033681\n",
      "..Overrun....no improvement\n",
      "Epoch: 6, Train Loss: 0.11051871, Train f1: 0.94392110, Val Loss: 0.01240359, Val f1: 0.58330390, overrun_counter 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc97207aaecc47ea8a3e36aa06a75867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7batch = 0 of 504duraation = 0.05653386513392131\n",
      "epoch = 7batch = 200 of 504duraation = 1.8980957786242167\n",
      "epoch = 7batch = 400 of 504duraation = 3.7069493969281515\n",
      "..Overrun....no improvement\n",
      "Epoch: 7, Train Loss: 0.13991337, Train f1: 0.93120219, Val Loss: 0.01164729, Val f1: 0.60402453, overrun_counter 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fc61f91089420e9bf27c1f4fa9b3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8batch = 0 of 504duraation = 0.05993935664494832\n",
      "epoch = 8batch = 200 of 504duraation = 1.8999611496925355\n",
      "epoch = 8batch = 400 of 504duraation = 3.7041282296180724\n",
      "..Overrun....no improvement\n",
      "Epoch: 8, Train Loss: 0.11709215, Train f1: 0.94131660, Val Loss: 0.01155443, Val f1: 0.60245073, overrun_counter 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfe7e8940f74ad6a261f64f04a4ac0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9batch = 0 of 504duraation = 0.058668895562489824\n",
      "epoch = 9batch = 200 of 504duraation = 1.9142412066459655\n",
      "epoch = 9batch = 400 of 504duraation = 3.7255746205647786\n",
      "..Overrun....no improvement\n",
      "Epoch: 9, Train Loss: 0.11162954, Train f1: 0.94370706, Val Loss: 0.01201886, Val f1: 0.59325407, overrun_counter 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df50bd705a94bc2accc0a0fcc2f55a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10batch = 0 of 504duraation = 0.05479330619176229\n",
      "epoch = 10batch = 200 of 504duraation = 1.8992243806521099\n",
      "epoch = 10batch = 400 of 504duraation = 3.703384832541148\n",
      "..Overrun....no improvement\n",
      "Epoch: 10, Train Loss: 0.11446094, Train f1: 0.94363366, Val Loss: 0.01178701, Val f1: 0.59077622, overrun_counter 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93645cf0236749e4a996076b8ebb7f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11batch = 0 of 504duraation = 0.05578242937723796\n",
      "epoch = 11batch = 200 of 504duraation = 1.9203490138053894\n",
      "epoch = 11batch = 400 of 504duraation = 3.723174055417379\n",
      "..Overrun....no improvement\n",
      "Epoch: 11, Train Loss: 0.13010871, Train f1: 0.93783266, Val Loss: 0.01166624, Val f1: 0.60060057, overrun_counter 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76981254ae524669bb77ae97f9a3dcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12batch = 0 of 504duraation = 0.05928200880686442\n",
      "epoch = 12batch = 200 of 504duraation = 1.9030999104181925\n",
      "epoch = 12batch = 400 of 504duraation = 3.7080572724342344\n",
      "..Overrun....no improvement\n",
      "Epoch: 12, Train Loss: 0.12389975, Train f1: 0.94123575, Val Loss: 0.01174562, Val f1: 0.59924684, overrun_counter 11\n"
     ]
    }
   ],
   "source": [
    "model =Model('convnext_small',224)\n",
    "#filepath = \"../../models/model_e0_2022_09_30_04_47_29.pth\"\n",
    "#model_epcoh_15 = load_model(filepath,model)\n",
    "model, lr_log = train_model(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3b4dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x,b = torchaudio.load(\"../../data/audio/221529.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa13f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_new = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "val_loader_new = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=2,\n",
    "        num_workers=0, pin_memory=pin_memory  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33fd322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 15360])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_epcoh_10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_141227/1561255155.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_epcoh_10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_epcoh_10' is not defined"
     ]
    }
   ],
   "source": [
    "val_iter = iter(val_loader_new)\n",
    "x1,y1 = val_iter.next()\n",
    "print(x1.shape)\n",
    "model = model_epcoh_10\n",
    "model.to('cuda')\n",
    "x_g = x1.to('cuda')\n",
    "model(x_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fad85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = df_val_offset\n",
    "model = model_epcoh_10\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "for idx,(x,y) in enumerate(val_dataset):\n",
    "    print(idx)\n",
    "    print(y)\n",
    "    x = x.to('cuda').float()\n",
    "    print(\"x shape = \" +str(x.shape))\n",
    "    #x_new = x.unsqueeze(dim = 1)\n",
    "    print(\"x_new shape = \" +str(x_new.shape))\n",
    "    x_new = x.to('cuda')\n",
    "    y_pred = model(x_new)['prediction']\n",
    "    y_pred_cpu = y_pred.cpu().detach()\n",
    "    preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "    df_erroriloc[idx]['y_hat'] = preds\n",
    "    del x_new\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1260db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,15360)\n",
    "x = x.unsqueeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51653450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_offset.head()\n",
    "path_temp = \"../data/audio/\"\n",
    "for i,row in df_val_offset.iterrows():\n",
    "    print(\"i = \" +str(i))\n",
    "    print(\"id = \" + str(int(row['id'])))\n",
    "    file = str(int(row['id']))+\".wav\"\n",
    "    print(file)\n",
    "    path = path_temp + file\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "    if inp_rate != config.rate:\n",
    "        import torchaudio.transforms as T\n",
    "        resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[1] < config.rate*min_length:\n",
    "        #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "        f_out = pad_mean(waveform)\n",
    "    else:\n",
    "        f = waveform[0]\n",
    "        f_out = f.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81409d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(df):\n",
    "    \n",
    "    path_name = \"../data/audio/\"\n",
    "    file = df.loc[idx]['id'])}.wav\")\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"\")\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            \n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"returning x of shape ...\" + str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the model checkpoint as a parameter as input\n",
    "# read the val df\n",
    "#get the tensor rep for the offset.\n",
    "#pass it to the model get add get the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728199a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    label.append(np.random.rand(9))\n",
    "    pred.append(np.random.rand(9))\n",
    "print(label)\n",
    "print(pred)\n",
    "print(classification_report(label, pred, target_names= classes, labels= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb39c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.randn(4,9)\n",
    "y_pred.shape\n",
    "#y_pred_np = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_np\n",
    "# y_pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(x,y) in enumerate(test_loader):\n",
    "    print(\"idx = \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6c410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63c0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c719f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
