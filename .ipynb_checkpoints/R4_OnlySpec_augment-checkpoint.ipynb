{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48704b88",
   "metadata": {},
   "source": [
    "### GOAL- check whether Label smoothing helps in improving perf\n",
    "- base R4_LabelSmoothing.ipynb\n",
    "- add Batch norm 2D\n",
    "- Explore Exponential learning rate\n",
    "- RandomChoice on Time and Freq Masking and RandomApply on all 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06bd3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325b3c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29289fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af6d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_1.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_2.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_3.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_4.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81565d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip /content/humbugdb_neurips_2021_1.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_2.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_3.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_4.zip?download=1 -d '/content/HumBugDB/data/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68091e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch_audiomentations in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (2.0.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (2.0.0)\n",
      "Requirement already satisfied: librosa>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.8.1)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.2.4)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.2.7)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.21.4)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.10.3.post1)\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.53.1)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (2.1.9)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.1.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.5.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (5.1.0)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.24.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (59.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa>=0.6.0->torch_audiomentations) (3.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.26.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in /opt/conda/lib/python3.8/site-packages (from resampy>=0.2.2->librosa>=0.6.0->torch_audiomentations) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torch_audiomentations) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (2.21)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (3.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (11.7.4.91)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (3.4.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (4.0.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (11.7.91)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torch_audiomentations) (0.37.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7.0->torch_audiomentations) (3.26.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7.0->torch_audiomentations) (16.0.1)\n",
      "Requirement already satisfied: primePy>=1.3 in /opt/conda/lib/python3.8/site-packages (from torch-pitch-shift>=1.2.2->torch_audiomentations) (1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch>=1.7.0->torch_audiomentations) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2021.10.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch>=1.7.0->torch_audiomentations) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.21.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (6.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.28.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib!=3.6.1,>=3.1->seaborn) (1.2.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib!=3.6.1,>=3.1->seaborn) (59.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_audiomentations\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aca7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose,AddBackgroundNoise , AddColoredNoise , ApplyImpulseResponse,PeakNormalization,TimeInversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540dcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c8194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e60839",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5b12c",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b00fc1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f9ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac003a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5993b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dda27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f5bf71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features,Spectrogram\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3539e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64ca05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0f1e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers= 8\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    num_workers=1\n",
    "    \n",
    "     \n",
    "\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1704f1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3300c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd076215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    stride = step_frac*min_length\n",
    "#     print(\"min_length = \" +str(min_length))\n",
    "#     print(\"step_frac = \" +str(step_frac))\n",
    "#     print(\"stride = \" +str(stride))\n",
    "    for _,row in df.iterrows():\n",
    "        #processed_data keeps track of the tensor_values processed thus far\n",
    "        if row['length'] > min_length:\n",
    "            processed_data = 0\n",
    "            #total_data is the total tensor present in the audio\n",
    "            total_data = config.rate*row['length']\n",
    "            #print(\"********\")\n",
    "            count = 0\n",
    "            #print(\"count = \" +str(count))\n",
    "            #print(\"id = \" + str(row['id']) + \" duration = \" +str(row['length']) + \"total x vals = \" + str(total_data))\n",
    "            inner_loop_flag = False\n",
    "            #print(\"going into the inner loop to offset....\")\n",
    "            while(processed_data < total_data):\n",
    "                #print(\"inside inner loop.....\")\n",
    "                start = count*stride*config.rate\n",
    "                #now find out the row_len\n",
    "                if total_data - (start + min_length*config.rate) >= 0:\n",
    "                    #print(\"full chunk \")\n",
    "                    row_len = min_length\n",
    "                    end = start + row_len*config.rate\n",
    "                    audio_offsets.append({'id':row['id'], 'offset':count, 'length': row_len,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "                    #print(\"count = \" +str(count) + \"offset = \" +str(count) + \"start = \" +str(start) + \"end = \" +str(end))\n",
    "                    #print(\"for count.... = \" + str(count) + \"processed data = \" +str(processed_data))\n",
    "                    count+=1\n",
    "                    processed_data = (count*stride)*config.rate\n",
    "                    \n",
    "                else:\n",
    "                    inner_loop_flag = True\n",
    "                    break\n",
    "                    \n",
    "                                                       \n",
    "            #for processing residual data\n",
    "            if(inner_loop_flag):\n",
    "                #print(\"processing residual ....processed \" +str(processed_data) + \" of \" + str(total_data))\n",
    "                start = count*stride*config.rate\n",
    "                resid_durn = round((total_data - processed_data)/config.rate,2)\n",
    "                end = total_data\n",
    "                #print(\"for...\" + str(row['id']) + \" adding the residual data in the data frame with duration = \" + str(resid_durn))\n",
    "                audio_offsets.append({'id':row['id'], 'offset':count, 'length':resid_durn ,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "            \n",
    "        elif short_audio:\n",
    "            start = 0\n",
    "            end = row['length']*config.rate\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind'],'start':0 , 'end':end})\n",
    "    return pd.DataFrame(audio_offsets)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ab32b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4914a229",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "854fa98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "...                     ...                              ...           ...  \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "\n",
       "[6008 rows x 19 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if DEBUG:\n",
    "#     df = pd.read_csv(config.data_df_msc_test)\n",
    "# else:\n",
    "df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe3f9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce84d293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "090dde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2999bf2",
   "metadata": {},
   "source": [
    "At this stage we have all extracted the data with specie information and have encoded the specie encoding in a col = 'specie_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842a031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b070e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data- this is as per the humbug paper\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54abb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89b83562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879</td>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881</td>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1882</td>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1883</td>\n",
       "      <td>221150</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>4546</td>\n",
       "      <td>222615</td>\n",
       "      <td>30.72</td>\n",
       "      <td>IFA_86_39_3439.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>4547</td>\n",
       "      <td>222585</td>\n",
       "      <td>25.60</td>\n",
       "      <td>IFA_86_40_3440.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>4548</td>\n",
       "      <td>222586</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_10_3450.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>4549</td>\n",
       "      <td>222596</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_11_3451.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>4550</td>\n",
       "      <td>222614</td>\n",
       "      <td>38.40</td>\n",
       "      <td>IFA_87_12_3452.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2288 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      id  length                name  sample_rate record_datetime  \\\n",
       "0      1879  221103    2.56   IFA_17_24_664.wav        44100  30-01-20 00:00   \n",
       "1      1880  221111    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "2      1881  221110    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "3      1882  221149    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "4      1883  221150    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "...     ...     ...     ...                 ...          ...             ...   \n",
       "2283   4546  222615   30.72  IFA_86_39_3439.wav        44100  23-08-20 00:00   \n",
       "2284   4547  222585   25.60  IFA_86_40_3440.wav        44100  23-08-20 00:00   \n",
       "2285   4548  222586   40.90  IFA_87_10_3450.wav        44100  23-08-20 00:00   \n",
       "2286   4549  222596   40.90  IFA_87_11_3451.wav        44100  23-08-20 00:00   \n",
       "2287   4550  222614   38.40  IFA_87_12_3452.wav        44100  23-08-20 00:00   \n",
       "\n",
       "     sound_type         species  gender fed  ... age  method mic_type  \\\n",
       "0      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "1      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "2      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "3      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "4      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "...         ...             ...     ...  ..  ...  ..     ...      ...   \n",
       "2283   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2284   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2285   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2286   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2287   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "\n",
       "     device_type   country            district  province    place  \\\n",
       "0         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "3         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "4         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "...          ...       ...                 ...       ...      ...   \n",
       "2283      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2284      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2285      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2286      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2287      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "\n",
       "     location_type specie_ind  \n",
       "0              cup          7  \n",
       "1              cup          7  \n",
       "2              cup          7  \n",
       "3              cup          0  \n",
       "4              cup          0  \n",
       "...            ...        ...  \n",
       "2283           cup          3  \n",
       "2284           cup          3  \n",
       "2285           cup          3  \n",
       "2286           cup          3  \n",
       "2287           cup          3  \n",
       "\n",
       "[2288 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a9fed28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "import seaborn as sns\n",
    "sns.countplot(x = 'species', data = df_all , ax = ax , hue = 'gender',palette='dark')\n",
    "#ax.bar_label(ax.containers[0])\n",
    "#ax.bar_label(ax.containers[-1], fmt='Count:\\n%.2f', label_type='center')\n",
    "plt.xticks(rotation=90 )\n",
    "plt.title(\"Distribution of Species \")\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('figure', titlesize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc2565",
   "metadata": {},
   "source": [
    "### Train-Test split( avoiding sklearn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bde964dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d69311c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e925b09",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90a69037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65052256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cd09c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4950b",
   "metadata": {},
   "source": [
    "We've confirmed that there is no recording that is common in Train,Test,val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759277c",
   "metadata": {},
   "source": [
    "### Next, we perform \"offsets\", spliting each(long) recording into multiple 1.92 secs chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e929915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b3596c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train offset = 35043\n",
      "length of test offset = 11043\n",
      "length of val offset = 9466\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ece22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701b05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da3eef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95906561",
   "metadata": {},
   "source": [
    "### Let's check for data leakage in offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79506cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_test_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e99ca915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffda40ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325c0b2",
   "metadata": {},
   "source": [
    "### At this stage we've a dataframe of recordin ids and each row corresponds to a 1.92 secs recording or shorter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f475cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6859bd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32524317 0.56223527 3.61119126 0.62576786 1.97670352 4.1207667\n",
      " 3.00231323 5.25855342]\n"
     ]
    }
   ],
   "source": [
    "#Class imbalance \n",
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a83f12d",
   "metadata": {},
   "source": [
    "Let us now get the class distribution for each of the dataframes- train,test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102e520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b31bc5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = train\n",
      "i = 0\n",
      "13468\n",
      "DF type = train\n",
      "i = 1\n",
      "7791\n",
      "DF type = train\n",
      "i = 2\n",
      "1213\n",
      "DF type = train\n",
      "i = 3\n",
      "7000\n",
      "DF type = train\n",
      "i = 4\n",
      "2216\n",
      "DF type = train\n",
      "i = 5\n",
      "1063\n",
      "DF type = train\n",
      "i = 6\n",
      "1459\n",
      "DF type = train\n",
      "i = 7\n",
      "833\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3cb5d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = Val\n",
      "i = 0\n",
      "3908\n",
      "DF type = Val\n",
      "i = 1\n",
      "2175\n",
      "DF type = Val\n",
      "i = 2\n",
      "264\n",
      "DF type = Val\n",
      "i = 3\n",
      "1981\n",
      "DF type = Val\n",
      "i = 4\n",
      "318\n",
      "DF type = Val\n",
      "i = 5\n",
      "260\n",
      "DF type = Val\n",
      "i = 6\n",
      "478\n",
      "DF type = Val\n",
      "i = 7\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20b0e821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = test\n",
      "i = 0\n",
      "4717\n",
      "DF type = test\n",
      "i = 1\n",
      "2105\n",
      "DF type = test\n",
      "i = 2\n",
      "484\n",
      "DF type = test\n",
      "i = 3\n",
      "2114\n",
      "DF type = test\n",
      "i = 4\n",
      "559\n",
      "DF type = test\n",
      "i = 5\n",
      "350\n",
      "DF type = test\n",
      "i = 6\n",
      "491\n",
      "DF type = test\n",
      "i = 7\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c771ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6feb2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17714901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a77f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e3b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1cb7e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e3d3c",
   "metadata": {},
   "source": [
    "### Class Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f3ba516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aug(spec_gram , aug_flag = \"Y\", newsize = (1025, 31)):\n",
    "        \n",
    "        from torchvision.transforms.autoaugment import AutoAugmentPolicy\n",
    "        import torchvision.transforms as transforms\n",
    "        \n",
    "        aug_flag_y = transforms.Compose([\n",
    "            transforms.GaussianBlur(3),#image conversion\n",
    "            transforms.RandomErasing(),\n",
    "            transforms.Normalize(mean = 2.7360104e-05 , std = .0061507192)\n",
    "            ])\n",
    "        \n",
    "        \n",
    "        aug_flag_n = transforms.Compose([\n",
    "            transforms.Normalize(mean = 2.7360104e-05 , std = .0061507192)\n",
    "            ])\n",
    "        \n",
    "            \n",
    "        \n",
    "        if (aug_flag == \"Y\"):\n",
    "            rgb_img_auto_aug = aug_flag_y(spec_gram)\n",
    "            #print(\"type(rgb_img_auto_aug) = \" +str(type(rgb_img_auto_aug)))\n",
    "            \n",
    "            return rgb_img_auto_aug\n",
    "        else:\n",
    "            \n",
    "            img_tensor = aug_flag_n(spec_gram)\n",
    "            #print(\"type(img_tensor) = \" +str(type(img_tensor)))\n",
    "            return img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb56be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a6af3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8b48ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221103</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221111</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id  offset  length  specie_ind    start      end\n",
       "0      0  221103       0    1.92           7      0.0  15360.0\n",
       "1      1  221103       1    1.92           7   5120.0  20480.0\n",
       "2      2  221103       2    1.28           7  10240.0  20480.0\n",
       "3      3  221111       0    1.92           7      0.0  15360.0\n",
       "4      4  221111       1    1.92           7   5120.0  20480.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2470711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    softmax = nn.Softmax()\n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            y_pred = model(x,train = False)['prediction']\n",
    "            #y_pred_smax = softmax(y_pred)\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "453fdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(y_true, smoothing=0.1):\n",
    "    # Convert one-hot encoded true labels to soft labels\n",
    "    confidence = 1.0 - smoothing\n",
    "    y_true = y_true.unsqueeze(dim = 1)\n",
    "    print(\" INSIDE SMOOTH _Lables y_true shape = \",y_true.shape)\n",
    "    label_shape = torch.Size((y_true.size(0), y_true.size(1)))\n",
    "    #print(\"label shape = \",label_shape.shape)\n",
    "    print(\"y_true = \",y_true)\n",
    "    y_true = torch.empty(label_shape).fill_(smoothing / (y_true.size(1) - 1)).to(y_true.device)\n",
    "    y_true.scatter_(1, y_true.argmax(dim=1, keepdim=True), confidence)\n",
    "    print(\"post smoothing , y true = \",y_true)\n",
    "    return y_true.squeeze(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c681b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )\n",
    "def train_model(train_loader, val_loader,test_loader, model = None,  classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion_1 = nn.CrossEntropyLoss(weight=weights_adj,label_smoothing=.1)\n",
    "    criterion_2 = nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    base_optimiser = timm.optim.AdamP(model.parameters(), lr=.00015)\n",
    "    look_optimiser = timm.optim.Lookahead(base_optimiser)\n",
    "    cooldown_epoch = 50\n",
    "    \n",
    "    #optimiser = timm.optim.AdamW(model.parameters(), lr=config_pytorch.lr)\n",
    "    #timm.optim.Lookahead(optimiser, alpha=0.5, k=6)\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(base_optimiser, t_initial= num_epochs,lr_min= 15e-7,warmup_t = 3,warmup_lr_init= 5e-5,noise_std=.075)\n",
    "    \n",
    "    \n",
    "    #optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    softmax = nn.Softmax()\n",
    "    all_train_f1 = []\n",
    "    all_val_f1 = []\n",
    "    \n",
    "    lr_log = []\n",
    "    for e in range(num_epochs + cooldown_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        #tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(train_loader):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 200 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. x device = \" +str(x.device))\n",
    "                \n",
    "            \n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            x_sum = torch.sum(x,axis = 1)\n",
    "            x_sum.unsqueeze(dim = 1)\n",
    "                                  \n",
    "            with autocast():\n",
    "                y_pred = model(x,train = True)['prediction']\n",
    "                #y_pred_smax = softmax(y_pred)\n",
    "                preds = torch.argmax(y_pred, axis = 1)\n",
    "                if e < 20 :\n",
    "                    loss = criterion_1(y_pred, y)\n",
    "                else:\n",
    "                    loss = criterion_2(y_pred, y)\n",
    "            \n",
    "            if DEBUG:\n",
    "                    print(\"y_pred  = \" +str(y_pred))\n",
    "                    print(\"preds = \" +str(preds))\n",
    "                   \n",
    "            train_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "                \n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            base_optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),error_if_nonfinite=False ,max_norm = 1.0 )\n",
    "            base_optimiser.step()\n",
    "            del x\n",
    "            del y\n",
    "            del y_pred,preds\n",
    "        \n",
    "        #lr_log.append(lr)\n",
    "        look_optimiser.sync_lookahead()\n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        all_train_f1.append(train_f1)\n",
    "        if DEBUG:\n",
    "            print(\"train acc = \" +str(train_acc))\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"val\")\n",
    "        all_val_f1.append(val_f1)\n",
    "        all_val_loss.append(val_loss)\n",
    "        if DEBUG:\n",
    "            print(\"val F1 = \" + str(val_f1))\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir,  checkpoint_name))\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.model_dir,  checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"********************************\")\n",
    "            from sklearn.metrics import classification_report\n",
    "            _, _ , all_y_test,all_y_pred_test = test_model(model, test_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"test\")\n",
    "            # at times output is not getting printed. Could be due to multi threading and hence adding sleep\n",
    "            time.sleep(2)\n",
    "            sys.stdout.flush()\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            time.sleep(2)\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , classes)\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        scheduler.step(e+1)\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4b7b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])\n",
    "\n",
    "\n",
    "# apply_augmentation = Compose(transforms=[AddColoredNoise(p = 1) ,TimeInversion( p = 1) ,PolarityInversion(p = 1)])\n",
    "\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6be28352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "                   \n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "       # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            print(\"offset = \" + str(offset))\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        x_val = x[:,offset:int(offset+config.rate*self.min_length)]\n",
    "        #now that we have final x- let's create specgram and add augmentations.\n",
    "        \n",
    "             \n",
    "            \n",
    "        return (x_val,self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd9bc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"offset = \" + str(offset))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "            \n",
    "        x_val = x[:,offset:int(offset+config.rate*self.min_length)]\n",
    "            \n",
    "        return (x_val,self.audio_df.loc[idx]['specie_ind'])\n",
    "        \n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8130ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c353af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/audio\n"
     ]
    }
   ],
   "source": [
    "print(config.data_dir)\n",
    "train_dataset = MozTrainDataset(df_train_offset,  config.data_dir, min_length , transform = None)\n",
    "val_dataset = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "test_dataset = MozTestDataset(df_test_offset,  config.data_dir, min_length)\n",
    "#error_dataset = MozErrAnalysisDataset(df_val_offset,  config.data_dir, min_length = config.min_duration)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True\n",
    "    , pin_memory=True )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,shuffle = True\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        num_workers= num_workers, pin_memory=pin_memory,shuffle = True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce6d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5913f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 35043\n",
      "Length of train loader = 8761\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train dataset = \" +str(len(train_dataset)))\n",
    "print(\"Length of train loader = \" +str(len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a224699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_itr = iter(train_loader)\n",
    "# a,b = train_itr.next()\n",
    "# print(a.shape)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10ad6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "#                               window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "#                            sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "# x = spec_layer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c40f240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_mod = Model('convnext_small',224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e2469eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mod(a)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "938a1746",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4b4de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.25)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "                #### end layer freezing\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.spec_layer = Spectrogram.MelSpectrogram(sr = 8000 , n_fft=int(config.NFFT),verbose = False,trainable_mel=False, trainable_STFT= False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features= 1)\n",
    "        #self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "        \n",
    "    def forward(self, x,train = True):\n",
    "        # first compute spectrogram\n",
    "        spec_gram = self.spec_layer(x)\n",
    "        #print(\"post spec gram shape = \",spec_gram.shape)\n",
    "        spec_gram = self.batch_norm(spec_gram.unsqueeze(dim = 1))\n",
    "        #print(\"post norm shape = \",spec_gram.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if train == True:\n",
    "                #generate a random number and if condition is met apply aug\n",
    "                ta_transformations_rndm_choice = VT.RandomChoice([AT.FrequencyMasking(freq_mask_param=100),AT.TimeMasking(time_mask_param=50)], p=[.6, .6])\n",
    "                ta_transformations_rndm_apply = VT.RandomApply([AT.FrequencyMasking(freq_mask_param=100),AT.TimeMasking(time_mask_param=50)],p = .15)\n",
    "                spec_gram = ta_transformations_rndm_choice(spec_gram)\n",
    "                spec_gram = ta_transformations_rndm_apply(spec_gram)\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        x = self.sizer(spec_gram.squeeze(dim = 1))\n",
    "        #print(\"post sizer shape = \",x.shape)\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "        #print(\"post unsqueeze shape = \",x.shape)\n",
    "        \n",
    "        # then repeat channels\n",
    "        del spec_gram\n",
    "        if DEBUG:\n",
    "            print(\"Final shape that goes to backbone = \" + str(x.shape))\n",
    "        if torch.sum(x) == 0:\n",
    "            print(\"ZERO INPUT in forward\")\n",
    "            x  = x+torch.tensor(1e-6)\n",
    "                \n",
    "        x = self.backbone(x)\n",
    "        #print(\"x shape = \" + str(x.shape))\n",
    "        #print(\"x = \" +str(x))\n",
    "        #pred = nn.Softmax(x)\n",
    "        pred = x\n",
    "        #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "        #print(pred)\n",
    "        output = {\"prediction\": pred }\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "400313ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "idx = 3255\n",
      "offset = 16\n",
      "shape of x post augmentation = torch.Size([1, 204320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 24768\n",
      "offset = 19\n",
      "shape of x post augmentation = torch.Size([1, 347680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 11125\n",
      "offset = 33\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 16553\n",
      "offset = 22\n",
      "shape of x post augmentation = torch.Size([1, 163840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 27213\n",
      "offset = 33\n",
      "shape of x post augmentation = torch.Size([1, 225280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 33612\n",
      "offset = 17\n",
      "shape of x post augmentation = torch.Size([1, 122880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 13785\n",
      "offset = 25\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 34608\n",
      "offset = 53\n",
      "shape of x post augmentation = torch.Size([1, 450080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32658\n",
      "offset = 45\n",
      "shape of x post augmentation = torch.Size([1, 327680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 18751\n",
      "offset = 22\n",
      "shape of x post augmentation = torch.Size([1, 163840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 19847\n",
      "offset = 18\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 33599\n",
      "offset = 4\n",
      "shape of x post augmentation = torch.Size([1, 122880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 0\n",
      "epoch = 0batch = 0 of 8761duraation = 0.0023659666379292807\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.2471,  0.1251, -0.0332, -0.2930,  0.1309, -0.5552,  0.4675,  0.5688],\n",
      "        [ 0.1847,  0.2041,  0.0233, -0.7573,  0.3572, -0.3755,  0.0328,  0.0811],\n",
      "        [ 0.1349,  0.0142,  0.0127, -0.5278, -0.0063, -0.6724,  0.4824,  0.2913],\n",
      "        [-0.0075, -0.3079,  0.1128, -0.4382, -0.2086, -0.3472,  0.0310, -0.0699]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([7, 4, 6, 2], device='cuda:0')\n",
      "batch_ind = 0\n",
      "y_pred_cpu = tensor([[ 0.2471,  0.1251, -0.0332, -0.2930,  0.1309, -0.5552,  0.4675,  0.5688],\n",
      "        [ 0.1847,  0.2041,  0.0233, -0.7573,  0.3572, -0.3755,  0.0328,  0.0811],\n",
      "        [ 0.1349,  0.0142,  0.0127, -0.5278, -0.0063, -0.6724,  0.4824,  0.2913],\n",
      "        [-0.0075, -0.3079,  0.1128, -0.4382, -0.2086, -0.3472,  0.0310, -0.0699]],\n",
      "       dtype=torch.float16)\n",
      "idx = 25796\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 654880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 24044\n",
      "offset = 45\n",
      "shape of x post augmentation = torch.Size([1, 348160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 10106\n",
      "offset = 17\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 4590\n",
      "offset = 10\n",
      "shape of x post augmentation = torch.Size([1, 163840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 1\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[ 0.2856,  0.0835,  0.1429, -0.0836, -0.1300, -0.2981, -0.0503,  0.0804],\n",
      "        [ 0.1182, -0.1127,  0.0277, -0.1589, -0.0132, -0.2976,  0.2141,  0.1021],\n",
      "        [ 0.2947,  0.0941,  0.0016, -0.0518,  0.0209, -0.1693,  0.0095,  0.1167],\n",
      "        [ 0.3010,  0.0520,  0.0898, -0.1515,  0.1005, -0.2578, -0.0153,  0.2040]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 6, 0, 0], device='cuda:0')\n",
      "batch_ind = 1\n",
      "y_pred_cpu = tensor([[ 0.2856,  0.0835,  0.1429, -0.0836, -0.1300, -0.2981, -0.0503,  0.0804],\n",
      "        [ 0.1182, -0.1127,  0.0277, -0.1589, -0.0132, -0.2976,  0.2141,  0.1021],\n",
      "        [ 0.2947,  0.0941,  0.0016, -0.0518,  0.0209, -0.1693,  0.0095,  0.1167],\n",
      "        [ 0.3010,  0.0520,  0.0898, -0.1515,  0.1005, -0.2578, -0.0153,  0.2040]],\n",
      "       dtype=torch.float16)\n",
      "idx = 12702\n",
      "offset = 0\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 34909\n",
      "offset = 49\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 30467\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 245280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 5017\n",
      "offset = 44\n",
      "shape of x post augmentation = torch.Size([1, 265760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 2\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.2462, -0.0604,  0.0750, -0.9189,  0.1228, -0.3188,  0.4668,  0.4255],\n",
      "        [ 0.3582,  0.0978, -0.2830, -0.1619,  0.4146, -0.8091,  0.4019, -0.1340],\n",
      "        [ 0.3909, -0.1121,  0.0498, -0.5537, -0.0809, -0.5669,  0.8560, -0.1412],\n",
      "        [ 0.3193,  0.0725, -0.1111, -0.3337,  0.0527, -0.5679,  0.1113,  0.3323]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([6, 4, 6, 7], device='cuda:0')\n",
      "batch_ind = 2\n",
      "y_pred_cpu = tensor([[ 0.2462, -0.0604,  0.0750, -0.9189,  0.1228, -0.3188,  0.4668,  0.4255],\n",
      "        [ 0.3582,  0.0978, -0.2830, -0.1619,  0.4146, -0.8091,  0.4019, -0.1340],\n",
      "        [ 0.3909, -0.1121,  0.0498, -0.5537, -0.0809, -0.5669,  0.8560, -0.1412],\n",
      "        [ 0.3193,  0.0725, -0.1111, -0.3337,  0.0527, -0.5679,  0.1113,  0.3323]],\n",
      "       dtype=torch.float16)\n",
      "idx = 17816\n",
      "offset = 16\n",
      "shape of x post augmentation = torch.Size([1, 307200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 8716\n",
      "offset = 32\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 12922\n",
      "offset = 18\n",
      "shape of x post augmentation = torch.Size([1, 306720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 16280\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 286240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 3\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.3796, -0.0208, -0.5474, -0.2922,  0.2496,  0.3882,  0.0699, -0.3096],\n",
      "        [-0.3496, -0.1401, -0.0139, -0.2472, -0.0807, -0.3545,  0.7563,  0.0047],\n",
      "        [ 0.2705, -0.2065, -0.2200, -0.3865,  0.0479, -0.1378,  0.0396, -0.0936],\n",
      "        [ 0.1101, -0.1989, -0.1494, -0.3467,  0.2915, -0.3296,  0.0688, -0.2834]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([5, 6, 0, 4], device='cuda:0')\n",
      "batch_ind = 3\n",
      "y_pred_cpu = tensor([[ 0.3796, -0.0208, -0.5474, -0.2922,  0.2496,  0.3882,  0.0699, -0.3096],\n",
      "        [-0.3496, -0.1401, -0.0139, -0.2472, -0.0807, -0.3545,  0.7563,  0.0047],\n",
      "        [ 0.2705, -0.2065, -0.2200, -0.3865,  0.0479, -0.1378,  0.0396, -0.0936],\n",
      "        [ 0.1101, -0.1989, -0.1494, -0.3467,  0.2915, -0.3296,  0.0688, -0.2834]],\n",
      "       dtype=torch.float16)\n",
      "idx = 8910\n",
      "offset = 39\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 17953\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 26657\n",
      "offset = 54\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 34080\n",
      "offset = 11\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 4\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.1993, -0.4485, -0.5381,  0.2595,  0.2047, -0.1852,  0.4312, -0.4419],\n",
      "        [-0.1470,  0.3989, -0.2247, -0.5415,  0.0822, -0.1658,  0.1011,  0.1937],\n",
      "        [ 0.0670,  0.1203,  0.1035, -0.0084,  0.1206, -0.3286, -0.1110,  0.1359],\n",
      "        [ 0.2200, -0.2212, -0.1995,  0.1149,  0.2378, -0.2460, -0.2983,  0.0680]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([6, 1, 7, 4], device='cuda:0')\n",
      "batch_ind = 4\n",
      "y_pred_cpu = tensor([[ 0.1993, -0.4485, -0.5381,  0.2595,  0.2047, -0.1852,  0.4312, -0.4419],\n",
      "        [-0.1470,  0.3989, -0.2247, -0.5415,  0.0822, -0.1658,  0.1011,  0.1937],\n",
      "        [ 0.0670,  0.1203,  0.1035, -0.0084,  0.1206, -0.3286, -0.1110,  0.1359],\n",
      "        [ 0.2200, -0.2212, -0.1995,  0.1149,  0.2378, -0.2460, -0.2983,  0.0680]],\n",
      "       dtype=torch.float16)\n",
      "idx = 15023\n",
      "offset = 19\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 22636\n",
      "offset = 1\n",
      "shape of x post augmentation = torch.Size([1, 40960])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 17399\n",
      "offset = 8\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 11026\n",
      "offset = 18\n",
      "shape of x post augmentation = torch.Size([1, 224800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 5\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[-0.0573,  0.2957,  0.0158,  0.0806, -0.0012, -0.1037,  0.2340,  0.1099],\n",
      "        [-0.0762,  0.1238,  0.0400,  0.0299, -0.1553, -0.1449,  0.0172,  0.1157],\n",
      "        [-0.1560,  0.4404,  0.1655,  0.2949, -0.0954, -0.2500, -0.0634,  0.0333],\n",
      "        [ 0.1451,  0.0804,  0.0080,  0.0179, -0.0732, -0.0672, -0.0028,  0.1260]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 1, 0], device='cuda:0')\n",
      "batch_ind = 5\n",
      "y_pred_cpu = tensor([[-0.0573,  0.2957,  0.0158,  0.0806, -0.0012, -0.1037,  0.2340,  0.1099],\n",
      "        [-0.0762,  0.1238,  0.0400,  0.0299, -0.1553, -0.1449,  0.0172,  0.1157],\n",
      "        [-0.1560,  0.4404,  0.1655,  0.2949, -0.0954, -0.2500, -0.0634,  0.0333],\n",
      "        [ 0.1451,  0.0804,  0.0080,  0.0179, -0.0732, -0.0672, -0.0028,  0.1260]],\n",
      "       dtype=torch.float16)\n",
      "idx = 9426\n",
      "offset = 15\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 29121\n",
      "offset = 35\n",
      "shape of x post augmentation = torch.Size([1, 286720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 117\n",
      "offset = 1\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 11310\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 61440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 6\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.3445, -0.0793, -0.3389,  0.0085,  0.1235, -0.3105,  0.0955, -0.3330],\n",
      "        [ 0.1921, -0.2151, -0.0647,  0.0133,  0.1021, -0.1702, -0.0164, -0.0410],\n",
      "        [ 0.2181,  0.0808, -0.1694,  0.0803, -0.2108, -0.1500, -0.1619,  0.0042],\n",
      "        [ 0.2947, -0.2040,  0.0843, -0.2439,  0.3096,  0.1654,  0.0511, -0.0656]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 0, 0, 4], device='cuda:0')\n",
      "batch_ind = 6\n",
      "y_pred_cpu = tensor([[ 0.3445, -0.0793, -0.3389,  0.0085,  0.1235, -0.3105,  0.0955, -0.3330],\n",
      "        [ 0.1921, -0.2151, -0.0647,  0.0133,  0.1021, -0.1702, -0.0164, -0.0410],\n",
      "        [ 0.2181,  0.0808, -0.1694,  0.0803, -0.2108, -0.1500, -0.1619,  0.0042],\n",
      "        [ 0.2947, -0.2040,  0.0843, -0.2439,  0.3096,  0.1654,  0.0511, -0.0656]],\n",
      "       dtype=torch.float16)\n",
      "idx = 2236\n",
      "offset = 18\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 3946\n",
      "offset = 21\n",
      "shape of x post augmentation = torch.Size([1, 245280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 20761\n",
      "offset = 11\n",
      "shape of x post augmentation = torch.Size([1, 122400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 15905\n",
      "offset = 15\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 7\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[-0.1186, -0.0600, -0.1416, -0.0094,  0.0828, -0.5889, -0.3030,  0.0151],\n",
      "        [ 0.2106,  0.3855, -0.0114,  0.3015, -0.1320, -0.2886, -0.2325,  0.0410],\n",
      "        [-0.1573, -0.0020, -0.1788, -0.0447,  0.2329, -0.3135,  0.3108,  0.0706],\n",
      "        [ 0.0342,  0.2625, -0.0595, -0.2216, -0.5132, -0.4050,  0.0485,  0.0448]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([4, 1, 6, 1], device='cuda:0')\n",
      "batch_ind = 7\n",
      "y_pred_cpu = tensor([[-0.1186, -0.0600, -0.1416, -0.0094,  0.0828, -0.5889, -0.3030,  0.0151],\n",
      "        [ 0.2106,  0.3855, -0.0114,  0.3015, -0.1320, -0.2886, -0.2325,  0.0410],\n",
      "        [-0.1573, -0.0020, -0.1788, -0.0447,  0.2329, -0.3135,  0.3108,  0.0706],\n",
      "        [ 0.0342,  0.2625, -0.0595, -0.2216, -0.5132, -0.4050,  0.0485,  0.0448]],\n",
      "       dtype=torch.float16)\n",
      "idx = 17374\n",
      "offset = 9\n",
      "shape of x post augmentation = torch.Size([1, 142880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 5169\n",
      "offset = 2\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 26900\n",
      "offset = 27\n",
      "shape of x post augmentation = torch.Size([1, 163840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 14538\n",
      "offset = 46\n",
      "shape of x post augmentation = torch.Size([1, 265760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 8\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.5884, -0.2157, -0.5356,  0.1526,  0.2893, -1.0430, -0.0549,  0.8477],\n",
      "        [ 0.3689,  0.1208, -0.1842,  0.1578,  0.2666, -0.5229, -0.0513, -0.2173],\n",
      "        [ 0.1995, -0.2091, -0.1936, -0.1203,  0.1396, -0.1594,  0.1520, -0.1318],\n",
      "        [ 0.0398,  0.0359, -0.2744,  0.1936,  0.4915, -0.4487, -0.0360, -0.0432]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([7, 0, 0, 4], device='cuda:0')\n",
      "batch_ind = 8\n",
      "y_pred_cpu = tensor([[ 0.5884, -0.2157, -0.5356,  0.1526,  0.2893, -1.0430, -0.0549,  0.8477],\n",
      "        [ 0.3689,  0.1208, -0.1842,  0.1578,  0.2666, -0.5229, -0.0513, -0.2173],\n",
      "        [ 0.1995, -0.2091, -0.1936, -0.1203,  0.1396, -0.1594,  0.1520, -0.1318],\n",
      "        [ 0.0398,  0.0359, -0.2744,  0.1936,  0.4915, -0.4487, -0.0360, -0.0432]],\n",
      "       dtype=torch.float16)\n",
      "idx = 23193\n",
      "offset = 14\n",
      "shape of x post augmentation = torch.Size([1, 204320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 4631\n",
      "offset = 18\n",
      "shape of x post augmentation = torch.Size([1, 307200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 18291\n",
      "offset = 3\n",
      "shape of x post augmentation = torch.Size([1, 143360])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 31053\n",
      "offset = 32\n",
      "shape of x post augmentation = torch.Size([1, 265760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 9\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.0172, -0.0147,  0.0256,  0.0656,  0.2463,  0.0056,  0.0602,  0.0724],\n",
      "        [-0.2686, -0.2751, -0.3772,  0.0374, -0.0983, -0.0605, -0.0534,  0.2075],\n",
      "        [-0.2957, -0.0403, -0.3196,  0.3357,  0.1373, -0.4341,  0.0861,  0.0924],\n",
      "        [-0.0217, -0.1337, -0.2686, -0.1372,  0.3379, -0.5923,  0.0390, -0.0645]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([4, 7, 3, 4], device='cuda:0')\n",
      "batch_ind = 9\n",
      "y_pred_cpu = tensor([[ 0.0172, -0.0147,  0.0256,  0.0656,  0.2463,  0.0056,  0.0602,  0.0724],\n",
      "        [-0.2686, -0.2751, -0.3772,  0.0374, -0.0983, -0.0605, -0.0534,  0.2075],\n",
      "        [-0.2957, -0.0403, -0.3196,  0.3357,  0.1373, -0.4341,  0.0861,  0.0924],\n",
      "        [-0.0217, -0.1337, -0.2686, -0.1372,  0.3379, -0.5923,  0.0390, -0.0645]],\n",
      "       dtype=torch.float16)\n",
      "idx = 21528\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 347680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 22562\n",
      "offset = 40\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 12508\n",
      "offset = 0\n",
      "shape of x post augmentation = torch.Size([1, 61440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 5362\n",
      "offset = 5\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 10\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.3411, -0.2600, -0.2128,  0.3611,  0.2343, -0.2102,  0.2168, -0.0707],\n",
      "        [ 0.0777,  0.1222,  0.0042, -0.0914, -0.2097, -0.0753, -0.1599,  0.3921],\n",
      "        [ 0.2441,  0.0875,  0.0524,  0.2178,  0.3479, -0.4343,  0.0320, -0.2275],\n",
      "        [ 0.1835, -0.1016, -0.1053,  0.3528,  0.0511, -0.4858, -0.2325,  0.1421]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([3, 7, 4, 3], device='cuda:0')\n",
      "batch_ind = 10\n",
      "y_pred_cpu = tensor([[ 0.3411, -0.2600, -0.2128,  0.3611,  0.2343, -0.2102,  0.2168, -0.0707],\n",
      "        [ 0.0777,  0.1222,  0.0042, -0.0914, -0.2097, -0.0753, -0.1599,  0.3921],\n",
      "        [ 0.2441,  0.0875,  0.0524,  0.2178,  0.3479, -0.4343,  0.0320, -0.2275],\n",
      "        [ 0.1835, -0.1016, -0.1053,  0.3528,  0.0511, -0.4858, -0.2325,  0.1421]],\n",
      "       dtype=torch.float16)\n",
      "idx = 34408\n",
      "offset = 40\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 1352\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 28811\n",
      "offset = 5\n",
      "shape of x post augmentation = torch.Size([1, 347680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 6379\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 224800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 11\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[-0.0576,  0.3579, -0.0383,  0.0804, -0.1776, -0.3452,  0.0966,  0.1671],\n",
      "        [ 0.2844,  0.2847, -0.0899,  0.0532,  0.0898,  0.0014,  0.1317,  0.0730],\n",
      "        [-0.0032,  0.6597,  0.1312,  0.2144,  0.0745, -0.2744, -0.0359, -0.1160],\n",
      "        [ 0.0721,  0.1990, -0.0521,  0.0833,  0.0748, -0.6528, -0.0906,  0.0140]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 1, 1], device='cuda:0')\n",
      "batch_ind = 11\n",
      "y_pred_cpu = tensor([[-0.0576,  0.3579, -0.0383,  0.0804, -0.1776, -0.3452,  0.0966,  0.1671],\n",
      "        [ 0.2844,  0.2847, -0.0899,  0.0532,  0.0898,  0.0014,  0.1317,  0.0730],\n",
      "        [-0.0032,  0.6597,  0.1312,  0.2144,  0.0745, -0.2744, -0.0359, -0.1160],\n",
      "        [ 0.0721,  0.1990, -0.0521,  0.0833,  0.0748, -0.6528, -0.0906,  0.0140]],\n",
      "       dtype=torch.float16)\n",
      "idx = 20718\n",
      "offset = 15\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 17538\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 9654\n",
      "offset = 17\n",
      "shape of x post augmentation = torch.Size([1, 306720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32041\n",
      "offset = 35\n",
      "shape of x post augmentation = torch.Size([1, 450080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 12\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[-0.1052,  0.0905, -0.0145,  0.0428,  0.6973, -0.2428, -0.5425, -0.2546],\n",
      "        [-0.0638, -0.0056, -0.0746, -0.0511,  0.2208, -0.4089, -0.2705, -0.2053],\n",
      "        [ 0.1188, -0.0062, -0.1663,  0.1335,  0.0877, -0.2771, -0.2627,  0.1921],\n",
      "        [ 0.1831,  0.2476, -0.1731,  0.2803,  0.1399, -0.5796, -0.5469,  0.0768]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([4, 4, 7, 3], device='cuda:0')\n",
      "batch_ind = 12\n",
      "y_pred_cpu = tensor([[-0.1052,  0.0905, -0.0145,  0.0428,  0.6973, -0.2428, -0.5425, -0.2546],\n",
      "        [-0.0638, -0.0056, -0.0746, -0.0511,  0.2208, -0.4089, -0.2705, -0.2053],\n",
      "        [ 0.1188, -0.0062, -0.1663,  0.1335,  0.0877, -0.2771, -0.2627,  0.1921],\n",
      "        [ 0.1831,  0.2476, -0.1731,  0.2803,  0.1399, -0.5796, -0.5469,  0.0768]],\n",
      "       dtype=torch.float16)\n",
      "idx = 15567\n",
      "offset = 34\n",
      "shape of x post augmentation = torch.Size([1, 368160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 22200\n",
      "offset = 43\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 26528\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 31383\n",
      "offset = 3\n",
      "shape of x post augmentation = torch.Size([1, 430080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 13\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.0489, -0.0199,  0.2639,  0.2988,  0.3491, -0.3628, -0.1815, -0.0851],\n",
      "        [-0.0869,  0.0864, -0.0759, -0.1181,  0.1360, -0.5610, -0.5967, -0.0179],\n",
      "        [ 0.3972,  0.0853, -0.0508,  0.0811,  0.2805, -0.5674, -0.5415,  0.0544],\n",
      "        [-0.0703, -0.0600,  0.0729, -0.2289,  0.3723, -0.1947, -0.3086,  0.2195]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([4, 4, 0, 4], device='cuda:0')\n",
      "batch_ind = 13\n",
      "y_pred_cpu = tensor([[ 0.0489, -0.0199,  0.2639,  0.2988,  0.3491, -0.3628, -0.1815, -0.0851],\n",
      "        [-0.0869,  0.0864, -0.0759, -0.1181,  0.1360, -0.5610, -0.5967, -0.0179],\n",
      "        [ 0.3972,  0.0853, -0.0508,  0.0811,  0.2805, -0.5674, -0.5415,  0.0544],\n",
      "        [-0.0703, -0.0600,  0.0729, -0.2289,  0.3723, -0.1947, -0.3086,  0.2195]],\n",
      "       dtype=torch.float16)\n",
      "idx = 12950\n",
      "offset = 46\n",
      "shape of x post augmentation = torch.Size([1, 306720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 15448\n",
      "offset = 12\n",
      "shape of x post augmentation = torch.Size([1, 204320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32540\n",
      "offset = 71\n",
      "shape of x post augmentation = torch.Size([1, 429600])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 19739\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 14\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[-0.0901,  0.6318, -0.2260,  0.2189,  0.1186, -0.1755, -0.2153, -0.0886],\n",
      "        [ 0.1826,  0.4385, -0.2268,  0.3096,  0.0043, -0.3826,  0.0742,  0.0822],\n",
      "        [ 0.2788,  0.3101, -0.0129,  0.1170, -0.0649, -0.4397, -0.1787, -0.0496],\n",
      "        [ 0.0516,  0.2218,  0.0886,  0.0290, -0.0544, -0.2656, -0.1387, -0.0873]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 1, 1], device='cuda:0')\n",
      "batch_ind = 14\n",
      "y_pred_cpu = tensor([[-0.0901,  0.6318, -0.2260,  0.2189,  0.1186, -0.1755, -0.2153, -0.0886],\n",
      "        [ 0.1826,  0.4385, -0.2268,  0.3096,  0.0043, -0.3826,  0.0742,  0.0822],\n",
      "        [ 0.2788,  0.3101, -0.0129,  0.1170, -0.0649, -0.4397, -0.1787, -0.0496],\n",
      "        [ 0.0516,  0.2218,  0.0886,  0.0290, -0.0544, -0.2656, -0.1387, -0.0873]],\n",
      "       dtype=torch.float16)\n",
      "idx = 16711\n",
      "offset = 2\n",
      "shape of x post augmentation = torch.Size([1, 163840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 12646\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 245280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 1609\n",
      "offset = 58\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 30597\n",
      "offset = 51\n",
      "shape of x post augmentation = torch.Size([1, 368160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 15\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[-0.0651,  0.1790, -0.1002,  0.0203,  0.2284, -0.4482,  0.1632,  0.1670],\n",
      "        [ 0.0719, -0.0889, -0.0381,  0.1484,  0.3962, -0.6245, -0.2957, -0.2485],\n",
      "        [ 0.0477,  0.1326, -0.3481,  0.1320,  0.1361, -0.6953, -0.2510, -0.0142],\n",
      "        [ 0.0594,  0.3992, -0.1471,  0.2415, -0.0511, -0.4641,  0.0185, -0.1713]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([4, 4, 4, 1], device='cuda:0')\n",
      "batch_ind = 15\n",
      "y_pred_cpu = tensor([[-0.0651,  0.1790, -0.1002,  0.0203,  0.2284, -0.4482,  0.1632,  0.1670],\n",
      "        [ 0.0719, -0.0889, -0.0381,  0.1484,  0.3962, -0.6245, -0.2957, -0.2485],\n",
      "        [ 0.0477,  0.1326, -0.3481,  0.1320,  0.1361, -0.6953, -0.2510, -0.0142],\n",
      "        [ 0.0594,  0.3992, -0.1471,  0.2415, -0.0511, -0.4641,  0.0185, -0.1713]],\n",
      "       dtype=torch.float16)\n",
      "idx = 8262\n",
      "offset = 46\n",
      "shape of x post augmentation = torch.Size([1, 265760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 14360\n",
      "offset = 38\n",
      "shape of x post augmentation = torch.Size([1, 388640])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 10784\n",
      "offset = 11\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32832\n",
      "offset = 28\n",
      "shape of x post augmentation = torch.Size([1, 368160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 16\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[-0.1050,  0.2991,  0.0798,  0.2681, -0.2444, -0.6313, -0.0078, -0.2122],\n",
      "        [ 0.2722,  0.2849,  0.0046,  0.0532, -0.1366, -0.4656, -0.1542,  0.0740],\n",
      "        [ 0.6489,  0.4285, -0.1582,  0.1600,  0.0399, -0.2749, -0.1342,  0.1990],\n",
      "        [ 0.1436,  0.3857, -0.0182, -0.1985, -0.1860, -0.3975, -0.2964, -0.1565]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 0, 1], device='cuda:0')\n",
      "batch_ind = 16\n",
      "y_pred_cpu = tensor([[-0.1050,  0.2991,  0.0798,  0.2681, -0.2444, -0.6313, -0.0078, -0.2122],\n",
      "        [ 0.2722,  0.2849,  0.0046,  0.0532, -0.1366, -0.4656, -0.1542,  0.0740],\n",
      "        [ 0.6489,  0.4285, -0.1582,  0.1600,  0.0399, -0.2749, -0.1342,  0.1990],\n",
      "        [ 0.1436,  0.3857, -0.0182, -0.1985, -0.1860, -0.3975, -0.2964, -0.1565]],\n",
      "       dtype=torch.float16)\n",
      "idx = 15928\n",
      "offset = 38\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 4533\n",
      "offset = 4\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 33752\n",
      "offset = 33\n",
      "shape of x post augmentation = torch.Size([1, 184320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 29123\n",
      "offset = 37\n",
      "shape of x post augmentation = torch.Size([1, 286720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 17\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[ 1.8835e-01,  5.2832e-01,  2.6459e-02,  8.1787e-02, -5.6702e-02,\n",
      "         -3.9771e-01, -1.1969e-01, -7.9803e-03],\n",
      "        [ 2.5952e-01,  4.0430e-01,  1.9629e-01,  1.0452e-02, -2.3901e-01,\n",
      "         -3.0347e-01, -1.8152e-01, -5.5408e-04],\n",
      "        [ 1.6467e-01,  6.9678e-01, -1.8251e-04, -7.9407e-02, -2.9492e-01,\n",
      "         -2.6611e-01,  3.5667e-03, -2.3340e-01],\n",
      "        [ 1.5906e-01,  8.4717e-01,  1.9446e-01,  1.7297e-01, -5.1660e-01,\n",
      "         -5.2930e-01, -2.9761e-01, -1.2189e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 1, 1], device='cuda:0')\n",
      "batch_ind = 17\n",
      "y_pred_cpu = tensor([[ 1.8835e-01,  5.2832e-01,  2.6459e-02,  8.1787e-02, -5.6702e-02,\n",
      "         -3.9771e-01, -1.1969e-01, -7.9803e-03],\n",
      "        [ 2.5952e-01,  4.0430e-01,  1.9629e-01,  1.0452e-02, -2.3901e-01,\n",
      "         -3.0347e-01, -1.8152e-01, -5.5408e-04],\n",
      "        [ 1.6467e-01,  6.9678e-01, -1.8251e-04, -7.9407e-02, -2.9492e-01,\n",
      "         -2.6611e-01,  3.5667e-03, -2.3340e-01],\n",
      "        [ 1.5906e-01,  8.4717e-01,  1.9446e-01,  1.7297e-01, -5.1660e-01,\n",
      "         -5.2930e-01, -2.9761e-01, -1.2189e-01]], dtype=torch.float16)\n",
      "idx = 34154\n",
      "offset = 46\n",
      "shape of x post augmentation = torch.Size([1, 286240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 3036\n",
      "offset = 16\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 17200\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 33688\n",
      "offset = 10\n",
      "shape of x post augmentation = torch.Size([1, 184320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 18\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.1976,  0.1890,  0.0785,  0.0518,  0.0563, -0.6914,  0.1954, -0.2091],\n",
      "        [ 0.3303, -0.2607, -0.1652,  0.1315,  0.1105, -0.5508, -0.2888, -0.2484],\n",
      "        [-0.2834, -0.0119,  0.0112, -0.0638, -0.1305, -0.9224, -0.1064, -0.0584],\n",
      "        [ 0.5981,  0.4607, -0.1543,  0.2803,  0.3096, -0.7603, -0.4639, -0.4417]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 0, 2, 0], device='cuda:0')\n",
      "batch_ind = 18\n",
      "y_pred_cpu = tensor([[ 0.1976,  0.1890,  0.0785,  0.0518,  0.0563, -0.6914,  0.1954, -0.2091],\n",
      "        [ 0.3303, -0.2607, -0.1652,  0.1315,  0.1105, -0.5508, -0.2888, -0.2484],\n",
      "        [-0.2834, -0.0119,  0.0112, -0.0638, -0.1305, -0.9224, -0.1064, -0.0584],\n",
      "        [ 0.5981,  0.4607, -0.1543,  0.2803,  0.3096, -0.7603, -0.4639, -0.4417]],\n",
      "       dtype=torch.float16)\n",
      "idx = 21144\n",
      "offset = 63\n",
      "shape of x post augmentation = torch.Size([1, 388640])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 13571\n",
      "offset = 9\n",
      "shape of x post augmentation = torch.Size([1, 224800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32484\n",
      "offset = 15\n",
      "shape of x post augmentation = torch.Size([1, 429600])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 22174\n",
      "offset = 17\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 19\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[ 0.3188,  0.6099, -0.0421, -0.2131, -0.3813, -0.1129, -0.3032, -0.4258],\n",
      "        [ 0.0818,  0.4370,  0.0598,  0.0632, -0.3967, -0.4211, -0.2107,  0.1572],\n",
      "        [ 0.2238,  0.2294,  0.0614, -0.0046, -0.3472, -0.5371, -0.3809, -0.1588],\n",
      "        [ 0.0796,  0.5986, -0.1572,  0.1162, -0.3599, -0.0718, -0.4214, -0.2793]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 1, 1], device='cuda:0')\n",
      "batch_ind = 19\n",
      "y_pred_cpu = tensor([[ 0.3188,  0.6099, -0.0421, -0.2131, -0.3813, -0.1129, -0.3032, -0.4258],\n",
      "        [ 0.0818,  0.4370,  0.0598,  0.0632, -0.3967, -0.4211, -0.2107,  0.1572],\n",
      "        [ 0.2238,  0.2294,  0.0614, -0.0046, -0.3472, -0.5371, -0.3809, -0.1588],\n",
      "        [ 0.0796,  0.5986, -0.1572,  0.1162, -0.3599, -0.0718, -0.4214, -0.2793]],\n",
      "       dtype=torch.float16)\n",
      "idx = 22542\n",
      "offset = 20\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 25345\n",
      "offset = 22\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 7313\n",
      "offset = 42\n",
      "shape of x post augmentation = torch.Size([1, 286240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 4091\n",
      "offset = 33\n",
      "shape of x post augmentation = torch.Size([1, 225280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 20\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[ 2.6562e-01,  6.2158e-01,  2.4612e-02, -6.3049e-02, -1.8518e-01,\n",
      "         -2.6978e-01, -2.1277e-01, -2.6199e-02],\n",
      "        [ 2.0422e-01,  1.0518e+00, -2.2266e-01, -2.5296e-04, -2.2461e-01,\n",
      "         -1.2598e-01, -3.3008e-01, -1.0193e-01],\n",
      "        [ 5.4932e-02,  4.5850e-01,  2.3596e-01,  2.3572e-01, -3.2202e-01,\n",
      "         -3.0688e-01, -3.4180e-01, -7.7087e-02],\n",
      "        [ 2.7588e-01,  4.5972e-01, -4.3152e-02, -6.9153e-02, -3.6401e-01,\n",
      "         -3.8867e-01, -1.3672e-01, -1.0974e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 1, 1], device='cuda:0')\n",
      "batch_ind = 20\n",
      "y_pred_cpu = tensor([[ 2.6562e-01,  6.2158e-01,  2.4612e-02, -6.3049e-02, -1.8518e-01,\n",
      "         -2.6978e-01, -2.1277e-01, -2.6199e-02],\n",
      "        [ 2.0422e-01,  1.0518e+00, -2.2266e-01, -2.5296e-04, -2.2461e-01,\n",
      "         -1.2598e-01, -3.3008e-01, -1.0193e-01],\n",
      "        [ 5.4932e-02,  4.5850e-01,  2.3596e-01,  2.3572e-01, -3.2202e-01,\n",
      "         -3.0688e-01, -3.4180e-01, -7.7087e-02],\n",
      "        [ 2.7588e-01,  4.5972e-01, -4.3152e-02, -6.9153e-02, -3.6401e-01,\n",
      "         -3.8867e-01, -1.3672e-01, -1.0974e-01]], dtype=torch.float16)\n",
      "idx = 23273\n",
      "offset = 5\n",
      "shape of x post augmentation = torch.Size([1, 368160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 9086\n",
      "offset = 11\n",
      "shape of x post augmentation = torch.Size([1, 143360])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 15663\n",
      "offset = 60\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 16678\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 21\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.4436,  0.1998, -0.1198, -0.0317, -0.0319, -0.2178, -0.3745,  0.1837],\n",
      "        [ 0.3469,  0.2153, -0.1604,  0.1582, -0.0561,  0.0253, -0.0271,  0.0565],\n",
      "        [ 0.4546,  0.2808,  0.3364, -0.0848, -0.1356, -0.3459, -0.4976, -0.1522],\n",
      "        [ 0.4424,  0.2949,  0.0732, -0.1265,  0.2117, -0.3835, -0.2964, -0.1447]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 0, 0, 0], device='cuda:0')\n",
      "batch_ind = 21\n",
      "y_pred_cpu = tensor([[ 0.4436,  0.1998, -0.1198, -0.0317, -0.0319, -0.2178, -0.3745,  0.1837],\n",
      "        [ 0.3469,  0.2153, -0.1604,  0.1582, -0.0561,  0.0253, -0.0271,  0.0565],\n",
      "        [ 0.4546,  0.2808,  0.3364, -0.0848, -0.1356, -0.3459, -0.4976, -0.1522],\n",
      "        [ 0.4424,  0.2949,  0.0732, -0.1265,  0.2117, -0.3835, -0.2964, -0.1447]],\n",
      "       dtype=torch.float16)\n",
      "idx = 22210\n",
      "offset = 53\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 28741\n",
      "offset = 35\n",
      "shape of x post augmentation = torch.Size([1, 265760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 8217\n",
      "offset = 1\n",
      "shape of x post augmentation = torch.Size([1, 265760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 9897\n",
      "offset = 15\n",
      "shape of x post augmentation = torch.Size([1, 204320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 22\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.3577,  0.4231, -0.0538,  0.1113, -0.0947, -0.5737, -0.2727, -0.0101],\n",
      "        [ 0.4409,  0.5132,  0.1188,  0.1824, -0.1959, -0.4827, -0.1855, -0.0278],\n",
      "        [ 0.1307,  0.6172, -0.1417,  0.3291, -0.4729, -0.5205, -0.1680,  0.0556],\n",
      "        [ 0.2717,  0.8013, -0.0903, -0.2146, -0.5566, -0.2810, -0.5156, -0.0709]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 1, 1], device='cuda:0')\n",
      "batch_ind = 22\n",
      "y_pred_cpu = tensor([[ 0.3577,  0.4231, -0.0538,  0.1113, -0.0947, -0.5737, -0.2727, -0.0101],\n",
      "        [ 0.4409,  0.5132,  0.1188,  0.1824, -0.1959, -0.4827, -0.1855, -0.0278],\n",
      "        [ 0.1307,  0.6172, -0.1417,  0.3291, -0.4729, -0.5205, -0.1680,  0.0556],\n",
      "        [ 0.2717,  0.8013, -0.0903, -0.2146, -0.5566, -0.2810, -0.5156, -0.0709]],\n",
      "       dtype=torch.float16)\n",
      "idx = 33605\n",
      "offset = 10\n",
      "shape of x post augmentation = torch.Size([1, 122880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 20092\n",
      "offset = 20\n",
      "shape of x post augmentation = torch.Size([1, 184320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 7789\n",
      "offset = 35\n",
      "shape of x post augmentation = torch.Size([1, 245280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 9084\n",
      "offset = 9\n",
      "shape of x post augmentation = torch.Size([1, 143360])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 23\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.4712, -0.2935,  0.3376,  0.1493,  0.1316, -0.3667, -0.3923,  0.7900],\n",
      "        [ 0.3079,  0.0154, -0.1520,  0.2183, -0.0685, -0.2427, -0.5483, -0.1929],\n",
      "        [ 0.6240,  0.2551, -0.0855,  0.0018, -0.2671, -0.5122, -0.6792,  0.2595],\n",
      "        [ 0.2651,  0.1477, -0.2515,  0.0086,  0.2363, -0.2112, -0.6802, -0.1738]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([7, 0, 0, 0], device='cuda:0')\n",
      "batch_ind = 23\n",
      "y_pred_cpu = tensor([[ 0.4712, -0.2935,  0.3376,  0.1493,  0.1316, -0.3667, -0.3923,  0.7900],\n",
      "        [ 0.3079,  0.0154, -0.1520,  0.2183, -0.0685, -0.2427, -0.5483, -0.1929],\n",
      "        [ 0.6240,  0.2551, -0.0855,  0.0018, -0.2671, -0.5122, -0.6792,  0.2595],\n",
      "        [ 0.2651,  0.1477, -0.2515,  0.0086,  0.2363, -0.2112, -0.6802, -0.1738]],\n",
      "       dtype=torch.float16)\n",
      "idx = 8877\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32002\n",
      "offset = 11\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 17282\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 143360])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 20322\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 184320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 24\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.5098,  0.4939, -0.4270,  0.1403, -0.4546, -0.7495, -0.8096, -0.0150],\n",
      "        [ 0.3240,  0.5117,  0.0939,  0.1715, -0.0720, -0.1925, -0.1998, -0.2181],\n",
      "        [ 0.4487,  0.1890, -0.3115,  0.0790, -0.2612, -0.1365, -0.2079,  0.0352],\n",
      "        [ 0.1693,  0.0093, -0.2086,  0.3108, -0.3750, -0.3384, -0.4390, -0.2686]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 1, 0, 3], device='cuda:0')\n",
      "batch_ind = 24\n",
      "y_pred_cpu = tensor([[ 0.5098,  0.4939, -0.4270,  0.1403, -0.4546, -0.7495, -0.8096, -0.0150],\n",
      "        [ 0.3240,  0.5117,  0.0939,  0.1715, -0.0720, -0.1925, -0.1998, -0.2181],\n",
      "        [ 0.4487,  0.1890, -0.3115,  0.0790, -0.2612, -0.1365, -0.2079,  0.0352],\n",
      "        [ 0.1693,  0.0093, -0.2086,  0.3108, -0.3750, -0.3384, -0.4390, -0.2686]],\n",
      "       dtype=torch.float16)\n",
      "idx = 21634\n",
      "offset = 46\n",
      "shape of x post augmentation = torch.Size([1, 286720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 28249\n",
      "offset = 57\n",
      "shape of x post augmentation = torch.Size([1, 388640])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 4341\n",
      "offset = 22\n",
      "shape of x post augmentation = torch.Size([1, 388640])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 27541\n",
      "offset = 66\n",
      "shape of x post augmentation = torch.Size([1, 429600])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 25\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.2546,  0.5078, -0.1394,  0.1511, -0.2433, -0.3914, -0.8184, -0.2991],\n",
      "        [ 0.0196,  0.6250, -0.1348,  0.1969, -0.0117, -0.3757, -0.4668,  0.0316],\n",
      "        [ 0.4802,  0.5049,  0.0252,  0.0178, -0.5879, -0.2849, -0.5112, -0.0028],\n",
      "        [ 0.4771,  0.2556,  0.0531,  0.2432, -0.2151, -0.5625, -0.3821, -0.3718]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 1, 0], device='cuda:0')\n",
      "batch_ind = 25\n",
      "y_pred_cpu = tensor([[ 0.2546,  0.5078, -0.1394,  0.1511, -0.2433, -0.3914, -0.8184, -0.2991],\n",
      "        [ 0.0196,  0.6250, -0.1348,  0.1969, -0.0117, -0.3757, -0.4668,  0.0316],\n",
      "        [ 0.4802,  0.5049,  0.0252,  0.0178, -0.5879, -0.2849, -0.5112, -0.0028],\n",
      "        [ 0.4771,  0.2556,  0.0531,  0.2432, -0.2151, -0.5625, -0.3821, -0.3718]],\n",
      "       dtype=torch.float16)\n",
      "idx = 4785\n",
      "offset = 34\n",
      "shape of x post augmentation = torch.Size([1, 265760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 30448\n",
      "offset = 39\n",
      "shape of x post augmentation = torch.Size([1, 224800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 1295\n",
      "offset = 0\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 1472\n",
      "offset = 8\n",
      "shape of x post augmentation = torch.Size([1, 183840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 26\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.3225,  0.1646, -0.0640,  0.3120, -0.2183, -0.0367, -0.5503,  0.0245],\n",
      "        [ 0.4414,  0.5620, -0.4529,  0.2233, -0.2028, -0.6152, -0.3567, -0.3186],\n",
      "        [ 0.5298,  0.2769, -0.0864,  0.0264, -0.1935, -0.3284, -0.4546,  0.1710],\n",
      "        [ 0.2505,  0.3296, -0.5708, -0.1698, -0.4209, -0.2957, -1.0820, -0.1801]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 1, 0, 1], device='cuda:0')\n",
      "batch_ind = 26\n",
      "y_pred_cpu = tensor([[ 0.3225,  0.1646, -0.0640,  0.3120, -0.2183, -0.0367, -0.5503,  0.0245],\n",
      "        [ 0.4414,  0.5620, -0.4529,  0.2233, -0.2028, -0.6152, -0.3567, -0.3186],\n",
      "        [ 0.5298,  0.2769, -0.0864,  0.0264, -0.1935, -0.3284, -0.4546,  0.1710],\n",
      "        [ 0.2505,  0.3296, -0.5708, -0.1698, -0.4209, -0.2957, -1.0820, -0.1801]],\n",
      "       dtype=torch.float16)\n",
      "idx = 4236\n",
      "offset = 10\n",
      "shape of x post augmentation = torch.Size([1, 61440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 31083\n",
      "offset = 12\n",
      "shape of x post augmentation = torch.Size([1, 183840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 30242\n",
      "offset = 4\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32997\n",
      "offset = 18\n",
      "shape of x post augmentation = torch.Size([1, 143360])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 27\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.4597,  0.5762, -0.1539,  0.0129,  0.0537, -0.0867, -0.2764, -0.3811],\n",
      "        [ 0.6455,  0.2510, -0.1899,  0.2830, -0.5464, -0.0031, -0.4785, -0.1370],\n",
      "        [ 0.2778,  0.0644, -0.6729,  0.4067, -0.1660, -0.1748,  0.0021,  0.2195],\n",
      "        [ 0.4712,  0.0187, -0.2350,  0.1699, -0.2817, -0.1859, -0.3596, -0.4207]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 0, 3, 0], device='cuda:0')\n",
      "batch_ind = 27\n",
      "y_pred_cpu = tensor([[ 0.4597,  0.5762, -0.1539,  0.0129,  0.0537, -0.0867, -0.2764, -0.3811],\n",
      "        [ 0.6455,  0.2510, -0.1899,  0.2830, -0.5464, -0.0031, -0.4785, -0.1370],\n",
      "        [ 0.2778,  0.0644, -0.6729,  0.4067, -0.1660, -0.1748,  0.0021,  0.2195],\n",
      "        [ 0.4712,  0.0187, -0.2350,  0.1699, -0.2817, -0.1859, -0.3596, -0.4207]],\n",
      "       dtype=torch.float16)\n",
      "idx = 12586\n",
      "offset = 7\n",
      "shape of x post augmentation = torch.Size([1, 142880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 29348\n",
      "offset = 17\n",
      "shape of x post augmentation = torch.Size([1, 245280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 1113\n",
      "offset = 33\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 18773\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 28\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.5586,  0.3201, -0.4663,  0.0278, -0.1000, -0.5269, -0.3679, -0.3799],\n",
      "        [ 0.2852,  0.3125, -0.1635, -0.0892, -0.0527, -0.4167, -0.6001, -0.2747],\n",
      "        [ 0.4668,  0.7036, -0.4285,  0.3281, -0.2249, -0.1968, -0.2954,  0.1172],\n",
      "        [ 0.7500,  0.5552, -0.2096, -0.2207, -0.4548, -0.0465, -0.5117, -0.1975]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 1, 1, 0], device='cuda:0')\n",
      "batch_ind = 28\n",
      "y_pred_cpu = tensor([[ 0.5586,  0.3201, -0.4663,  0.0278, -0.1000, -0.5269, -0.3679, -0.3799],\n",
      "        [ 0.2852,  0.3125, -0.1635, -0.0892, -0.0527, -0.4167, -0.6001, -0.2747],\n",
      "        [ 0.4668,  0.7036, -0.4285,  0.3281, -0.2249, -0.1968, -0.2954,  0.1172],\n",
      "        [ 0.7500,  0.5552, -0.2096, -0.2207, -0.4548, -0.0465, -0.5117, -0.1975]],\n",
      "       dtype=torch.float16)\n",
      "idx = 7489\n",
      "offset = 20\n",
      "shape of x post augmentation = torch.Size([1, 306720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 20420\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 306720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 18432\n",
      "offset = 0\n",
      "shape of x post augmentation = torch.Size([1, 347680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 7347\n",
      "offset = 22\n",
      "shape of x post augmentation = torch.Size([1, 204320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 29\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.2357,  0.4846, -0.2491,  0.1521, -0.2360, -0.1567, -0.3113, -0.1261],\n",
      "        [ 0.2167,  0.3989, -0.4561,  0.1917,  0.1125,  0.1018, -0.8130, -0.3804],\n",
      "        [ 0.7935,  0.1503, -0.3145, -0.1130,  0.0064,  0.1020, -0.5225, -0.5225],\n",
      "        [ 0.5894,  0.3706,  0.0310, -0.2681, -0.0674, -0.3694, -0.3796, -0.3862]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 0, 0], device='cuda:0')\n",
      "batch_ind = 29\n",
      "y_pred_cpu = tensor([[ 0.2357,  0.4846, -0.2491,  0.1521, -0.2360, -0.1567, -0.3113, -0.1261],\n",
      "        [ 0.2167,  0.3989, -0.4561,  0.1917,  0.1125,  0.1018, -0.8130, -0.3804],\n",
      "        [ 0.7935,  0.1503, -0.3145, -0.1130,  0.0064,  0.1020, -0.5225, -0.5225],\n",
      "        [ 0.5894,  0.3706,  0.0310, -0.2681, -0.0674, -0.3694, -0.3796, -0.3862]],\n",
      "       dtype=torch.float16)\n",
      "idx = 15047\n",
      "offset = 1\n",
      "shape of x post augmentation = torch.Size([1, 61440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 13744\n",
      "offset = 19\n",
      "shape of x post augmentation = torch.Size([1, 184320])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32261\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 470560])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 34574\n",
      "offset = 19\n",
      "shape of x post augmentation = torch.Size([1, 450080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 30\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.3879,  0.1874, -0.4282,  0.0071, -0.2030, -0.2211, -0.0115, -0.1077],\n",
      "        [ 0.7061,  0.3518, -0.2859, -0.0654, -0.4602, -0.3103, -0.2737, -0.3064],\n",
      "        [ 0.5137,  0.3032, -0.2520,  0.1688, -0.0934, -0.1188, -0.5732, -0.1201],\n",
      "        [ 0.1398,  0.4836, -0.7109,  0.3340, -0.1643, -0.0288, -0.6592, -0.7549]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 0, 0, 1], device='cuda:0')\n",
      "batch_ind = 30\n",
      "y_pred_cpu = tensor([[ 0.3879,  0.1874, -0.4282,  0.0071, -0.2030, -0.2211, -0.0115, -0.1077],\n",
      "        [ 0.7061,  0.3518, -0.2859, -0.0654, -0.4602, -0.3103, -0.2737, -0.3064],\n",
      "        [ 0.5137,  0.3032, -0.2520,  0.1688, -0.0934, -0.1188, -0.5732, -0.1201],\n",
      "        [ 0.1398,  0.4836, -0.7109,  0.3340, -0.1643, -0.0288, -0.6592, -0.7549]],\n",
      "       dtype=torch.float16)\n",
      "idx = 485\n",
      "offset = 41\n",
      "shape of x post augmentation = torch.Size([1, 286240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 7288\n",
      "offset = 17\n",
      "shape of x post augmentation = torch.Size([1, 286240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 1466\n",
      "offset = 2\n",
      "shape of x post augmentation = torch.Size([1, 183840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 1337\n",
      "offset = 24\n",
      "shape of x post augmentation = torch.Size([1, 142880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 31\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.4658,  0.3149, -0.2847,  0.4736, -0.3911,  0.0235, -0.2915, -0.1851],\n",
      "        [ 0.3149,  0.5000, -0.1573, -0.2357, -0.2708, -0.1730, -0.5684, -0.2981],\n",
      "        [ 0.4041,  0.3450, -0.2070,  0.1439, -0.0273, -0.1215, -0.1238, -0.2769],\n",
      "        [ 0.3218,  0.1709, -0.3528,  0.3008,  0.0071, -0.0420, -0.1261,  0.1255]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([3, 1, 0, 0], device='cuda:0')\n",
      "batch_ind = 31\n",
      "y_pred_cpu = tensor([[ 0.4658,  0.3149, -0.2847,  0.4736, -0.3911,  0.0235, -0.2915, -0.1851],\n",
      "        [ 0.3149,  0.5000, -0.1573, -0.2357, -0.2708, -0.1730, -0.5684, -0.2981],\n",
      "        [ 0.4041,  0.3450, -0.2070,  0.1439, -0.0273, -0.1215, -0.1238, -0.2769],\n",
      "        [ 0.3218,  0.1709, -0.3528,  0.3008,  0.0071, -0.0420, -0.1261,  0.1255]],\n",
      "       dtype=torch.float16)\n",
      "idx = 14659\n",
      "offset = 8\n",
      "shape of x post augmentation = torch.Size([1, 225280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 1531\n",
      "offset = 3\n",
      "shape of x post augmentation = torch.Size([1, 122880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32294\n",
      "offset = 56\n",
      "shape of x post augmentation = torch.Size([1, 470560])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 22834\n",
      "offset = 10\n",
      "shape of x post augmentation = torch.Size([1, 61440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 32\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.5127,  0.2761, -0.3120,  0.2590, -0.1462, -0.0723, -0.2244, -0.0900],\n",
      "        [ 0.2842,  0.1704, -0.1089, -0.1177, -0.2012, -0.4265, -0.0970, -0.1473],\n",
      "        [ 0.6089,  0.4043, -0.1190, -0.1536, -0.3760, -0.1031, -0.4636,  0.2473],\n",
      "        [ 0.1526,  0.1632, -0.4070,  0.4561, -0.2498,  0.0024, -0.3018,  0.1716]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 0, 0, 3], device='cuda:0')\n",
      "batch_ind = 32\n",
      "y_pred_cpu = tensor([[ 0.5127,  0.2761, -0.3120,  0.2590, -0.1462, -0.0723, -0.2244, -0.0900],\n",
      "        [ 0.2842,  0.1704, -0.1089, -0.1177, -0.2012, -0.4265, -0.0970, -0.1473],\n",
      "        [ 0.6089,  0.4043, -0.1190, -0.1536, -0.3760, -0.1031, -0.4636,  0.2473],\n",
      "        [ 0.1526,  0.1632, -0.4070,  0.4561, -0.2498,  0.0024, -0.3018,  0.1716]],\n",
      "       dtype=torch.float16)\n",
      "idx = 34018\n",
      "offset = 12\n",
      "shape of x post augmentation = torch.Size([1, 327680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 21693\n",
      "offset = 4\n",
      "shape of x post augmentation = torch.Size([1, 61440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 7917\n",
      "offset = 3\n",
      "shape of x post augmentation = torch.Size([1, 286720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 7017\n",
      "offset = 8\n",
      "shape of x post augmentation = torch.Size([1, 81440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 33\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.4343,  0.4045, -0.2842,  0.3008, -0.3066, -0.3662, -0.1598, -0.0126],\n",
      "        [ 0.3479,  0.3042, -0.3474,  0.2971, -0.1410, -0.4026, -0.1602,  0.0490],\n",
      "        [ 0.1136,  0.4827, -0.4734,  0.3865, -0.3030, -0.2179, -0.1527,  0.2147],\n",
      "        [ 0.4180,  0.3101, -0.2087, -0.0122,  0.0589, -0.2417, -0.1050, -0.4641]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 0, 1, 0], device='cuda:0')\n",
      "batch_ind = 33\n",
      "y_pred_cpu = tensor([[ 0.4343,  0.4045, -0.2842,  0.3008, -0.3066, -0.3662, -0.1598, -0.0126],\n",
      "        [ 0.3479,  0.3042, -0.3474,  0.2971, -0.1410, -0.4026, -0.1602,  0.0490],\n",
      "        [ 0.1136,  0.4827, -0.4734,  0.3865, -0.3030, -0.2179, -0.1527,  0.2147],\n",
      "        [ 0.4180,  0.3101, -0.2087, -0.0122,  0.0589, -0.2417, -0.1050, -0.4641]],\n",
      "       dtype=torch.float16)\n",
      "idx = 18818\n",
      "offset = 58\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 27683\n",
      "offset = 28\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 33395\n",
      "offset = 4\n",
      "shape of x post augmentation = torch.Size([1, 61440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 24854\n",
      "offset = 39\n",
      "shape of x post augmentation = torch.Size([1, 368160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 34\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 3.5132e-01,  3.4619e-01, -1.8835e-01,  1.0730e-01,  1.3782e-01,\n",
      "         -3.5498e-01, -8.5144e-02, -1.2164e-01],\n",
      "        [-7.5912e-03,  1.8689e-01, -4.2920e-01,  3.4399e-01, -3.4851e-02,\n",
      "         -2.8442e-01, -2.0935e-01,  9.2712e-02],\n",
      "        [ 4.7510e-01,  3.5254e-01,  4.5074e-02,  4.8004e-02,  1.0480e-01,\n",
      "         -3.3618e-01, -1.8298e-01,  3.7050e-04],\n",
      "        [ 2.5537e-01,  7.2510e-01, -4.4678e-01,  4.5581e-01, -2.9712e-01,\n",
      "         -5.0342e-01, -2.3621e-01, -2.0996e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 3, 0, 1], device='cuda:0')\n",
      "batch_ind = 34\n",
      "y_pred_cpu = tensor([[ 3.5132e-01,  3.4619e-01, -1.8835e-01,  1.0730e-01,  1.3782e-01,\n",
      "         -3.5498e-01, -8.5144e-02, -1.2164e-01],\n",
      "        [-7.5912e-03,  1.8689e-01, -4.2920e-01,  3.4399e-01, -3.4851e-02,\n",
      "         -2.8442e-01, -2.0935e-01,  9.2712e-02],\n",
      "        [ 4.7510e-01,  3.5254e-01,  4.5074e-02,  4.8004e-02,  1.0480e-01,\n",
      "         -3.3618e-01, -1.8298e-01,  3.7050e-04],\n",
      "        [ 2.5537e-01,  7.2510e-01, -4.4678e-01,  4.5581e-01, -2.9712e-01,\n",
      "         -5.0342e-01, -2.3621e-01, -2.0996e-01]], dtype=torch.float16)\n",
      "idx = 28331\n",
      "offset = 65\n",
      "shape of x post augmentation = torch.Size([1, 470560])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 15576\n",
      "offset = 43\n",
      "shape of x post augmentation = torch.Size([1, 368160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 9059\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 3715\n",
      "offset = 3\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 35\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.2131,  0.8428, -0.3176,  0.3787, -0.4348, -0.3723, -0.2051,  0.3269],\n",
      "        [-0.0069,  0.7124, -0.2037,  0.5957, -0.4224, -0.3789, -0.3108, -0.0152],\n",
      "        [ 0.3813,  0.2255, -0.3708,  0.4844, -0.3491, -0.0509, -0.4163,  0.0853],\n",
      "        [ 0.1406,  0.4397, -0.4937,  0.1589, -0.0391, -0.4094,  0.0086, -0.0879]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 1, 3, 1], device='cuda:0')\n",
      "batch_ind = 35\n",
      "y_pred_cpu = tensor([[ 0.2131,  0.8428, -0.3176,  0.3787, -0.4348, -0.3723, -0.2051,  0.3269],\n",
      "        [-0.0069,  0.7124, -0.2037,  0.5957, -0.4224, -0.3789, -0.3108, -0.0152],\n",
      "        [ 0.3813,  0.2255, -0.3708,  0.4844, -0.3491, -0.0509, -0.4163,  0.0853],\n",
      "        [ 0.1406,  0.4397, -0.4937,  0.1589, -0.0391, -0.4094,  0.0086, -0.0879]],\n",
      "       dtype=torch.float16)\n",
      "idx = 9931\n",
      "offset = 11\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 32351\n",
      "offset = 8\n",
      "shape of x post augmentation = torch.Size([1, 143360])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 31419\n",
      "offset = 39\n",
      "shape of x post augmentation = torch.Size([1, 430080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 15927\n",
      "offset = 37\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 36\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[-0.0685,  0.6348, -0.5630,  0.3547, -0.2720, -0.1903, -0.1663,  0.3047],\n",
      "        [ 0.3462,  0.3408, -0.2590,  0.1796, -0.3345, -0.1415, -0.0836,  0.1648],\n",
      "        [ 0.2053,  0.5044, -0.3147,  0.5488, -0.2859, -0.2903, -0.2715, -0.2197],\n",
      "        [ 0.5142,  0.3950, -0.3340,  0.6113, -0.4983, -0.3374, -0.0464, -0.1440]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 0, 3, 3], device='cuda:0')\n",
      "batch_ind = 36\n",
      "y_pred_cpu = tensor([[-0.0685,  0.6348, -0.5630,  0.3547, -0.2720, -0.1903, -0.1663,  0.3047],\n",
      "        [ 0.3462,  0.3408, -0.2590,  0.1796, -0.3345, -0.1415, -0.0836,  0.1648],\n",
      "        [ 0.2053,  0.5044, -0.3147,  0.5488, -0.2859, -0.2903, -0.2715, -0.2197],\n",
      "        [ 0.5142,  0.3950, -0.3340,  0.6113, -0.4983, -0.3374, -0.0464, -0.1440]],\n",
      "       dtype=torch.float16)\n",
      "idx = 5980\n",
      "offset = 8\n",
      "shape of x post augmentation = torch.Size([1, 347680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 22043\n",
      "offset = 0\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 26309\n",
      "offset = 45\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 34428\n",
      "offset = 9\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 37\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.2939,  0.6924, -0.8296,  0.2258, -0.3984, -0.5928, -0.2362, -0.1482],\n",
      "        [ 0.1315,  0.2524, -0.5845,  0.5498, -0.2949, -0.0045, -0.0330,  0.1925],\n",
      "        [ 0.2151,  0.2462, -0.3772,  0.1974, -0.5176, -0.2312, -0.3420, -0.0372],\n",
      "        [ 0.2915,  0.5601, -0.3962,  0.3289, -0.2096, -0.1329, -0.0883,  0.1379]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 3, 1, 1], device='cuda:0')\n",
      "batch_ind = 37\n",
      "y_pred_cpu = tensor([[ 0.2939,  0.6924, -0.8296,  0.2258, -0.3984, -0.5928, -0.2362, -0.1482],\n",
      "        [ 0.1315,  0.2524, -0.5845,  0.5498, -0.2949, -0.0045, -0.0330,  0.1925],\n",
      "        [ 0.2151,  0.2462, -0.3772,  0.1974, -0.5176, -0.2312, -0.3420, -0.0372],\n",
      "        [ 0.2915,  0.5601, -0.3962,  0.3289, -0.2096, -0.1329, -0.0883,  0.1379]],\n",
      "       dtype=torch.float16)\n",
      "idx = 33093\n",
      "offset = 5\n",
      "\n",
      "shape of x post augmentation = torch.Size([1, 40960])from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 12708\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 5697\n",
      "offset = 58\n",
      "shape of x post augmentation = torch.Size([1, 429600])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 24222\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 266240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 38\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.2559,  0.0635, -0.5576,  0.5156, -0.3396, -0.0767, -0.0604,  0.2072],\n",
      "        [ 0.2600,  0.3840, -0.4041,  0.3992, -0.2316, -0.0940, -0.0506,  0.2834],\n",
      "        [ 0.2793,  0.3003, -0.4070,  0.3899, -0.5806, -0.4119, -0.0648, -0.0269],\n",
      "        [ 0.3044,  0.1779, -0.2498,  0.5181, -0.2859, -0.1978,  0.1573, -0.0558]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([3, 3, 3, 3], device='cuda:0')\n",
      "batch_ind = 38\n",
      "y_pred_cpu = tensor([[ 0.2559,  0.0635, -0.5576,  0.5156, -0.3396, -0.0767, -0.0604,  0.2072],\n",
      "        [ 0.2600,  0.3840, -0.4041,  0.3992, -0.2316, -0.0940, -0.0506,  0.2834],\n",
      "        [ 0.2793,  0.3003, -0.4070,  0.3899, -0.5806, -0.4119, -0.0648, -0.0269],\n",
      "        [ 0.3044,  0.1779, -0.2498,  0.5181, -0.2859, -0.1978,  0.1573, -0.0558]],\n",
      "       dtype=torch.float16)\n",
      "idx = 14320\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 81920])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 5619\n",
      "offset = 1\n",
      "shape of x post augmentation = torch.Size([1, 20000])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 14597\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 348160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 28289\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 470560])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 39\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[-0.2277, -0.1497, -0.5566,  0.3088, -0.8633,  0.1847, -0.2854,  0.0493],\n",
      "        [ 0.4182,  0.2189, -0.3162,  0.5718, -0.2434, -0.1267, -0.1211, -0.1326],\n",
      "        [ 0.6074,  0.4915, -0.4373,  0.3672, -0.5586, -0.2144, -0.4519,  0.1256],\n",
      "        [ 0.1824,  0.0742, -0.5337,  0.5562, -0.4751, -0.2742, -0.0630,  0.1191]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([3, 3, 0, 3], device='cuda:0')\n",
      "batch_ind = 39\n",
      "y_pred_cpu = tensor([[-0.2277, -0.1497, -0.5566,  0.3088, -0.8633,  0.1847, -0.2854,  0.0493],\n",
      "        [ 0.4182,  0.2189, -0.3162,  0.5718, -0.2434, -0.1267, -0.1211, -0.1326],\n",
      "        [ 0.6074,  0.4915, -0.4373,  0.3672, -0.5586, -0.2144, -0.4519,  0.1256],\n",
      "        [ 0.1824,  0.0742, -0.5337,  0.5562, -0.4751, -0.2742, -0.0630,  0.1191]],\n",
      "       dtype=torch.float16)\n",
      "idx = 10783\n",
      "offset = 10\n",
      "shape of x post augmentation = torch.Size([1, 204800])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 3494\n",
      "offset = 5\n",
      "shape of x post augmentation = torch.Size([1, 163840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 19873\n",
      "offset = 5\n",
      "shape of x post augmentation = torch.Size([1, 225280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 15743\n",
      "offset = 23\n",
      "shape of x post augmentation = torch.Size([1, 388640])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 40\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.0566,  0.3479, -0.5303,  0.3308, -0.3726, -0.4170, -0.2318, -0.1630],\n",
      "        [ 0.0388,  0.2428, -0.5669,  0.4619, -0.5859, -0.1716, -0.5039, -0.0427],\n",
      "        [ 0.4021,  0.3308, -0.2881,  0.5327, -0.3267, -0.4404, -0.1573, -0.2917],\n",
      "        [ 0.2279,  0.3608, -0.4326,  0.3103, -0.4092, -0.4241, -0.1509, -0.1142]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 3, 3, 1], device='cuda:0')\n",
      "batch_ind = 40\n",
      "y_pred_cpu = tensor([[ 0.0566,  0.3479, -0.5303,  0.3308, -0.3726, -0.4170, -0.2318, -0.1630],\n",
      "        [ 0.0388,  0.2428, -0.5669,  0.4619, -0.5859, -0.1716, -0.5039, -0.0427],\n",
      "        [ 0.4021,  0.3308, -0.2881,  0.5327, -0.3267, -0.4404, -0.1573, -0.2917],\n",
      "        [ 0.2279,  0.3608, -0.4326,  0.3103, -0.4092, -0.4241, -0.1509, -0.1142]],\n",
      "       dtype=torch.float16)\n",
      "idx = 31398\n",
      "offset = 18\n",
      "shape of x post augmentation = torch.Size([1, 430080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 31860\n",
      "offset = 20\n",
      "shape of x post augmentation = torch.Size([1, 306720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 16936\n",
      "offset = 10\n",
      "shape of x post augmentation = torch.Size([1, 61440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 9842\n",
      "offset = 0\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 41\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.1648,  0.1680, -0.1787,  0.2793, -0.2166, -0.1785, -0.2057, -0.0825],\n",
      "        [ 0.6357, -0.1853, -0.4038,  0.3992, -0.4387, -0.3047, -0.1442,  0.3359],\n",
      "        [ 0.6909,  0.2377, -0.4150,  0.1509, -0.1360, -0.4956, -0.2710, -0.0343],\n",
      "        [ 0.4399,  0.2883, -0.4021,  0.3284, -0.3254, -0.2534, -0.0923, -0.2196]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([3, 0, 0, 0], device='cuda:0')\n",
      "batch_ind = 41\n",
      "y_pred_cpu = tensor([[ 0.1648,  0.1680, -0.1787,  0.2793, -0.2166, -0.1785, -0.2057, -0.0825],\n",
      "        [ 0.6357, -0.1853, -0.4038,  0.3992, -0.4387, -0.3047, -0.1442,  0.3359],\n",
      "        [ 0.6909,  0.2377, -0.4150,  0.1509, -0.1360, -0.4956, -0.2710, -0.0343],\n",
      "        [ 0.4399,  0.2883, -0.4021,  0.3284, -0.3254, -0.2534, -0.0923, -0.2196]],\n",
      "       dtype=torch.float16)\n",
      "idx = 25491\n",
      "offset = 36\n",
      "shape of x post augmentation = torch.Size([1, 245280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 11609\n",
      "offset = 10\n",
      "shape of x post augmentation = torch.Size([1, 245280])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 11654\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 163360])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 14263\n",
      "offset = 19\n",
      "shape of x post augmentation = torch.Size([1, 286240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 42\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[ 0.5088,  0.3879, -0.3477,  0.2527, -0.4927, -0.3159, -0.1626,  0.0145],\n",
      "        [ 0.2893,  0.9673, -0.1726,  0.2434, -0.7471, -0.6616,  0.0030, -0.5093],\n",
      "        [ 0.6284,  0.8335, -0.2250,  0.3779, -0.6523, -0.7036, -0.2307, -0.2401],\n",
      "        [ 0.1066,  0.2542, -0.1516,  0.4434,  0.0275, -0.2827, -0.1603,  0.0327]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([0, 1, 1, 3], device='cuda:0')\n",
      "batch_ind = 42\n",
      "y_pred_cpu = tensor([[ 0.5088,  0.3879, -0.3477,  0.2527, -0.4927, -0.3159, -0.1626,  0.0145],\n",
      "        [ 0.2893,  0.9673, -0.1726,  0.2434, -0.7471, -0.6616,  0.0030, -0.5093],\n",
      "        [ 0.6284,  0.8335, -0.2250,  0.3779, -0.6523, -0.7036, -0.2307, -0.2401],\n",
      "        [ 0.1066,  0.2542, -0.1516,  0.4434,  0.0275, -0.2827, -0.1603,  0.0327]],\n",
      "       dtype=torch.float16)\n",
      "idx = 11906\n",
      "offset = 22\n",
      "shape of x post augmentation = torch.Size([1, 122880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 29568\n",
      "offset = 6\n",
      "shape of x post augmentation = torch.Size([1, 286240])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 16602\n",
      "offset = 15\n",
      "shape of x post augmentation = torch.Size([1, 368160])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 20727\n",
      "offset = 24\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 43\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[ 0.4243,  0.4963, -0.3811,  0.2627, -0.4697, -0.2023, -0.1405, -0.4192],\n",
      "        [ 0.5400,  0.0746, -0.1874,  0.2360, -0.0865, -0.3086, -0.2283, -0.0661],\n",
      "        [ 0.4067,  0.3872, -0.1891,  0.1967, -0.3599, -0.3257, -0.2168, -0.1381],\n",
      "        [ 0.2317,  0.2097, -0.1803,  0.0342, -0.3250, -0.1221, -0.1238, -0.1542]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 0, 0, 0], device='cuda:0')\n",
      "batch_ind = 43\n",
      "y_pred_cpu = tensor([[ 0.4243,  0.4963, -0.3811,  0.2627, -0.4697, -0.2023, -0.1405, -0.4192],\n",
      "        [ 0.5400,  0.0746, -0.1874,  0.2360, -0.0865, -0.3086, -0.2283, -0.0661],\n",
      "        [ 0.4067,  0.3872, -0.1891,  0.1967, -0.3599, -0.3257, -0.2168, -0.1381],\n",
      "        [ 0.2317,  0.2097, -0.1803,  0.0342, -0.3250, -0.1221, -0.1238, -0.1542]],\n",
      "       dtype=torch.float16)\n",
      "idx = 20539\n",
      "offset = 33\n",
      "shape of x post augmentation = torch.Size([1, 450080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 16717\n",
      "offset = 8\n",
      "shape of x post augmentation = torch.Size([1, 163840])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 5627\n",
      "offset = 7\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 20577\n",
      "offset = 71\n",
      "shape of x post augmentation = torch.Size([1, 450080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 44\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "ZERO INPUT in forward\n",
      "y_pred  = tensor([[ 0.1703,  0.3088, -0.4199,  0.1517, -0.1779, -0.5371, -0.0920, -0.2927],\n",
      "        [ 0.1146,  0.1787, -0.3511,  0.2107, -0.4346, -0.4307,  0.0056, -0.0934],\n",
      "        [ 0.1700,  0.3979, -0.5122,  0.2046, -0.2742, -0.2301,  0.0566, -0.0512],\n",
      "        [ 0.3218,  0.4089, -0.4766,  0.1920, -0.5181, -0.3909, -0.0892, -0.1000]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([1, 3, 1, 1], device='cuda:0')\n",
      "batch_ind = 44\n",
      "y_pred_cpu = tensor([[ 0.1703,  0.3088, -0.4199,  0.1517, -0.1779, -0.5371, -0.0920, -0.2927],\n",
      "        [ 0.1146,  0.1787, -0.3511,  0.2107, -0.4346, -0.4307,  0.0056, -0.0934],\n",
      "        [ 0.1700,  0.3979, -0.5122,  0.2046, -0.2742, -0.2301,  0.0566, -0.0512],\n",
      "        [ 0.3218,  0.4089, -0.4766,  0.1920, -0.5181, -0.3909, -0.0892, -0.1000]],\n",
      "       dtype=torch.float16)\n",
      "idx = 11109\n",
      "offset = 17\n",
      "shape of x post augmentation = torch.Size([1, 245760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 10209\n",
      "offset = 63\n",
      "shape of x post augmentation = torch.Size([1, 430080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 20287\n",
      "offset = 1\n",
      "shape of x post augmentation = torch.Size([1, 81440])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 31402\n",
      "offset = 22\n",
      "shape of x post augmentation = torch.Size([1, 430080])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 45\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[-0.0401,  0.2812, -0.7646,  0.3420, -0.2751, -0.3889,  0.0749,  0.1735],\n",
      "        [-0.0511, -0.3147, -0.9307,  0.3999, -0.5142, -0.1638,  0.0363, -0.3091],\n",
      "        [ 0.2708, -0.1871, -0.8042,  0.5186, -0.5449,  0.0868, -0.0108,  0.3010],\n",
      "        [ 0.0762,  0.0434, -0.9590,  0.7026, -0.3833, -0.0897, -0.1370,  0.3147]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([3, 3, 3, 3], device='cuda:0')\n",
      "batch_ind = 45\n",
      "y_pred_cpu = tensor([[-0.0401,  0.2812, -0.7646,  0.3420, -0.2751, -0.3889,  0.0749,  0.1735],\n",
      "        [-0.0511, -0.3147, -0.9307,  0.3999, -0.5142, -0.1638,  0.0363, -0.3091],\n",
      "        [ 0.2708, -0.1871, -0.8042,  0.5186, -0.5449,  0.0868, -0.0108,  0.3010],\n",
      "        [ 0.0762,  0.0434, -0.9590,  0.7026, -0.3833, -0.0897, -0.1370,  0.3147]],\n",
      "       dtype=torch.float16)\n",
      "idx = 29324\n",
      "offset = 62\n",
      "shape of x post augmentation = torch.Size([1, 347680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 24299\n",
      "offset = 49\n",
      "shape of x post augmentation = torch.Size([1, 327200])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 21319\n",
      "offset = 13\n",
      "shape of x post augmentation = torch.Size([1, 347680])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 20757\n",
      "offset = 7\n",
      "shape of x post augmentation = torch.Size([1, 122400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 46\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 3.0322e-01,  2.0251e-01, -4.0454e-01,  3.0615e-01, -3.5986e-01,\n",
      "         -2.7490e-01, -2.2083e-01, -5.7526e-03],\n",
      "        [ 8.2520e-02, -2.4048e-01, -2.7588e-01,  7.2266e-01, -9.6313e-02,\n",
      "          9.1003e-02, -5.9326e-01,  1.8640e-01],\n",
      "        [ 4.5288e-02,  1.7908e-01, -6.1523e-01,  4.5972e-01, -4.0674e-01,\n",
      "         -9.2840e-04, -1.7981e-01,  9.4531e-01],\n",
      "        [ 1.1157e-01,  7.2021e-02, -4.5850e-01,  2.3596e-01, -2.3682e-01,\n",
      "         -2.5024e-01, -5.3027e-01, -9.7046e-03]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([3, 3, 7, 3], device='cuda:0')\n",
      "batch_ind = 46\n",
      "y_pred_cpu = tensor([[ 3.0322e-01,  2.0251e-01, -4.0454e-01,  3.0615e-01, -3.5986e-01,\n",
      "         -2.7490e-01, -2.2083e-01, -5.7526e-03],\n",
      "        [ 8.2520e-02, -2.4048e-01, -2.7588e-01,  7.2266e-01, -9.6313e-02,\n",
      "          9.1003e-02, -5.9326e-01,  1.8640e-01],\n",
      "        [ 4.5288e-02,  1.7908e-01, -6.1523e-01,  4.5972e-01, -4.0674e-01,\n",
      "         -9.2840e-04, -1.7981e-01,  9.4531e-01],\n",
      "        [ 1.1157e-01,  7.2021e-02, -4.5850e-01,  2.3596e-01, -2.3682e-01,\n",
      "         -2.5024e-01, -5.3027e-01, -9.7046e-03]], dtype=torch.float16)\n",
      "idx = 13618\n",
      "offset = 14\n",
      "shape of x post augmentation = torch.Size([1, 122880])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 13939\n",
      "offset = 9\n",
      "shape of x post augmentation = torch.Size([1, 265760])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 12960\n",
      "offset = 56\n",
      "shape of x post augmentation = torch.Size([1, 306720])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "idx = 22403\n",
      "offset = 17\n",
      "shape of x post augmentation = torch.Size([1, 102400])\n",
      "from get_item of train, returning  x of shape = torch.Size([1, 15360])\n",
      "inside train loop.. batch_ind = 47\n",
      "inside train loop.. x device = cuda:0\n",
      "post spec gram shape =  torch.Size([4, 128, 31])\n",
      "post norm shape =  torch.Size([4, 1, 128, 31])\n",
      "post sizer shape =  torch.Size([4, 224, 224])\n",
      "post unsqueeze shape =  torch.Size([4, 1, 224, 224])\n",
      "Final shape that goes to backbone = torch.Size([4, 1, 224, 224])\n",
      "y_pred  = tensor([[ 0.4031, -0.1164, -0.6719,  0.6265, -0.1597,  0.0190, -0.1836,  0.2147],\n",
      "        [-0.2410,  0.0681, -1.0283,  0.8042, -0.0941, -0.0865, -0.4246, -0.0123],\n",
      "        [ 0.3110,  0.1720, -0.9829,  0.6001, -0.3784,  0.3716, -0.1370,  0.2612],\n",
      "        [ 0.2095, -0.1831, -0.8594,  0.7510, -0.2844, -0.1282, -0.1428,  0.0315]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\n",
      "preds = tensor([3, 3, 3, 3], device='cuda:0')\n",
      "batch_ind = 47\n",
      "y_pred_cpu = tensor([[ 0.4031, -0.1164, -0.6719,  0.6265, -0.1597,  0.0190, -0.1836,  0.2147],\n",
      "        [-0.2410,  0.0681, -1.0283,  0.8042, -0.0941, -0.0865, -0.4246, -0.0123],\n",
      "        [ 0.3110,  0.1720, -0.9829,  0.6001, -0.3784,  0.3716, -0.1370,  0.2612],\n",
      "        [ 0.2095, -0.1831, -0.8594,  0.7510, -0.2844, -0.1282, -0.1428,  0.0315]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5582/3965849374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#filepath = \"../../models/model_e73_2022_10_08_07_44_27.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model_epcoh_99 = load_model(filepath,model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mclass_weights\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5582/316917586.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, test_loader, model, classes, class_weights, num_epochs, n_channels)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mbase_optimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/timm/optim/adamp.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wd_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/timm/optim/adamp.py\u001b[0m in \u001b[0;36mprojection\u001b[0;34m(p, grad, perturb, delta, wd_ratio, eps)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# FIXME this is a problem for PyTorch XLA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mparam_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mperturb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model =Model('convnext_xlarge_in22k',224)\n",
    "#model =Model('convnext_small',224)\n",
    "#filepath = \"../../models/model_e73_2022_10_08_07_44_27.pth\"\n",
    "#model_epcoh_99 = load_model(filepath,model)\n",
    "model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1 = train_model(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103bee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model=Model('convnext_small',224)):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92685af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len of all_train_f1 = \"+str(len(all_train_f1)))\n",
    "print(\"len of all_val_f1 = \"+str(len(all_val_f1)))\n",
    "print(\"len of all_val_loss = \"+str(len(all_val_loss)))\n",
    "print(\"len of all_train_loss = \"+str(len(all_train_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec3dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554aa17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef79263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12085bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99be7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_f1_final =  [v for i, v in enumerate(all_val_f1) if i % 2 == 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_loss_final =  [v for i, v in enumerate(all_val_loss) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_f1_final =  [v for i, v in enumerate(all_train_f1) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39084bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_train_f1_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf97a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({'train_loss':all_train_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781caf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['val_f1'] = all_val_f1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['train_f1'] = all_train_f1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['val_loss'] = all_val_loss_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.fillna(0)\n",
    "plot_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39976c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.iloc[129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(plot_df['train_f1','val_f1']);\n",
    "plt.figure(figsize=(8,6)) \n",
    "sns.lineplot(plot_df[['train_f1','val_f1']])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score - ConvNext Small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c292a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) \n",
    "sns.lineplot(plot_df[['train_loss','val_loss']])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss - ConvNext Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('file1.csv')\n",
    "plot_df.to_csv(\"plot_df_convNext_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3be731",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ddf19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e19219",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_new = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "val_loader_new = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=2,\n",
    "        num_workers=0, pin_memory=pin_memory  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8864a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = df_val_offset\n",
    "model = model_epcoh_10\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "for idx,(x,y) in enumerate(val_dataset):\n",
    "    print(idx)\n",
    "    print(y)\n",
    "    x = x.to('cuda').float()\n",
    "    print(\"x shape = \" +str(x.shape))\n",
    "    #x_new = x.unsqueeze(dim = 1)\n",
    "    print(\"x_new shape = \" +str(x_new.shape))\n",
    "    x_new = x.to('cuda')\n",
    "    y_pred = model(x_new)['prediction']\n",
    "    y_pred_cpu = y_pred.cpu().detach()\n",
    "    preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "    df_erroriloc[idx]['y_hat'] = preds\n",
    "    del x_new\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43002240",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,15360)\n",
    "x = x.unsqueeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ce8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_offset.head()\n",
    "path_temp = \"../data/audio/\"\n",
    "for i,row in df_val_offset.iterrows():\n",
    "    print(\"i = \" +str(i))\n",
    "    print(\"id = \" + str(int(row['id'])))\n",
    "    file = str(int(row['id']))+\".wav\"\n",
    "    print(file)\n",
    "    path = path_temp + file\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "    if inp_rate != config.rate:\n",
    "        import torchaudio.transforms as T\n",
    "        resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[1] < config.rate*min_length:\n",
    "        #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "        f_out = pad_mean(waveform)\n",
    "    else:\n",
    "        f = waveform[0]\n",
    "        f_out = f.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad82766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(df):\n",
    "    \n",
    "    path_name = \"../data/audio/\"\n",
    "    file = df.loc[idx]['id'])}.wav\")\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"\")\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            \n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"returning x of shape ...\" + str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ffe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the model checkpoint as a parameter as input\n",
    "# read the val df\n",
    "#get the tensor rep for the offset.\n",
    "#pass it to the model get add get the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03333cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    label.append(np.random.rand(9))\n",
    "    pred.append(np.random.rand(9))\n",
    "print(label)\n",
    "print(pred)\n",
    "print(classification_report(label, pred, target_names= classes, labels= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0324c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.randn(4,9)\n",
    "y_pred.shape\n",
    "#y_pred_np = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_np\n",
    "# y_pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca682481",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ac0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(x,y) in enumerate(test_loader):\n",
    "    print(\"idx = \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bb0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3c3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8e621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
