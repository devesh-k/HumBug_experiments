{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e6e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some standard imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb61a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the checkpoint API\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "import torch.nn as nn\n",
    "\n",
    "# create a simple Sequential model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5),\n",
    "    nn.ReLU()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b978da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create the model inputs\n",
    "input_var = Variable(torch.randn(1, 100), requires_grad=True)\n",
    "print(input_var.shape)\n",
    "# set the number of checkpoint segments\n",
    "segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38a70f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('0', Linear(in_features=100, out_features=50, bias=True)), ('1', ReLU()), ('2', Linear(in_features=50, out_features=20, bias=True)), ('3', ReLU()), ('4', Linear(in_features=20, out_features=5, bias=True)), ('5', ReLU())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d35c5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.1500, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "out.sum() =  tensor(0.1500, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# get the modules in the model. These modules should be in the order\n",
    "# the model should be executed\n",
    "modules = [module for k, module in model._modules.items()]\n",
    "#print(modules)\n",
    "\n",
    "# now call the checkpoint API and get the output\n",
    "out = checkpoint_sequential(modules, segments, input_var)\n",
    "print(out)\n",
    "# run the backwards pass on the model. For backwards pass, for simplicity purpose, \n",
    "# we won't calculate the loss and rather backprop on out.sum()\n",
    "model.zero_grad()\n",
    "print(\"out.sum() = \", out.sum())\n",
    "out.sum().backward()\n",
    "\n",
    "# now we save the output and parameter gradients that we will use for comparison purposes with\n",
    "# the non-checkpointed run.\n",
    "output_checkpointed = out.data.clone()\n",
    "grad_checkpointed = {}\n",
    "for name, param in model.named_parameters():\n",
    "#     print(\"name = \", name)\n",
    "#     print(\"param = \", param)\n",
    "    grad_checkpointed[name] = param.grad.data.clone()\n",
    "\n",
    "#print(\"grad_checkpointed = \",grad_checkpointed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43d3d9",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b691d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the model \n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1888a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f10cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f17ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e958020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "460010e5",
   "metadata": {},
   "source": [
    "### Potential Useful ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f87457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def lstm_forward(x):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(x.device)\n",
    "            out, _ = self.lstm(x, (h0, c0))\n",
    "            return out\n",
    "        out = checkpoint.checkpoint(lstm_forward, x)\n",
    "        out = self.fc(out[-1, :, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3960cd68",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchaudio.models.wav2vec2' has no attribute 'base'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2765932/4240062605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwav2vec2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m classifier = nn.Sequential(\n\u001b[1;32m     22\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchaudio.models.wav2vec2' has no attribute 'base'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import torchaudio\n",
    "\n",
    "class Wav2Vec_1(nn.Module):\n",
    "    def __init__(self, feature_extractor, classifier):\n",
    "        super(Wav2Vec, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        def feature_extractor_forward(x):\n",
    "            features = self.feature_extractor(x)\n",
    "            return features\n",
    "        features = checkpoint.checkpoint(feature_extractor_forward, x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "\n",
    "feature_extractor = torchaudio.models.wav2vec2.base(pretrained=True)\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "model = Wav2Vec(feature_extractor, classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8661a949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft were not used when initializing Wav2Vec2ConformerForSequenceClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ConformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ConformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ConformerForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft and are newly initialized: ['classifier.weight', 'classifier.bias', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Wav2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_886054/380264182.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWav2Vec2ConformerForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'facebook/wav2vec2-conformer-rope-large-960h-ft'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook/wav2vec2-conformer-rope-large-960h-ft\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msampling_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"longest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWav2Vec_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_886054/380264182.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feature_extractor, classifier)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWav2Vec_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWav2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Wav2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC\n",
    "from transformers import Wav2Vec2ConformerForSequenceClassification, Wav2Vec2Tokenizer\n",
    "from transformers import AutoProcessor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "class Wav2Vec_2(nn.Module):\n",
    "    def __init__(self, feature_extractor, classifier):\n",
    "        super(Wav2Vec, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        def feature_extractor_forward(x):\n",
    "            features = self.feature_extractor(x).last_hidden_state\n",
    "            return features\n",
    "        features = checkpoint.checkpoint(feature_extractor_forward, x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "\n",
    "#feature_extractor = transformers.Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "pretrained_model = Wav2Vec2ConformerForSequenceClassification.from_pretrained('facebook/wav2vec2-conformer-rope-large-960h-ft',output_hidden_states=False ,output_attentions=False,return_dict=True,num_labels = 8)\n",
    "feature_extractor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\",sampling_rate = 16000,return_tensors=\"pt\",padding= \"longest\")\n",
    "model_test = Wav2Vec_2(feature_extractor, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808b8acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting fairseq\n",
      "  Downloading fairseq-0.12.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.0 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from fairseq) (4.62.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from fairseq) (1.21.4)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.8/site-packages (from fairseq) (1.15.0)\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.8/site-packages (from fairseq) (0.29.24)\n",
      "Collecting omegaconf<2.1\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from fairseq) (0.11.0+cu113)\n",
      "Collecting hydra-core<1.1,>=1.0.7\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 120.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacrebleu>=1.4.12\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 116.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bitarray\n",
      "  Downloading bitarray-2.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 119.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from fairseq) (2021.11.10)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from fairseq) (1.11.0+cu113)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.8/site-packages (from hydra-core<1.1,>=1.0.7->fairseq) (5.8.0)\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 117.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /opt/conda/lib/python3.8/site-packages (from omegaconf<2.1->fairseq) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from omegaconf<2.1->fairseq) (4.0.1)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 103.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi->fairseq) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq) (3.8.0)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=95541da83e9b733f9a2c4aca8b4a1d4723890d604084fe811997e59b840a2b18\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hnq0vvhg/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: portalocker, omegaconf, lxml, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
      "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 fairseq-0.12.2 hydra-core-1.0.7 lxml-4.9.2 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b462a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2ConformerConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"conformer_conv_dropout\": 0.1,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_depthwise_kernel_size\": 31,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_source_positions\": 5000,\n",
      "  \"model_type\": \"wav2vec2-conformer\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embeddings_type\": \"relative\",\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"rotary_embedding_base\": 10000,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.26.0.dev0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": null,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel \n",
    "from transformers import Wav2Vec2ConformerConfig, Wav2Vec2ConformerModel\n",
    "configuration = Wav2Vec2ConformerConfig()\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b741d0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "tensor = torch.randn(4, 47, 1024)\n",
    "\n",
    "# Extract the tensors from the first and second dimension\n",
    "tensor_2d = tensor.reshape(4, -1)[:, :1024]\n",
    "\n",
    "# Print the shape of the new tensor\n",
    "print(tensor_2d.shape) # Output: torch.Size([4, 1024])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0095ebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft were not used when initializing Wav2Vec2ConformerModel: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ConformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ConformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel \n",
    "from transformers import Wav2Vec2ConformerConfig, Wav2Vec2ConformerModel\n",
    "batch_size = 4\n",
    "pr_tr_model = Wav2Vec2ConformerModel.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f429035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, pre_tr_model ,input_size = batch_size, hidden_size = 768 , num_classes = 8):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.backbone = pre_tr_model\n",
    "        #self.linear = nn.Linear(hidden_size , 1024)\n",
    "        self.output = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask = False):\n",
    "        backbone_op = self.backbone(input_ids)\n",
    "        print(backbone_op[0].shape)\n",
    "        #backbone_op = self.backbone(input_ids, attention_mask=False).last_hidden_state\n",
    "        backbone_op_reshp  = backbone_op[0].reshape(batch_size, -1)[:, :backbone_op[0].shape[2]]\n",
    "        print(backbone_op_reshp.shape)\n",
    "        \n",
    "        #linear_output = self.linear(backbone_op_reshp)\n",
    "        #print(linear_output.shape)\n",
    "        output = self.output(backbone_op_reshp)\n",
    "        print(output)\n",
    "        print(\"output shape = \" , output.shape)\n",
    "        out_smax = nn.Softmax(dim = 1)\n",
    "        out = out_smax(output)\n",
    "        print(\"out = \",out)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d022f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = MyModel(pr_tr_model).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "275d7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,15360).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e3de58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 47, 1024])\n",
      "torch.Size([4, 1024])\n",
      "tensor([[-0.0710, -0.0052, -0.2053,  0.0306, -0.1015,  0.1254,  0.0235,  0.1185],\n",
      "        [-0.0797, -0.0488, -0.1649,  0.0861, -0.1126,  0.0800,  0.0008,  0.1160],\n",
      "        [-0.0924, -0.0546, -0.1958,  0.0443, -0.1407,  0.0581,  0.0326,  0.0323],\n",
      "        [-0.0916, -0.1021, -0.1760,  0.0835, -0.0999,  0.0666,  0.0530,  0.1047]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "output shape =  torch.Size([4, 8])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-276e9e64ba62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bbd04a977eaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output shape = \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout_smax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_smax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'dim'"
     ]
    }
   ],
   "source": [
    "o = t_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fcd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512e5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
