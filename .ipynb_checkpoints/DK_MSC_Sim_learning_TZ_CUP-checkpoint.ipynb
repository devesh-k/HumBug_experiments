{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cbb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5ab65",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52327b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f3db928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f357ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score,accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "805ba4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f321778",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc21367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity learning\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Callable\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from typing import Dict, Union, Optional, List\n",
    "import os\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization,TimeInversion\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c12d95",
   "metadata": {},
   "source": [
    "### Run all these function definition cells\n",
    "These have been extracted from the lib folder and are here to make them more easily editable.  Most of the action happens in *get_feat_torch*, which does feature extraction and *train_model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd70368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "DEBUG = FALSE\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614acd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e8233fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    for _,row in df.iterrows():\n",
    "        if row['length'] > min_length:\n",
    "            step_size = step_frac*min_length\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0, 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "            for i in range(1, int((row['length']-min_length)//step_size)):\n",
    "                audio_offsets.append({'id': row['id'], 'offset':int(min_length+(i*step_size)*config.rate), 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "        elif short_audio:\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e1041b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df_offset, indices):\n",
    "    list_df_ind = []\n",
    "    #print(\"len of indices = \" + str(len(indices)))\n",
    "    for ind in indices :\n",
    "        df_name = \"df_\"+ str(ind)\n",
    "        df_name = df_offset[df_offset['specie_ind'] == ind]\n",
    "        list_df_ind.append(df_name)\n",
    "    df_offset_trimmed = pd.concat(list_df_ind)\n",
    "    return(df_offset_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5ce159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3de311",
   "metadata": {},
   "source": [
    "### 3 The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f1b51",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d6d73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2e6133",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DEBUG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8970/274809433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_df_msc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DEBUG' is not defined"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(config.data_df_msc_test)\n",
    "else:\n",
    "    df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ad556",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SHORT_AUDIO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464da0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5efa983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677c48c",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65dd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d020101",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a525bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2861c",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f998fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c0007",
   "metadata": {},
   "source": [
    "### Next, we perform \"offsets\", spliting each(long) recording into multiple 1.92 secs chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46670be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset.drop('index',axis =1,inplace = True)\n",
    "df_val_offset.drop('index',axis =1,inplace = True)\n",
    "df_test_offset.drop('index',axis =1,inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebdd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset.to_csv(\"train_offset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db6184",
   "metadata": {},
   "source": [
    "### Let's check for data leakage in offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_train_offset , df_test_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_train_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_test_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a392a85",
   "metadata": {},
   "source": [
    "### At this stage we've a dataframe of recordin ids and each row corresponds to a 1.92 secs recording or shorter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "specie_dist = []\n",
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "        if type_df == \"train\" :\n",
    "            specie_dist.append(len(df_temp))\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9362b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "specie_dist = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "#print(class_weights)\n",
    "\n",
    "\n",
    "specie_dist_ar = np.array(specie_dist)\n",
    "print(specie_dist_ar)\n",
    "sum_all = np.sum(specie_dist_ar)\n",
    "print(\"sum_all = \" +str(sum_all))\n",
    "specie_dist_pr = specie_dist_ar/sum_all\n",
    "print(\"specie_dist_pr = \" +str(specie_dist_pr))\n",
    "sum_pr = np.sum(specie_dist_pr)\n",
    "print(sum_pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(df_val_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "specie_dist_val = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "#print(class_weights)\n",
    "\n",
    "\n",
    "specie_dist_ar = np.array(specie_dist)\n",
    "print(specie_dist_ar)\n",
    "sum_all = np.sum(specie_dist_ar)\n",
    "print(\"sum_all = \" +str(sum_all))\n",
    "specie_dist_pr = specie_dist_ar/sum_all\n",
    "print(\"specie_dist_pr = \" +str(specie_dist_pr))\n",
    "sum_pr = np.sum(specie_dist_pr)\n",
    "print(sum_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192918e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae99b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4870761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_siamese(model, loader,criterion, classes = classes,device=None , call = \"val\", DEBUG = DEBUG):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        all_y_pred = []\n",
    "        all_y = []\n",
    "        all_y_pred_dict = {}\n",
    "        count = 0\n",
    "        print(\"&&&&&&& INSIDE TEST&&&&&&&\" + \"for call = \" + str(call))\n",
    "        for ind, (mainImg, imgSets, label) in enumerate(loader):\n",
    "            mainImg = mainImg.to(device)\n",
    "            #print(\"mainImg shape = \" +str(mainImg.shape))\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            label = label.to(device)\n",
    "            label_cpu = label.cpu().detach()\n",
    "            all_y.append(label_cpu)\n",
    "            #print(\"all_y = \" +str(all_y))\n",
    "            y_hat_list = []\n",
    "            for i , testImg in enumerate(imgSets):\n",
    "                #print(\"i = \" +str(i))\n",
    "                testImg = testImg.to(device)\n",
    "                output = model(mainImg, testImg)\n",
    "                #output_cpu = output.cpu().detach()\n",
    "                #print(\"output = \\n\" + str(output) )\n",
    "                y_hat_list.append(output.cpu().detach())\n",
    "                #pred = output_cpu.index(max(output_cpu))\n",
    "                loss = criterion(output.to(device) , label.to(device))\n",
    "                test_loss+= loss.item()\n",
    "                #print(\"output_list = \" +str(output_list))\n",
    "                del output\n",
    "            ## finished checking against all the \n",
    "            del label\n",
    "            #print(\"y_hat_list = \" +str(y_hat_list))\n",
    "            pred = y_hat_list.index(max(y_hat_list))\n",
    "            #print(\"pred = \" +str(pred))\n",
    "            #print(\"prediction  for index  \" + str(ind) + \"=> \" + str(pred))\n",
    "            all_y_pred.append(pred)\n",
    "            \n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = np.array(all_y_pred)\n",
    "        test_loss = test_loss/len(test_loader)  \n",
    "        test_acc = accuracy_score(all_y.numpy(), all_y_pred)\n",
    "        #print(\"before returning , all_y = \" +str(all_y.numpy()))\n",
    "        #print(\"before returning , all_y_pred  = \" +str(all_y_pred))\n",
    "    \n",
    "    return test_loss, test_acc , all_y,all_y_pred    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d3f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese(model, train_loader, val_loader,test_loader , classes , num_epochs = num_epochs , DEBUG = DEBUG):\n",
    "    #loss_scaler = NativeScaler()\n",
    "    torch.manual_seed(0)\n",
    "    global_step = 0\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    model = model.to(device)\n",
    "    criterion = BinaryCrossEntropy(smoothing=0.1)\n",
    "    optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_val_acc = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_acc = -np.inf\n",
    "    best_train_loss = np.inf\n",
    "    best_train_acc = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    lr_log = []\n",
    "    sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    for e in range(num_epochs):\n",
    "        train_running_loss = 0.0 \n",
    "        model.train()\n",
    "        all_y_pred = []\n",
    "        all_y  = []\n",
    "        start_time = time.time()\n",
    "        for ind , (img1, img2, y) in enumerate(train_loader):\n",
    "            #print(\"ind inside train loop = \" + str(ind))\n",
    "            x1 = img1.to(device)\n",
    "            x2 = img2.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            #print(\"img1 shape = \" +str(img1.shape))\n",
    "            #print(\"img2 shape = \" +str(img2.shape))\n",
    "            \n",
    "            if ind % 600 == 0 :\n",
    "                elapsed_time = time.time()\n",
    "                time_since_epoch = (elapsed_time - start_time)/60\n",
    "                print(\"epoch = \"+ str(e) + \"processed batch \" + str(ind) + \" of \" + str(len(train_loader)))\n",
    "                print(\"duration = \" + str(time_since_epoch))\n",
    "            # AMP                \n",
    "            with autocast():\n",
    "                y_pred = model(x1,x2)\n",
    "                #print(\"y_pred Raw = \" + str(y_pred))\n",
    "                loss = criterion(y_pred, y)\n",
    "                #print(\"loss = \" +str(loss))   \n",
    "            train_running_loss += loss.item()\n",
    "            #print(\"train_running_loss = \" +str(train_running_loss))   \n",
    "            avg_train_loss = train_running_loss / len(train_loader)\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #print(\"y = \" +str(y))\n",
    "            y_pred_prob = sigmoid(y_pred)\n",
    "            #print(\"y_pred_prob = \" +str(y_pred_prob))\n",
    "            y_pred_bool = (y_pred_prob > .5).int()\n",
    "            #print(\"y_pred_bool = \" +str(y_pred_bool))\n",
    "            \n",
    "            \n",
    "            y_pred_cpu = y_pred_bool.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_bool = \" + str(y_pred_cpu))\n",
    "            all_y_pred.append(y_pred_cpu.cpu().detach())\n",
    "            #print(\"all_y_pred = \" +str(all_y_pred))\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            del x1\n",
    "            del x2\n",
    "            del y\n",
    "        \n",
    "\n",
    "        all_train_loss.append(train_running_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        #print(\"********BATCH PROCESS FINISH*********\")\n",
    "        #print(\"all_y = \" +str(all_y))\n",
    "        #print(\"all_y_pred = \" +str(all_y_pred))\n",
    "        \n",
    "        train_acc = accuracy_score(all_y.numpy(), all_y_pred.numpy())       \n",
    "        #val_loss, val_f1 , _,_ = test_model_siamese(model, val_loader, criterion = criterion, classes = classes ,device=device, call = \"val\")\n",
    "        val_loss, val_acc , _,_ = test_model_siamese(model, val_loader, criterion = criterion, classes = classes ,device=device, call = \"val\")\n",
    "        \n",
    "        #val_loss = test_model_siamese(model, val_loader, criterion, 0.5, device=device)\n",
    "        acc_metric = val_acc\n",
    "        best_acc_metric = best_val_acc\n",
    "        #check if the current val_loss is less than the best -val loss\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir,  checkpoint_name))\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train acc: %.8f, Val Loss: %.8f, Val acc: %.8f, overrun_counter %i' % (e, train_running_loss, train_acc, val_loss, val_acc,  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.model_dir,  checkpoint_name)) \n",
    "            print(\"Now printing classification report... \")\n",
    "            print(\"********************************\")\n",
    "            sys.stdout.flush()\n",
    "            from sklearn.metrics import classification_report\n",
    "            #_, _ , all_y_test,all_y_pred_test = test_model_siamese(model, test_loader, criterion = criterion, classes = classes ,device=device, call = \"test\")\n",
    "            _, _ , all_y_test,all_y_pred_test = test_model_siamese(model, test_loader, criterion = criterion, classes = classes ,device=device, call = \"test\")\n",
    "            \n",
    "            # at times output is not getting printed. Could be due to multi threading and hence adding sleep\n",
    "            sys.stdout.flush()\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test, target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            sys.stdout.flush()\n",
    "            plot_confusion_matrix(all_y_test.numpy() ,all_y_pred_test, classes)\n",
    "            best_epoch = e\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train acc: %.8f, Val Loss: %.8f, Val acc: %.8f, overrun_counter %i' % (e, train_running_loss, train_acc, val_loss, val_acc,  overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36587ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true,y_hat,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true,y_hat,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685edf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a file with 0s to make it a 1.92 sec file\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aug(x,rate):\n",
    "        apply_augmentation = Compose(transforms=[AddColoredNoise(p = .50) ,TimeInversion( p = .50) ,PolarityInversion(p = .50)])\n",
    "        aug_audio = apply_augmentation(x,sample_rate = rate)\n",
    "        return(aug_audio)\n",
    "    \n",
    "\n",
    "class augment_audio(nn.Module):\n",
    "    \"\"\"This is a class to introduce randomness in the data.\n",
    "    We implement it as a layer in the NN to ensure that it learns from the propertis of the data\"\"\"\n",
    "    def __init__(self , trainable = True, sample_rate = config.rate):\n",
    "        super().__init__()\n",
    "        self.trainable = trainable\n",
    "        self.rate = sample_rate\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "           \n",
    "        if self.trainable:\n",
    "            x = apply_aug(x , self.rate)\n",
    "        else:\n",
    "            x = x\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x.squeeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Moz_train_dataset(Dataset):\n",
    "    def __init__(self, audio_df, setSize , data_dir, min_length, cache=None, transform=None,rate = config.rate, DEBUG = DEBUG,p = specie_dist_pr):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "        self.setSize = setSize\n",
    "        self.p = p\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    \n",
    "    def _get_tensor_(self, path, resample=None):\n",
    "        waveform, inp_samp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_samp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_samp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[1] < config.rate*config.min_duration :\n",
    "            waveform = pad_mean(waveform)\n",
    "            if DEBUG:\n",
    "                print(\"need to pad\")\n",
    "            #waveform = waveform.unsqueeze(dim = 0)\n",
    "        \n",
    "        f = waveform[0]\n",
    "        f_out = f.unsqueeze(0)\n",
    "        \n",
    "        return f_out, config.rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        all_uniq_ind = self.audio_df.specie_ind.unique()\n",
    "        #print(\"idx in __get__item = \" + str(idx))\n",
    "        if idx % 2 == 0:\n",
    "            # select the same character for both images\n",
    "            category = int(np.random.choice(a= all_uniq_ind, size= 1, replace=False))\n",
    "            #select two images belonging to the category chosen.\n",
    "            df_temp = self.audio_df[self.audio_df['specie_ind']== category].sample(2,replace = False )\n",
    "            df_temp.reset_index(inplace = True)\n",
    "            if DEBUG:\n",
    "                print(\"df_temp = \" + str(df_temp))\n",
    "            row1 = df_temp.iloc[0]\n",
    "            row2 = df_temp.iloc[1]\n",
    "            if DEBUG:\n",
    "                print(\"row1.id = \" + str(row1.id))\n",
    "                print(\"row2.id = \" + str(row2.id))\n",
    "            label = 1.0\n",
    "            # x_full  and x2_full represnts the entire tensor representations of the wav file\n",
    "            x1_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{int(row1.id)}.wav\"), resample=config.rate)\n",
    "            x2_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{int(row2.id)}.wav\"), resample=config.rate)\n",
    "            if DEBUG:\n",
    "                print(\"x1_full shape = \" + str(x1_full.shape))\n",
    "                print(\"x2_full shape = \" + str(x2_full.shape))\n",
    "                        \n",
    "            r1_offset = int(row1.offset)\n",
    "            r2_offset = int(row2.offset)\n",
    "            if DEBUG:\n",
    "                print(\"self.min_length = \" +str(self.min_length))\n",
    "                print(\"even ind row1_offset = \" + str(r1_offset))\n",
    "                print(\"even ind row2_offset = \" + str(r2_offset))\n",
    "                print(\"row1_offset type = \" + str(type(r1_offset)))\n",
    "                print(\"row2_offset type = \" + str(type(r2_offset)))\n",
    "            \n",
    "            x1 = x1_full[:,r1_offset:int(r1_offset+config.rate*self.min_length)]\n",
    "            x2 = x2_full[:,r2_offset:int(r2_offset+config.rate*self.min_length)]\n",
    "            if DEBUG:\n",
    "                print(\"x1 shape = \" + str(x1.shape))\n",
    "                print(\"x2 shape = \" + str(x2.shape))\n",
    "            \n",
    "        else:\n",
    "            if DEBUG:\n",
    "                print(\"^^^^^^^^ODD INDEX^^^^^^^^^^\")\n",
    "            category1, category2 = np.random.choice(a= all_uniq_ind, size= 2, replace=False)\n",
    "            df_temp_cat1 = self.audio_df[self.audio_df['specie_ind']== int(category1)].sample(1,replace = False )\n",
    "            df_temp_cat2 = self.audio_df[self.audio_df['specie_ind']== int(category2)].sample(1,replace = False )\n",
    "            df_temp_cat1.reset_index(inplace = True)\n",
    "            df_temp_cat2.reset_index(inplace = True)\n",
    "            label = 0.0\n",
    "            \n",
    "            row1 = df_temp_cat1.iloc[0]\n",
    "            row2 = df_temp_cat2.iloc[0]\n",
    "            if DEBUG:\n",
    "                print(\"row1.id = \" + str(row1.id))\n",
    "                print(\"row2.id = \" + str(row2.id))\n",
    "            \n",
    "            # x_full  and x2_full represnts the entire tensor representations of the wav file\n",
    "            x1_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{int(row1.id)}.wav\"), resample=config.rate)\n",
    "            x2_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{int(row2.id)}.wav\"), resample=config.rate)\n",
    "            if DEBUG:\n",
    "                print(\"x1_full shape = \" + str(x1_full.shape))\n",
    "                print(\"x2_full shape = \" + str(x2_full.shape))\n",
    "            \n",
    "            r1_offset = int(row1.offset)\n",
    "            r2_offset = int(row2.offset)\n",
    "            if DEBUG:\n",
    "                print(\"self.min_length = \" +str(self.min_length))\n",
    "                print(\"row1_offset = \" + str(r1_offset))\n",
    "                print()\n",
    "                print(\"row1_offset type = \" + str(type(r1_offset)))\n",
    "                print(\"row2_offset = \" + str(r2_offset))\n",
    "                print(\"row2_offset type = \" + str(type(r2_offset)))\n",
    "            \n",
    "            x1 = x1_full[:,r1_offset:int(r1_offset+config.rate*self.min_length)]\n",
    "            x2 = x2_full[:,r2_offset:int(r2_offset+config.rate*self.min_length)]\n",
    "            if DEBUG:\n",
    "                print(\"x1 shape = \" + str(x1.shape))\n",
    "                print(\"x2 shape = \" + str(x2.shape))\n",
    "            \n",
    "            \n",
    "                                   \n",
    "        \n",
    "        return (x1,x2,torch.from_numpy(np.array([label], dtype=np.float32)))  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bea31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79897307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Moz_test_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, audio_df, setSize , data_dir, min_length, cache=None, transform=None,rate = config.rate,numway = len(classes)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "        self.setSize = setSize\n",
    "        self.numway = numway\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    \n",
    "    def _get_tensor_(self, path, resample=None):\n",
    "        waveform, inp_samp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_samp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_samp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[1] < config.rate*config.min_duration :\n",
    "            if DEBUG:\n",
    "                print(\"inside MozTest Dataset..\")\n",
    "                print(\"need to pad\")\n",
    "            waveform = pad_mean(waveform)\n",
    "            #waveform = waveform.unsqueeze(dim = 0)\n",
    "        \n",
    "        f = waveform[0]\n",
    "        f_out = f.unsqueeze(0)\n",
    "        \n",
    "        return f_out, config.rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        all_uniq_ind = self.audio_df.specie_ind.unique()\n",
    "        # find one main image\n",
    "        all_uniq_ind = df.specie_ind.unique()\n",
    "        category =  int(np.random.choice(a= all_uniq_ind, size= 1, replace=False))\n",
    "        #sample a random value from the category chosen above\n",
    "        df_main = self.audio_df[self.audio_df['specie_ind']== category].sample(1,replace = False )\n",
    "        df_main.reset_index(inplace = True)\n",
    "        if DEBUG:\n",
    "            print(\"df_main = \" + str(df_main))\n",
    "                 \n",
    "        # find n numbers of distinct images, 1 in the same set as the main\n",
    "        testSet = []\n",
    "        label = int(np.random.choice(a= all_uniq_ind, size= 1, replace=False))\n",
    "        if DEBUG:\n",
    "            print(\"label ->\" +str(label))\n",
    "            print(\"self.numway = \" +str(self.numway))\n",
    "        for i in range(self.numway):\n",
    "            #print(\"i = \" +str(i))\n",
    "            if i == label:\n",
    "                #estImgName = random.choice(os.listdir(imgDir))\n",
    "                df_name = \"df_temp_\" + str(label)\n",
    "                df_temp = self.audio_df[self.audio_df['specie_ind']== label].sample(1,replace = False )\n",
    "                #print(\"i == label\")\n",
    "                #print(\"df_temp = \" +str(df_temp))\n",
    "                            \n",
    "            else:\n",
    "                df_name = \"df_temp_\" + str(i)\n",
    "                testCategory = int(np.random.choice(a= all_uniq_ind, size= 1, replace=False))\n",
    "                df_temp = self.audio_df[self.audio_df['specie_ind']== testCategory].sample(1,replace = False )\n",
    "                \n",
    "                \n",
    "\n",
    "            testSet.append(df_temp)\n",
    "        df_test = pd.concat(testSet, ignore_index=True)\n",
    "        if DEBUG:\n",
    "            print(\"df_test = \" +str(df_test))\n",
    "            #you loophere on the dataframe to get x's \n",
    "            \n",
    "                \n",
    "        # x_full  and x2_full represnts the entire tensor representations of the wav file\n",
    "        #x1_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{row1.id}.wav\"), resample=config.rate)\n",
    "        main_row = df_main.iloc[0]\n",
    "        #print(main_row)\n",
    "        x_main_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{int(main_row.id)}.wav\"), resample=config.rate)\n",
    "        #print(\"x_main_full shape = \" + str(x_main_full.shape))\n",
    "        \n",
    "        main_offset = int(main_row.offset)\n",
    "        #print(\"main_offset  = \" + str(main_offset))\n",
    "        \n",
    "        x_main = x_main_full[:,main_offset:int(main_offset+config.rate*self.min_length)]\n",
    "        #print(\"x_main shape = \" + str(x_main.shape))\n",
    "        \n",
    "        x_test = []\n",
    "        for ind,row in df_test.iterrows():\n",
    "            row_id = row['id']\n",
    "            #print(\"inside loop.... row_id = \" + str(row_id))\n",
    "            x_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{int(row_id)}.wav\"), resample=config.rate)\n",
    "            #print(\"inside loop.... x_full = \" + str(x_full.shape))\n",
    "            offset = int(row['offset'])\n",
    "            #print(\"inside loop...offset  = \" + str(offset))\n",
    "            x_temp = x_full[:,offset:int(offset+config.rate*self.min_length)]\n",
    "            #print(\"inside loop...x_temp shape   = \" + str(x_temp.shape))\n",
    "            \n",
    "            x_test.append(x_temp)\n",
    "        \n",
    "        return x_main,x_test,torch.from_numpy(np.array([label], dtype=np.float32))  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size,threshold = .5):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True,  in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.2)\n",
    "        \n",
    "        self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.timeMasking = T.TimeMasking(time_mask_param=int(config.win_size*0.4), iid_masks=True)\n",
    "        self.freqMasking = T.FrequencyMasking(freq_mask_param=int((config.NFFT//4)*0.15), iid_masks=True)\n",
    "        self.norm_layer = Normalization(mode='framewise')\n",
    "        self.pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "        #self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "        #1000 due to the size of the final layer in convnext\n",
    "        self.fcOut = nn.Linear(1000, 1)\n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "        self.threshold  = threshold\n",
    "        \n",
    "        \n",
    "    def forward(self, x1,x2):\n",
    "        # first compute spectrogram\n",
    "        #spec1 = self.augment_layer(x1.squeeze())\n",
    "        spec1 = self.spec_layer(x1)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec1 = self.pcen_layer(spec1)\n",
    "        spec1 = self.norm_layer(spec1)\n",
    "        \n",
    "#         if self.training:\n",
    "#             spec = self.timeMasking(spec)\n",
    "#             spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec1 = self.sizer(spec1)\n",
    "        #print(\"spec1 shape post STFT = \" +str(spec1.shape))\n",
    "        x1 = spec1.unsqueeze(1)\n",
    "        #print(\"post unsqueeze x1 shape = \" +str(x1.shape))\n",
    "        # then repeat channels\n",
    "        x1 = self.backbone(x1)\n",
    "        x1 = self.sigmoid(x1)\n",
    "        #print(\"post backbone . x1 shape = \" +str(x1.shape))\n",
    "        \n",
    "        #spec2 = self.augment_layer(x2.squeeze())\n",
    "        spec2 = self.spec_layer(x2)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec2 = self.pcen_layer(spec2)\n",
    "        spec2 = self.norm_layer(spec2)\n",
    "        \n",
    "#         if self.training:\n",
    "#             spec = self.timeMasking(spec)\n",
    "#             spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec2 = self.sizer(spec2)\n",
    "        #print(\"spec2 shape post STFT = \" +str(spec2.shape))\n",
    "        x2 = spec2.unsqueeze(1)\n",
    "        #print(\"post unsqueeze x2 shape = \" +str(x2.shape))\n",
    "        # then repeat channels\n",
    "        x2 = self.backbone(x2)\n",
    "        #print(\"post backbone . x2 shape = \" +str(x2.shape))\n",
    "        x2 = self.sigmoid(x2)      \n",
    "        x = torch.abs(x1-x2)\n",
    "        #print(\" x shape = \" +str(x.shape))\n",
    "        out = self.fcOut(x)\n",
    "        #print(\"output of fcout = \" +str(x))\n",
    "        #print(\"output = \" +str(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ca3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SHORT_AUDIO = True\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "test_batch_size = 1\n",
    "DEBUG = False\n",
    "num_workers_test = 4\n",
    "num_workers_train =8\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 64\n",
    "\n",
    "if DEBUG:\n",
    "    train_size = 20\n",
    "else:\n",
    "    train_size = 100000\n",
    "\n",
    "train_dataset = Moz_train_dataset(audio_df = df_train_offset,data_dir = config.data_dir, setSize = train_size, min_length = config.min_duration, p = specie_dist_pr)\n",
    "val_dataset = Moz_test_dataset(audio_df = df_val_offset,data_dir = config.data_dir, setSize = 560, min_length = config.min_duration, numway = len(classes))\n",
    "test_dataset = Moz_test_dataset(audio_df = df_test_offset, setSize = 560,  data_dir = config.data_dir, min_length= config.min_duration, numway = len(classes))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers= num_workers_train, pin_memory=True, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=test_batch_size, num_workers= num_workers_test, pin_memory=False , shuffle = False )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, num_workers= num_workers_test, pin_memory=False , shuffle = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb958a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50d56fe3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822433dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model):\n",
    "    # Instantiate model to inspect\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26001bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether model returns the right values\n",
    "m_t = Model('convnext_small',224)\n",
    "x1 = torch.rand(4,1,15360)\n",
    "x2 = torch.rand(4,1,15360)\n",
    "x3 = torch.tensor(1)\n",
    "m_t(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Model('convnext_small',224)\n",
    "model, lr_log = train_siamese(model , train_loader, val_loader,test_loader,classes,num_epochs = num_epochs  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e310a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32546b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([[-0.0742],\n",
    "        [-0.6300],\n",
    "        [-0.2383],\n",
    "        [-0.1598]], device='cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e769dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
