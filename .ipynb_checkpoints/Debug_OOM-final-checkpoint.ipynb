{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a62ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fec746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.27.0.dev0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6024fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n",
    "from transformers import AutoProcessor, Wav2Vec2ConformerForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1d52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83f5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310c82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f671dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32f7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "#import config ,config_pytorch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41816420",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10,15360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a942d76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "var code_show_err = false;\n",
       "var code_toggle_err = function() {\n",
       "    var stderrNodes = document.querySelectorAll('[data-mime-type=\"application/vnd.jupyter.stderr\"]')\n",
       "    var stderr = Array.from(stderrNodes)\n",
       "    if (code_show_err){\n",
       "        stderr.forEach(ele => ele.style.display = 'block');\n",
       "    } else {\n",
       "        stderr.forEach(ele => ele.style.display = 'none');\n",
       "    }\n",
       "    code_show_err = !code_show_err\n",
       "}\n",
       "document.addEventListener('DOMContentLoaded', code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a onclick=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "var code_show_err = false;\n",
    "var code_toggle_err = function() {\n",
    "    var stderrNodes = document.querySelectorAll('[data-mime-type=\"application/vnd.jupyter.stderr\"]')\n",
    "    var stderr = Array.from(stderrNodes)\n",
    "    if (code_show_err){\n",
    "        stderr.forEach(ele => ele.style.display = 'block');\n",
    "    } else {\n",
    "        stderr.forEach(ele => ele.style.display = 'none');\n",
    "    }\n",
    "    code_show_err = !code_show_err\n",
    "}\n",
    "document.addEventListener('DOMContentLoaded', code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a onclick=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59eba59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4823b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft were not used when initializing Wav2Vec2ConformerForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ConformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ConformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ConformerForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-conformer-rope-large-960h-ft and are newly initialized: ['projector.weight', 'projector.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC\n",
    "from transformers import Wav2Vec2ConformerForSequenceClassification, Wav2Vec2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "#model = ConformerForSequenceClassification.from_pretrained('facebook/conformer-base')\n",
    " # load model and processor\n",
    "#model_checkpoint = \"facebook/wav2vec2-conformer-rel-pos-large\"\n",
    "#tokenizer = Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "#processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-conformer-rel-pos-large-960h-ft\")\n",
    "\n",
    "\n",
    "#num_labels=len(classes)\n",
    "#model = Wav2Vec2ConformerForCTC.from_pretrained(\"facebook/wav2vec2-conformer-rel-pos-large-960h-ft\")\n",
    "model = Wav2Vec2ConformerForSequenceClassification.from_pretrained('facebook/wav2vec2-conformer-rope-large-960h-ft' , num_labels = 8)\n",
    "#model = LongformerForSequenceClassification.from_pretrained('facebook/conformer-base')\n",
    "#model.config.num_labels = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1d94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ce5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1a162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d21789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 15360])\n"
     ]
    }
   ],
   "source": [
    "# load dummy dataset and read soundfiles\n",
    "# ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# print(ds)\n",
    "# tokenize\n",
    "#input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "input_values = processor(x, return_tensors=\"pt\").input_values\n",
    "\n",
    "print(input_values.shape)\n",
    "# retrieve logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc18b745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_temp =  SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0012, -0.0090,  0.0136,  0.0052,  0.0527,  0.0307, -0.0526, -0.0330],\n",
      "        [ 0.0119, -0.0150,  0.0014,  0.0058,  0.0513,  0.0296, -0.0473, -0.0393],\n",
      "        [ 0.0006, -0.0062,  0.0051, -0.0019,  0.0527,  0.0294, -0.0537, -0.0405],\n",
      "        [ 0.0046, -0.0082,  0.0171,  0.0067,  0.0588,  0.0285, -0.0490, -0.0399],\n",
      "        [ 0.0060, -0.0146,  0.0143, -0.0068,  0.0521,  0.0288, -0.0469, -0.0368],\n",
      "        [ 0.0034, -0.0026,  0.0103,  0.0119,  0.0534,  0.0354, -0.0482, -0.0419],\n",
      "        [ 0.0079, -0.0104,  0.0046,  0.0002,  0.0541,  0.0316, -0.0498, -0.0365],\n",
      "        [ 0.0159, -0.0187, -0.0134, -0.0159,  0.0512,  0.0246, -0.0479, -0.0530],\n",
      "        [ 0.0050, -0.0109,  0.0187,  0.0035,  0.0541,  0.0240, -0.0435, -0.0357],\n",
      "        [ 0.0040, -0.0165, -0.0004, -0.0048,  0.0517,  0.0198, -0.0499, -0.0444]],\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits =  tensor([[ 0.0012, -0.0090,  0.0136,  0.0052,  0.0527,  0.0307, -0.0526, -0.0330],\n",
      "        [ 0.0119, -0.0150,  0.0014,  0.0058,  0.0513,  0.0296, -0.0473, -0.0393],\n",
      "        [ 0.0006, -0.0062,  0.0051, -0.0019,  0.0527,  0.0294, -0.0537, -0.0405],\n",
      "        [ 0.0046, -0.0082,  0.0171,  0.0067,  0.0588,  0.0285, -0.0490, -0.0399],\n",
      "        [ 0.0060, -0.0146,  0.0143, -0.0068,  0.0521,  0.0288, -0.0469, -0.0368],\n",
      "        [ 0.0034, -0.0026,  0.0103,  0.0119,  0.0534,  0.0354, -0.0482, -0.0419],\n",
      "        [ 0.0079, -0.0104,  0.0046,  0.0002,  0.0541,  0.0316, -0.0498, -0.0365],\n",
      "        [ 0.0159, -0.0187, -0.0134, -0.0159,  0.0512,  0.0246, -0.0479, -0.0530],\n",
      "        [ 0.0050, -0.0109,  0.0187,  0.0035,  0.0541,  0.0240, -0.0435, -0.0357],\n",
      "        [ 0.0040, -0.0165, -0.0004, -0.0048,  0.0517,  0.0198, -0.0499, -0.0444]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "y_temp = model(input_values.squeeze())\n",
    "logits = y_temp.logits\n",
    "print(\"y_temp = \" , y_temp)\n",
    "#print(\"y_temp shape = \" , y_temp.shape)\n",
    "print(\"logits = \" , logits)\n",
    "_, predicted = torch.max(logits, dim=1)\n",
    "# take argmax and decode\n",
    "#print(\"y_temp = \" , y_temp)\n",
    "#print(\"y_temp shape = \" , y_temp.shape)\n",
    "#print(\"logits = \" , logits)\n",
    "#print(\"logits shape = \", logits.shape)\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0d6becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(predicted_ids)\n",
    "print(predicted_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1bdb23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109b98d",
   "metadata": {},
   "source": [
    "## try using raw audio as input- no torchaudio load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b1b8a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ComParE2022_VecNet/data/audio\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "aud_path = os.path.join(\"..\",\"ComParE2022_VecNet\",\"data\",\"audio\")\n",
    "print(aud_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "245e4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = os.listdir(aud_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d7b822e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1478.wav', '201476.wav', '220349.wav', '221078.wav']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_list)\n",
    "f_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b93ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_path_list = []\n",
    "for f in f_list[:10]:\n",
    "    aud_path_list.append(os.path.join(\"..\",\"ComParE2022_VecNet\",\"data\",\"audio\",f))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "377b399b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../ComParE2022_VecNet/data/audio/1478.wav',\n",
       " '../ComParE2022_VecNet/data/audio/201476.wav',\n",
       " '../ComParE2022_VecNet/data/audio/220349.wav',\n",
       " '../ComParE2022_VecNet/data/audio/221078.wav',\n",
       " '../ComParE2022_VecNet/data/audio/857.wav',\n",
       " '../ComParE2022_VecNet/data/audio/207533.wav',\n",
       " '../ComParE2022_VecNet/data/audio/208262.wav',\n",
       " '../ComParE2022_VecNet/data/audio/219116.wav',\n",
       " '../ComParE2022_VecNet/data/audio/3098.wav',\n",
       " '../ComParE2022_VecNet/data/audio/221951.wav']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2965486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio =  ../ComParE2022_VecNet/data/audio/1478.wav-> input value ret=   torch.Size([1, 1, 114167])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/1478.wav-> outputs=   torch.Size([1, 1, 1, 114167])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/201476.wav-> input value ret=   torch.Size([1, 1, 181417])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/201476.wav-> outputs=   torch.Size([1, 1, 1, 181417])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/220349.wav-> input value ret=   torch.Size([1, 1, 225791])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/220349.wav-> outputs=   torch.Size([1, 1, 1, 225791])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/221078.wav-> input value ret=   torch.Size([1, 1, 564480])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/221078.wav-> outputs=   torch.Size([1, 1, 1, 564480])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio =  ../ComParE2022_VecNet/data/audio/857.wav-> input value ret=   torch.Size([1, 1, 116399])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/857.wav-> outputs=   torch.Size([1, 1, 1, 116399])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/207533.wav-> input value ret=   torch.Size([1, 1, 20479])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/207533.wav-> outputs=   torch.Size([1, 1, 1, 20479])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/208262.wav-> input value ret=   torch.Size([1, 1, 20479])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/208262.wav-> outputs=   torch.Size([1, 1, 1, 20479])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/219116.wav-> input value ret=   torch.Size([1, 1, 434176])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/219116.wav-> outputs=   torch.Size([1, 1, 1, 434176])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio =  ../ComParE2022_VecNet/data/audio/3098.wav-> input value ret=   torch.Size([1, 1, 328102])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/3098.wav-> outputs=   torch.Size([1, 1, 1, 328102])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/221951.wav-> input value ret=   torch.Size([1, 1, 223146])\n",
      "audio =  ../ComParE2022_VecNet/data/audio/221951.wav-> outputs=   torch.Size([1, 1, 1, 223146])\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Load the pre-trained feature extractor and processor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "\n",
    "# Load your audio file\n",
    "for audio in aud_path_list:\n",
    "    audio_file = audio\n",
    "    # Use the feature extractor to extract the feature vectors from the audio file\n",
    "    aud_tensor,sr = torchaudio.load(audio)\n",
    "    input_values = feature_extractor(aud_tensor, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    print(\"audio = \",audio  + \"-> input value ret=  \",input_values.shape)\n",
    "    # Use the processor to tokenize the feature vectors and prepare them for input to the Wav2Vec2 model\n",
    "    outputs = processor(input_values, return_tensors=\"pt\")\n",
    "    print(\"audio = \",audio + \"-> outputs=  \",outputs['input_values'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da8eb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = [(torch.rand(15360),7),(torch.rand(5360),3),(torch.rand(35360),7),(torch.rand(55360),7),(torch.rand(4555360),7),(torch.rand(553360),7),(torch.rand(5570),7),(torch.rand(553560),7),(torch.rand(550),7),(torch.rand(59749),7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb83726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15360\n"
     ]
    }
   ],
   "source": [
    "t,_ = t_list[0]\n",
    "print(t.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6897b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "384aed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([x.shape[0] for x,_ in t_list])\n",
    "# print(max_len)\n",
    "# padded_batch = torch.zeros(len(t_list), max_len)\n",
    "# print(\"padded_batch sh = \" , padded_batch.shape)\n",
    "# for _, (x,_) in enumerate(t_list):\n",
    "#     print(\"i = \",i)\n",
    "#     print(\"x shape = \",x.shape)\n",
    "#     resid_len = max_len - x.shape[0]\n",
    "#     print(\"resid_len = \",resid_len)\n",
    "#     t = torch.zeros(resid_len)\n",
    "#     x = torch.cat((x,t),dim = - 1)\n",
    "#     print(x.shape)\n",
    "    \n",
    "# print(\"*****\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73c22669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_list_updated= [(torch.cat((x,(torch.zeros(max_len-x.shape[0]))),dim = -1),label) for (x,label) in t_list]\n",
    "#t_list_updated= [(torch.cat((x,(torch.zeros(max_len-x.shape[0]))),dim = -1), label) for (x,label) in t_list]\n",
    "t_list_updated= [(torch.cat((x,(torch.zeros(max_len-x.shape[0]))),dim = -1)) for x,_ in t_list]\n",
    "label_list = [label for _,label in t_list]\n",
    "\n",
    "# t_list_updated_n= []\n",
    "# for (x,label) in t_list:\n",
    "#     resid_len = max_len-x.shape[0]\n",
    "#     temp = torch.zeros(resid_len)\n",
    "#     print(\"temp shape = \" , temp.shape)\n",
    "#     new_t = torch.cat((x,temp), dim = -1 ) \n",
    "#     print(\"new_t shape = \" ,new_t.shape)\n",
    "#     t_list_updated_n.append((new_t,label))\n",
    "#     print(\"i = \",i)\n",
    "#     print(\"x shape = \",x.shape)\n",
    "#     resid_len = max_len - x.shape[1]\n",
    "#     print(\"resid_len = \",resid_len)\n",
    "#     t = torch.zeros(1,resid_len)\n",
    "#     x = torch.cat((x,t),dim =1)\n",
    "#     print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e515f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.4909, 0.6123, 0.4899,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.3979, 0.4114, 0.2797,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.1820, 0.4260, 0.8088,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4177, 0.2722, 0.0371,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.6871, 0.6704, 0.9758,  ..., 0.2805, 0.3669, 0.3737]), tensor([0.0843, 0.9900, 0.6038,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2704, 0.4417, 0.5621,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4776, 0.3924, 0.2700,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2804, 0.2762, 0.7287,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2708, 0.2195, 0.0691,  ..., 0.0000, 0.0000, 0.0000])]\n",
      "[7, 3, 7, 7, 7, 7, 7, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "print(t_list_updated)\n",
    "print(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d67f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a93b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd51f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30f45bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12b3ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 0\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22ca1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ff4c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset_dupe(Dataset):\n",
    "    def __init__(self, file_list,label_list):\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "        \n",
    "        #self.labels = labels\n",
    "        #processor = processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "        #self.processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\",sampling_rate = 16000,return_tensors=\"pt\",padding= \"longest\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(\"index = \", index)\n",
    "        #wav2_vec_rate = 16000\n",
    "        #file_path,label = self.file_list[index]\n",
    "        input_values  = self.file_list[index]\n",
    "        label = self.label_list[index]\n",
    "             \n",
    "        return input_values, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6c90991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_dupe(file_list,label_list, batch_size=batch_size):\n",
    "    dataset = AudioDataset_dupe(file_list,label_list)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers = num_workers, collate_fn = collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f89d21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \n",
    "    \"\"\"\n",
    "    Collate function to handle variable length audio sequences.\n",
    "    :param batch: List of tuples (audio feature tensor, label)\n",
    "    :return: Tuple of padded audio feature tensor and label tensor\n",
    "    \"\"\"\n",
    "    # Sort the batch by sequence length in descending order\n",
    "    print(\"inside collate...\")\n",
    "    print(\"batch   = \", batch )\n",
    "#     for b,l in batch:\n",
    "#         print(\"b shape = \",b.shape)\n",
    "    max_len = max([x.shape[0] for x,_ in batch])\n",
    "    #t_list_updated= [(torch.cat((x,(torch.zeros(max_len-x.shape[0]))),dim = -1),label) for (x,label) in t_list]\n",
    "    tensor_updated = [(torch.cat((x,(torch.zeros(max_len-x.shape[0]))),dim = -1)) for x,_ in t_list]\n",
    "    label_list = [label for _,label in t_list]\n",
    "    \n",
    "    #print(\"len (batch_updated ) = \", len(batch_updated))\n",
    "#     if DEBUG:\n",
    "#         for (x,label) in batch_updated:\n",
    "#             print(\"x shape = \",x.shape)\n",
    "#             print(\"label = \",label)\n",
    "\n",
    "    return tensor_updated,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "185b09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = load_dataset_dupe(t_list_updated,label_list ,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0da1b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ab44e2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index =  1\n",
      "index =  2\n",
      "index =  6\n",
      "index =  9\n",
      "inside collate...\n",
      "batch   =  [(tensor([0.3979, 0.4114, 0.2797,  ..., 0.0000, 0.0000, 0.0000]), 3), (tensor([0.1820, 0.4260, 0.8088,  ..., 0.0000, 0.0000, 0.0000]), 7), (tensor([0.2704, 0.4417, 0.5621,  ..., 0.0000, 0.0000, 0.0000]), 7), (tensor([0.2708, 0.2195, 0.0691,  ..., 0.0000, 0.0000, 0.0000]), 7)]\n",
      "******\n",
      "[tensor([0.4909, 0.6123, 0.4899,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.3979, 0.4114, 0.2797,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.1820, 0.4260, 0.8088,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4177, 0.2722, 0.0371,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.6871, 0.6704, 0.9758,  ..., 0.2805, 0.3669, 0.3737]), tensor([0.0843, 0.9900, 0.6038,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2704, 0.4417, 0.5621,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4776, 0.3924, 0.2700,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2804, 0.2762, 0.7287,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2708, 0.2195, 0.0691,  ..., 0.0000, 0.0000, 0.0000])]\n",
      "[7, 3, 7, 7, 7, 7, 7, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(loader)\n",
    "x,y = data_iter.next()\n",
    "print(\"******\")\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02c98279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index =  2\n",
      "index =  5\n",
      "index =  7\n",
      "index =  0\n",
      "inside collate...\n",
      "batch   =  [(tensor([0.1820, 0.4260, 0.8088,  ..., 0.0000, 0.0000, 0.0000]), 7), (tensor([0.0843, 0.9900, 0.6038,  ..., 0.0000, 0.0000, 0.0000]), 7), (tensor([0.4776, 0.3924, 0.2700,  ..., 0.0000, 0.0000, 0.0000]), 7), (tensor([0.4909, 0.6123, 0.4899,  ..., 0.0000, 0.0000, 0.0000]), 7)]\n",
      "%%%%%%%%%%%%%\n",
      "[tensor([0.4909, 0.6123, 0.4899,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.3979, 0.4114, 0.2797,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.1820, 0.4260, 0.8088,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4177, 0.2722, 0.0371,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.6871, 0.6704, 0.9758,  ..., 0.2805, 0.3669, 0.3737]), tensor([0.0843, 0.9900, 0.6038,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2704, 0.4417, 0.5621,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4776, 0.3924, 0.2700,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2804, 0.2762, 0.7287,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2708, 0.2195, 0.0691,  ..., 0.0000, 0.0000, 0.0000])]\n",
      "[7, 3, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "index =  1\n",
      "index =  8\n",
      "index =  4\n",
      "index =  6\n",
      "inside collate...\n",
      "batch   =  [(tensor([0.3979, 0.4114, 0.2797,  ..., 0.0000, 0.0000, 0.0000]), 3), (tensor([0.2804, 0.2762, 0.7287,  ..., 0.0000, 0.0000, 0.0000]), 7), (tensor([0.6871, 0.6704, 0.9758,  ..., 0.2805, 0.3669, 0.3737]), 7), (tensor([0.2704, 0.4417, 0.5621,  ..., 0.0000, 0.0000, 0.0000]), 7)]\n",
      "%%%%%%%%%%%%%\n",
      "[tensor([0.4909, 0.6123, 0.4899,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.3979, 0.4114, 0.2797,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.1820, 0.4260, 0.8088,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4177, 0.2722, 0.0371,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.6871, 0.6704, 0.9758,  ..., 0.2805, 0.3669, 0.3737]), tensor([0.0843, 0.9900, 0.6038,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2704, 0.4417, 0.5621,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4776, 0.3924, 0.2700,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2804, 0.2762, 0.7287,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2708, 0.2195, 0.0691,  ..., 0.0000, 0.0000, 0.0000])]\n",
      "[7, 3, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "index =  3\n",
      "index =  9\n",
      "inside collate...\n",
      "batch   =  [(tensor([0.4177, 0.2722, 0.0371,  ..., 0.0000, 0.0000, 0.0000]), 7), (tensor([0.2708, 0.2195, 0.0691,  ..., 0.0000, 0.0000, 0.0000]), 7)]\n",
      "%%%%%%%%%%%%%\n",
      "[tensor([0.4909, 0.6123, 0.4899,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.3979, 0.4114, 0.2797,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.1820, 0.4260, 0.8088,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4177, 0.2722, 0.0371,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.6871, 0.6704, 0.9758,  ..., 0.2805, 0.3669, 0.3737]), tensor([0.0843, 0.9900, 0.6038,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2704, 0.4417, 0.5621,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.4776, 0.3924, 0.2700,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2804, 0.2762, 0.7287,  ..., 0.0000, 0.0000, 0.0000]), tensor([0.2708, 0.2195, 0.0691,  ..., 0.0000, 0.0000, 0.0000])]\n",
      "[7, 3, 7, 7, 7, 7, 7, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "for x,y in loader:\n",
    "    print(\"%%%%%%%%%%%%%\")\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667b67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250280eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
