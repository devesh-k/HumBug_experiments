{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9684e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97544d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58994cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffff315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_1.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_2.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_3.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_4.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97507d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip /content/humbugdb_neurips_2021_1.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_2.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_3.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_4.zip?download=1 -d '/content/HumBugDB/data/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473aab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch_audiomentations in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.11.0+cu113)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.2.7)\n",
      "Requirement already satisfied: librosa>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.8.1)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.2.3)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.11.0+cu113)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (2.1.9)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (5.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (21.3)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.24.0)\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.53.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.21.4)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.1.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.10.3.post1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (59.4.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (0.36.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa>=0.6.0->torch_audiomentations) (3.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.26.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in /opt/conda/lib/python3.8/site-packages (from resampy>=0.2.2->librosa>=0.6.0->torch_audiomentations) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torch_audiomentations) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (2.21)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (4.0.1)\n",
      "Requirement already satisfied: primePy>=1.3 in /opt/conda/lib/python3.8/site-packages (from torch-pitch-shift>=1.2.2->torch_audiomentations) (1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.26.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.21.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (6.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.28.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib!=3.6.1,>=3.1->seaborn) (59.4.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib!=3.6.1,>=3.1->seaborn) (1.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_audiomentations\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fec44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose,AddBackgroundNoise , AddColoredNoise , ApplyImpulseResponse,PeakNormalization,TimeInversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ba6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567d7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb51ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1a0f1",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02713724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "558365f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446865d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f32a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c32d2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "#from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31058615",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features , Spectrogram\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5920c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d623fe02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca584162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers= 4\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "DEBUG = False\n",
    "num_epochs = 200\n",
    "if DEBUG:\n",
    "    batch_size = 32\n",
    "    test_batch_size = 4\n",
    "    num_workers=0\n",
    "    num_epochs = 4\n",
    "    \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a4192",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f843d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78bffed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    stride = step_frac*min_length\n",
    "#     print(\"min_length = \" +str(min_length))\n",
    "#     print(\"step_frac = \" +str(step_frac))\n",
    "#     print(\"stride = \" +str(stride))\n",
    "    for _,row in df.iterrows():\n",
    "        #processed_data keeps track of the tensor_values processed thus far\n",
    "        if row['length'] > min_length:\n",
    "            processed_data = 0\n",
    "            #total_data is the total tensor present in the audio\n",
    "            total_data = config.rate*row['length']\n",
    "            #print(\"********\")\n",
    "            count = 0\n",
    "#             print(\"count = \" +str(count))\n",
    "#             print(\"id = \" + str(row['id']) + \" duration = \" +str(row['length']) + \"total x vals = \" + str(total_data))\n",
    "            inner_loop_flag = False\n",
    "            #print(\"going into the inner loop to offset....\")\n",
    "            while(processed_data < total_data):\n",
    "                #print(\"inside inner loop.....\")\n",
    "                start = count*stride*config.rate\n",
    "                #now find out the row_len\n",
    "                if total_data - (start + min_length*config.rate) >= 0:\n",
    "                    #print(\"full chunk \")\n",
    "                    row_len = min_length\n",
    "                    end = start + row_len*config.rate\n",
    "                    audio_offsets.append({'id':row['id'], 'offset':count, 'length': row_len,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "                    #print(\"count = \" +str(count) + \"offset = \" +str(count) + \"start = \" +str(start) + \"end = \" +str(end))\n",
    "                    #print(\"for count.... = \" + str(count) + \"processed data = \" +str(processed_data))\n",
    "                    count+=1\n",
    "                    processed_data = (count*stride)*config.rate\n",
    "                    \n",
    "                else:\n",
    "                    inner_loop_flag = True\n",
    "                    break\n",
    "                    \n",
    "                                                       \n",
    "            #for processing residual data\n",
    "            if(inner_loop_flag):\n",
    "                #print(\"processing residual ....processed \" +str(processed_data) + \" of \" + str(total_data))\n",
    "                start = count*stride*config.rate\n",
    "                resid_durn = round((total_data - processed_data)/config.rate,2)\n",
    "                end = total_data\n",
    "                #print(\"for...\" + str(row['id']) + \" adding the residual data in the data frame with duration = \" + str(resid_durn))\n",
    "                audio_offsets.append({'id':row['id'], 'offset':count, 'length':resid_durn ,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "            \n",
    "        elif short_audio:\n",
    "            start = 0\n",
    "            end = row['length']*config.rate\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind'],'start':0 , 'end':end})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49f3f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fb4c8",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e7796e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "...                     ...                              ...           ...  \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "\n",
       "[6008 rows x 19 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if DEBUG:\n",
    "#     df = pd.read_csv(config.data_df_msc_test)\n",
    "# else:\n",
    "df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2114f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "194079ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d81dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cf1dc",
   "metadata": {},
   "source": [
    "At this stage we have all extracted the data with specie information and have encoded the specie encoding in a col = 'specie_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1988ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cce4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data- this is as per the humbug paper\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de0d462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1993149e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879</td>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881</td>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1882</td>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1883</td>\n",
       "      <td>221150</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>4546</td>\n",
       "      <td>222615</td>\n",
       "      <td>30.72</td>\n",
       "      <td>IFA_86_39_3439.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>4547</td>\n",
       "      <td>222585</td>\n",
       "      <td>25.60</td>\n",
       "      <td>IFA_86_40_3440.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>4548</td>\n",
       "      <td>222586</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_10_3450.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>4549</td>\n",
       "      <td>222596</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_11_3451.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>4550</td>\n",
       "      <td>222614</td>\n",
       "      <td>38.40</td>\n",
       "      <td>IFA_87_12_3452.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2288 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      id  length                name  sample_rate record_datetime  \\\n",
       "0      1879  221103    2.56   IFA_17_24_664.wav        44100  30-01-20 00:00   \n",
       "1      1880  221111    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "2      1881  221110    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "3      1882  221149    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "4      1883  221150    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "...     ...     ...     ...                 ...          ...             ...   \n",
       "2283   4546  222615   30.72  IFA_86_39_3439.wav        44100  23-08-20 00:00   \n",
       "2284   4547  222585   25.60  IFA_86_40_3440.wav        44100  23-08-20 00:00   \n",
       "2285   4548  222586   40.90  IFA_87_10_3450.wav        44100  23-08-20 00:00   \n",
       "2286   4549  222596   40.90  IFA_87_11_3451.wav        44100  23-08-20 00:00   \n",
       "2287   4550  222614   38.40  IFA_87_12_3452.wav        44100  23-08-20 00:00   \n",
       "\n",
       "     sound_type         species  gender fed  ... age  method mic_type  \\\n",
       "0      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "1      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "2      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "3      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "4      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "...         ...             ...     ...  ..  ...  ..     ...      ...   \n",
       "2283   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2284   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2285   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2286   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2287   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "\n",
       "     device_type   country            district  province    place  \\\n",
       "0         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "3         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "4         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "...          ...       ...                 ...       ...      ...   \n",
       "2283      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2284      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2285      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2286      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2287      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "\n",
       "     location_type specie_ind  \n",
       "0              cup          7  \n",
       "1              cup          7  \n",
       "2              cup          7  \n",
       "3              cup          0  \n",
       "4              cup          0  \n",
       "...            ...        ...  \n",
       "2283           cup          3  \n",
       "2284           cup          3  \n",
       "2285           cup          3  \n",
       "2286           cup          3  \n",
       "2287           cup          3  \n",
       "\n",
       "[2288 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55aed4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "import seaborn as sns\n",
    "sns.countplot(x = 'species', data = df_all , ax = ax , hue = 'gender',palette='dark')\n",
    "#ax.bar_label(ax.containers[0])\n",
    "#ax.bar_label(ax.containers[-1], fmt='Count:\\n%.2f', label_type='center')\n",
    "plt.xticks(rotation=90 )\n",
    "plt.title(\"Distribution of Species \")\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('figure', titlesize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894864d4",
   "metadata": {},
   "source": [
    "### Train-Test split( avoiding sklearn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebaf5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1600)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e558cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1600)\n",
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ccc6c0",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "502a50ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3703129e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81c3ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826a085",
   "metadata": {},
   "source": [
    "We've confirmed that there is no recording that is common in Train,Test,val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191ba83",
   "metadata": {},
   "source": [
    "### Next, we perform \"offsets\", spliting each(long) recording into multiple 1.92 secs chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f00c22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ee843cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train offset = 36662\n",
      "length of test offset = 10393\n",
      "length of val offset = 8497\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327acff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1fee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f83402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af31c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834039e",
   "metadata": {},
   "source": [
    "### Let's check for data leakage in offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56c22f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_test_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83f2306d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35f807db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, start_x, end_x, index_y, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0793377",
   "metadata": {},
   "source": [
    "### At this stage we've a dataframe of recordin ids and each row corresponds to a 1.92 secs recording or shorter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c11c6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "333c42ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31857838 0.6120943  3.02691546 0.59960094 2.22896401 4.1286036\n",
      " 2.72945205 5.81567259]\n"
     ]
    }
   ],
   "source": [
    "#Class imbalance \n",
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35523500",
   "metadata": {},
   "source": [
    "Let us now get the class distribution for each of the dataframes- train,test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc915d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ec7514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = train\n",
      "i = 0\n",
      "14385\n",
      "DF type = train\n",
      "i = 1\n",
      "7487\n",
      "DF type = train\n",
      "i = 2\n",
      "1514\n",
      "DF type = train\n",
      "i = 3\n",
      "7643\n",
      "DF type = train\n",
      "i = 4\n",
      "2056\n",
      "DF type = train\n",
      "i = 5\n",
      "1110\n",
      "DF type = train\n",
      "i = 6\n",
      "1679\n",
      "DF type = train\n",
      "i = 7\n",
      "788\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b0279af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = Val\n",
      "i = 0\n",
      "3344\n",
      "DF type = Val\n",
      "i = 1\n",
      "1825\n",
      "DF type = Val\n",
      "i = 2\n",
      "141\n",
      "DF type = Val\n",
      "i = 3\n",
      "1626\n",
      "DF type = Val\n",
      "i = 4\n",
      "465\n",
      "DF type = Val\n",
      "i = 5\n",
      "411\n",
      "DF type = Val\n",
      "i = 6\n",
      "531\n",
      "DF type = Val\n",
      "i = 7\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f9f03de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = test\n",
      "i = 0\n",
      "4364\n",
      "DF type = test\n",
      "i = 1\n",
      "2759\n",
      "DF type = test\n",
      "i = 2\n",
      "306\n",
      "DF type = test\n",
      "i = 3\n",
      "1826\n",
      "DF type = test\n",
      "i = 4\n",
      "572\n",
      "DF type = test\n",
      "i = 5\n",
      "152\n",
      "DF type = test\n",
      "i = 6\n",
      "218\n",
      "DF type = test\n",
      "i = 7\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854b22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e041991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add.shape))\n",
    "        #print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add.shape))\n",
    "        #print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90753a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b96f77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b2b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3abf75f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66f372",
   "metadata": {},
   "source": [
    "### Class Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589061b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60d50318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5429d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb044731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221103</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221111</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id  offset  length  specie_ind    start      end\n",
       "0      0  221103       0    1.92           7      0.0  15360.0\n",
       "1      1  221103       1    1.92           7   5120.0  20480.0\n",
       "2      2  221103       2    1.28           7  10240.0  20480.0\n",
       "3      3  221111       0    1.92           7      0.0  15360.0\n",
       "4      4  221111       1    1.92           7   5120.0  20480.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e8c607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_df(loader , trained_model, DEBUG = False):\n",
    "    err_dict = {'id': None,\n",
    "               'label': None,\n",
    "               'offset':None,\n",
    "               'y_hat':None}\n",
    "    model = trained_model\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        all_wav_id = []\n",
    "        all_offset = []\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y,offset,wav_id) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                print(\"y = \" + str(y))\n",
    "                print(\"offset = \" + str(offset))\n",
    "                print(\"wav_id = \" + str(wav_id))\n",
    "                \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            y_pred = model(x)['prediction']\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_wav_id.append(wav_id.cpu().detach())\n",
    "            all_offset.append(offset.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y).numpy()\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        all_wav_id = torch.cat(all_wav_id)\n",
    "        all_offset = torch.cat(all_offset)\n",
    "        \n",
    "        err_dict['id'] = all_wav_id\n",
    "        err_dict['label'] = all_y\n",
    "        err_dict['offset'] = all_offset\n",
    "        err_dict['y_hat'] = all_y_pred\n",
    "        df_err = pd.DataFrame.from_dict(err_dict)\n",
    "        df_err_uniq = df_err[df_err['label']!= df_err['y_hat']]\n",
    "        df_err_uniq.sort_values(by=['id','offset'])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"inside error ....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        #test_loss = test_loss/len(test_loader)\n",
    "        #test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return df_err_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f15f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    softmax = nn.Softmax()\n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\" $$$$$$$$inside test$$$$$$$$....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            has_nan = torch.isnan(x).any().item()\n",
    "            more_than_1 = torch.max(x)\n",
    "            assert not(has_nan) ,\"Tensor contains NaN values in TEST SET .\"\n",
    "            assert not(more_than_1 > 1), \"Tensor contain values greater than 1 in TEST SET .\"\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            output = model(x,train = False)\n",
    "            y_pred = output['probs']\n",
    "            #y_pred_smax = softmax(y_pred)\n",
    "            preds = output['preds']\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "            print(\" $$$$$$$$ exiting test$$$$$$$$....\")\n",
    "        \n",
    "        test_loss = test_loss\n",
    "        is_loss_nan = torch.isnan(torch.tensor(test_loss))\n",
    "        assert not(torch.isnan(is_loss_nan)) , \"NAN in TEST LOSS\"\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a55a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )\n",
    "def train_model(train_loader, val_loader,test_loader, model = None,  classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    optimiser = timm.optim.create_optimizer_v2(model.parameters(), opt='lookahead_adam', lr=0.0001)\n",
    "    scheduler = CosineAnnealingLR(optimiser, T_max=num_epochs, eta_min= 1e-5)\n",
    "    #optimiser = timm.optim.AdamW(model.parameters(), lr=config_pytorchlr)\n",
    "    timm.optim.Lookahead(optimiser, alpha=0.5, k=6)\n",
    "    \n",
    "    #optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    softmax = nn.Softmax()\n",
    "    all_train_f1 = []\n",
    "    all_val_f1 = []\n",
    "    \n",
    "    lr_log = []\n",
    "    for e in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        #tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(train_loader):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 500 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            has_nan = torch.isnan(x).any().item()\n",
    "            more_than_1 = torch.max(x)\n",
    "            assert not(has_nan) ,\"Tensor contains NaN values in TRAIN.\"\n",
    "            if DEBUG:\n",
    "                print(\"more_than_1 = \", more_than_1)\n",
    "            assert not(more_than_1 > 1), \"Tensor contain values greater than 1 in TRAIN .\"\n",
    "                                  \n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. x device = \" +str(x.device))\n",
    "                \n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            #global_step += 1\n",
    "            # AMP\n",
    "            #x_sum = torch.sum(x,axis = 1)\n",
    "            #x_sum.unsqueeze(dim = 1)\n",
    "            #zero_chk = torch.where((x_sum == 0))[0]\n",
    "            #if len(zero_chk) > 0:\n",
    "#                 print(\"ZERO ENCOUNTER\")\n",
    "#                 print(\"x = \" +str(x))\n",
    "#                 break\n",
    "                       \n",
    "            with autocast():\n",
    "                output = model(x,train = True)\n",
    "                y_pred = output['probs']\n",
    "                #y_pred_smax = softmax(y_pred)\n",
    "                preds = output['preds']\n",
    "                loss = criterion(y_pred, y)\n",
    "            \n",
    "            if DEBUG:\n",
    "                    print(\"y_pred  = \" +str(y_pred))\n",
    "                    print(\"preds = \" +str(preds))\n",
    "                \n",
    "                \n",
    "            #loss_scaler(loss, optimiser,parameters=model_parameters(model))\n",
    "            if loss.item() > 10000:\n",
    "                print(\"^^^^^^^^^^^^^^^^^ EXPLOSION^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                print(\"x sum = \" + str(torch.sum(x)))\n",
    "                print(\"current loss = \" + str(loss.item()))\n",
    "            train_loss += loss.item()\n",
    "            if DEBUG:\n",
    "                print(\"loss = \",loss.item())\n",
    "            is_loss_nan = torch.isnan(loss)\n",
    "            assert not(torch.isnan(is_loss_nan)) , \"NAN in Training LOSS\"\n",
    "            \n",
    "            all_y.append(y.cpu().detach())\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "                \n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),error_if_nonfinite=False ,max_norm = 5.0 )\n",
    "            optimiser.step()\n",
    "            del x\n",
    "            del y\n",
    "            del y_pred,preds\n",
    "        \n",
    "        #lr_log.append(lr)\n",
    "        #optimiser.sync_lookahead()\n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        all_train_f1.append(train_f1)\n",
    "        if DEBUG:\n",
    "            print(\"train_f1 = \" +str(train_f1))\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"val\")\n",
    "        all_val_f1.append(val_f1)\n",
    "        all_val_loss.append(val_loss)\n",
    "        if DEBUG:\n",
    "            print(\"val F1 = \" + str(val_f1))\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir,  checkpoint_name))\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.model_dir,  checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"********************************\")\n",
    "            from sklearn.metrics import classification_report\n",
    "            test_loss, test_f1 , all_y_test,all_y_pred_test = test_model(model, test_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"test\")\n",
    "            # at times output is not getting printed. Could be due to multi threading and hence adding sleep\n",
    "            time.sleep(2)\n",
    "            sys.stdout.flush()\n",
    "            print(\"test_loss = \",str(test_loss/len(test_loader)))\n",
    "            print(\"test_f1 = \",str(test_f1))\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            \n",
    "            time.sleep(2)\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , classes)\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        optimiser.sync_lookahead()\n",
    "        scheduler.step()\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23b216ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])\n",
    "\n",
    "\n",
    "# apply_augmentation = Compose(transforms=[AddColoredNoise(p = 1) ,TimeInversion( p = 1) ,PolarityInversion(p = 1)])\n",
    "\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e77132cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozErrAnalysisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        \n",
    "               \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'],offset, self.audio_df.loc[idx]['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8167df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        temp_id = int(self.audio_df.loc[idx]['id'])\n",
    "        path_var = self.data_dir +\"/\" +str(temp_id)+ str(\".wav\")\n",
    "        entire_aud, inp_rate = torchaudio.load(path_var)\n",
    "        if inp_rate != config.rate:\n",
    "            #print(\" Original sample rate = \" +str(inp_rate)+ \" resampling ...\")\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=entire_aud.dtype)\n",
    "            entire_aud = resampler(entire_aud)\n",
    "            #print(\"processsing file on \" +str(path_var) + \"Post resample shape =  \" + str(entire_aud.shape))\n",
    "        \n",
    "        aud_len = self.audio_df.loc[idx]['length']\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        #print(\"sliced val = \" +str(int((offset+config.min_duration)*config.rate)))\n",
    "        start_pos = int(round(self.audio_df.loc[idx]['start']))\n",
    "        #print(\"start_pos = \" +str(start_pos))\n",
    "        end_pos =  int(round(self.audio_df.loc[idx]['end']))\n",
    "        #print(\"end_pos = \" +str(end_pos))\n",
    "        x = entire_aud[:,start_pos:end_pos]\n",
    "        #print(\"extracted x = \" +str(x))\n",
    "        #print(\"x shape = \" +str(x.shape))\n",
    "        if aud_len < config.min_duration:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            #print(\"padding on \" +str(path_var))\n",
    "            f_out = pad_mean(x)\n",
    "            #print(\"returning from padding  SHape = \" +str(f_out.shape))\n",
    "        else:\n",
    "            f_out = x[0]\n",
    "            f_out = f_out.unsqueeze(0)\n",
    "            \n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            #print(\"offset = \" + str(offset))\n",
    "            #print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(f_out.shape))\n",
    "        \n",
    "        #x_val = x[:,start:end]\n",
    "        #now that we have final x- let's create specgram and add augmentations.\n",
    "                 \n",
    "        return (f_out,self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ae7c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ApplyAug(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.aug_flag_y = transforms.Compose([\n",
    "            transforms.GaussianBlur(3),\n",
    "            transforms.RandomErasing(),\n",
    "            #transforms.Normalize(mean=2.7360104e-05, std=.0061507192)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, spec_gram, aug_flag):\n",
    "        if aug_flag == \"Y\":\n",
    "            rgb_img_auto_aug = self.aug_flag_y(spec_gram)\n",
    "            return rgb_img_auto_aug\n",
    "        else:\n",
    "            return spec_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f82e46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name ,input_size = 4, hidden_size = 768 , num_classes = 8 , image_size = 224 , batch_size = batch_size):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.25)\n",
    "        #spec = torchaudio.transforms.Spectrogram(config.NFFT//2 +1)\n",
    "        self.spec_layer = torchaudio.transforms.Spectrogram(config.NFFT//2 +1)\n",
    "        #self.linear = nn.Linear(hidden_size , 1024)\n",
    "        self.output = nn.Linear(1000, num_classes)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.aug_layer = ApplyAug()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_ids,train = True ,attention_mask = False):\n",
    "        #this will hold the output\n",
    "        output_dict = {'probs':None , 'preds':None}\n",
    "        spec_gram = self.spec_layer(input_ids)\n",
    "        spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "        assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after spec gram creation.\"\n",
    "        if DEBUG:\n",
    "            #print(\"post STFT , spec gram = \", spec_gram)\n",
    "            print(\"post STFT , spec gram max val = \", torch.max(spec_gram))\n",
    "            print(\"post STFT , spec gram min val = \", torch.min(spec_gram))\n",
    "            print(\"post STFT , spec_gram_nan_check = \", spec_gram_nan_check)\n",
    "                 \n",
    "        if train== True:\n",
    "            rand_aug_choice = torch.randint(low=0, high=100, size=(1,1),device = 'cuda',dtype=torch.int32)\n",
    "            #print(\"rand_aug_choice = \",rand_aug_choice)\n",
    "            if rand_aug_choice %2 == 0 :\n",
    "                spec_gram = self.aug_layer(spec_gram , aug_flag = \"Y\")\n",
    "                spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "                assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after aug layer Y \"\n",
    "                if DEBUG:\n",
    "                    #print(\"post aug_layer Y , spec gram = \", spec_gram)\n",
    "                    print(\"post aug_layer flag Y , spec gram max val = \", torch.max(spec_gram))\n",
    "                    print(\"post aug_layer flag Y, spec gram min val = \", torch.min(spec_gram))\n",
    "                    print(\"post aug_layer flag Y , spec_gram_nan_check = \", spec_gram_nan_check)\n",
    "                    print(\"post aug_layer flag Y , spec_gram_nan_check = \", spec_gram_nan_check)\n",
    "        \n",
    "        spec_gram = self.aug_layer(spec_gram , aug_flag = \"N\")\n",
    "        spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "        assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after aug layer Y \"\n",
    "               \n",
    "        if DEBUG:\n",
    "            #print(\"post aug_layer N , spec gram = \", spec_gram)\n",
    "            print(\"post aug_layer flag N , spec gram max val = \", torch.max(spec_gram))\n",
    "            print(\"post aug_layer flag N, spec gram min val = \", torch.min(spec_gram))\n",
    "            print(\"post aug_layer flag N , spec_gram_nan_check = \", spec_gram_nan_check)\n",
    "            print(\"post aug_layer flag N , spec_gram_nan_check = \", spec_gram_nan_check)\n",
    "            \n",
    "        # now reshape to image_size\n",
    "        spec_gram = spec_gram.view(batch_size,1,-1)\n",
    "        spec_gram = self.sizer(spec_gram)\n",
    "        spec_gram = spec_gram.unsqueeze(dim = 1)\n",
    "        mean = spec_gram.mean(dim=(0, 2,3), keepdim=True)\n",
    "        std = spec_gram.std(dim=(0, 2,3), keepdim=True)\n",
    "#         print(\"batch mean = \",mean)\n",
    "#         print(\"batch std = \",std)\n",
    "        normalized_spec_gram = (spec_gram - mean) / std\n",
    "        if DEBUG:\n",
    "            print(\"post sizer shape of spec_gram = \" , spec_gram.shape)\n",
    "            print(\"post Norm  , normalized_spec_gram max = \", torch.max(normalized_spec_gram))\n",
    "            print(\"post Norm  , normalized_spec_gram min = \", torch.min(normalized_spec_gram))\n",
    "            #now make it 3 channel \n",
    "        #spec_gram = torch.cat((spec_gram, spec_gram, spec_gram), dim=1).to('cuda')\n",
    "        if DEBUG:\n",
    "            print(\"post 3 channel shape of spec_gram = \" , spec_gram.shape)\n",
    "        backbone_op = self.backbone(normalized_spec_gram)\n",
    "        #print(\"backbone_op = \",backbone_op)\n",
    "        backbone_op_nan_check = torch.isnan(backbone_op).any().item()\n",
    "        assert not (backbone_op_nan_check) ,\"Tensor contains NaN values in the backbone OP \"\n",
    "        del normalized_spec_gram ,spec_gram,mean,std\n",
    "        if DEBUG:\n",
    "            print(\"backbone_op shape \",backbone_op.shape)\n",
    "            print(\"backbone_op = \", backbone_op)\n",
    "        out_smax = self.softmax(backbone_op)\n",
    "        if DEBUG:\n",
    "            print(\"out_smax = \",out_smax)\n",
    "            print(\"out_smax shape = \" , out_smax.shape)\n",
    "        #out_smax = self.softmax(output)\n",
    "        out = torch.argmax(out_smax , dim = 1)\n",
    "        #print(\"out = \",out)\n",
    "        \n",
    "        output_dict['probs'] = out_smax\n",
    "        output_dict['preds'] = out\n",
    "        #print(\"^^^^^ inside forward^^^^^^^\")\n",
    "        #del spec_gram ,mean, std\n",
    "        if DEBUG:\n",
    "            print(\"output_dict = \", output_dict)\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ecea3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(num_values ,df ,classes = classes):\n",
    "    new_df = pd.DataFrame()\n",
    "    for ind in range(len(classes)):\n",
    "        #print(\"ind = \", ind)\n",
    "        op = df[df['specie_ind'] == ind]\n",
    "        #print(\"len op = \", len(op))\n",
    "        op_new = op.sample(n = 1)\n",
    "        #print(\"rand_ind = \" , rand_ind)\n",
    "        #([df1, df2], axis=1)\n",
    "        new_df = pd.concat([op_new,new_df],axis = 0)\n",
    "        #print(\"elem = \" , elem)\n",
    "        #new_list.append(elem)\n",
    "    if len(new_df) < num_values:\n",
    "        diff =  num_values - len(new_df)\n",
    "        #print(\"diff = \", diff)\n",
    "        remaining_elems= df.sample(n = diff)\n",
    "        #print(\"len of remaining elems = \", len(remaining_elems))\n",
    "        new_df = pd.concat([remaining_elems,new_df],axis = 0)\n",
    "        \n",
    "    #print(\"new_df = \", new_df)    \n",
    "    new_df_1 = new_df.reset_index(drop = True)\n",
    "    return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f759fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14bfafac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221103</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221111</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36657</th>\n",
       "      <td>36657</td>\n",
       "      <td>222614</td>\n",
       "      <td>54</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3</td>\n",
       "      <td>276480.0</td>\n",
       "      <td>291840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36658</th>\n",
       "      <td>36658</td>\n",
       "      <td>222614</td>\n",
       "      <td>55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3</td>\n",
       "      <td>281600.0</td>\n",
       "      <td>296960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36659</th>\n",
       "      <td>36659</td>\n",
       "      <td>222614</td>\n",
       "      <td>56</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3</td>\n",
       "      <td>286720.0</td>\n",
       "      <td>302080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36660</th>\n",
       "      <td>36660</td>\n",
       "      <td>222614</td>\n",
       "      <td>57</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3</td>\n",
       "      <td>291840.0</td>\n",
       "      <td>307200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36661</th>\n",
       "      <td>36661</td>\n",
       "      <td>222614</td>\n",
       "      <td>58</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3</td>\n",
       "      <td>296960.0</td>\n",
       "      <td>307200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36662 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index      id  offset  length  specie_ind     start       end\n",
       "0          0  221103       0    1.92           7       0.0   15360.0\n",
       "1          1  221103       1    1.92           7    5120.0   20480.0\n",
       "2          2  221103       2    1.28           7   10240.0   20480.0\n",
       "3          3  221111       0    1.92           7       0.0   15360.0\n",
       "4          4  221111       1    1.92           7    5120.0   20480.0\n",
       "...      ...     ...     ...     ...         ...       ...       ...\n",
       "36657  36657  222614      54    1.92           3  276480.0  291840.0\n",
       "36658  36658  222614      55    1.92           3  281600.0  296960.0\n",
       "36659  36659  222614      56    1.92           3  286720.0  302080.0\n",
       "36660  36660  222614      57    1.92           3  291840.0  307200.0\n",
       "36661  36661  222614      58    1.28           3  296960.0  307200.0\n",
       "\n",
       "[36662 rows x 7 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b09d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d673a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/audio\n"
     ]
    }
   ],
   "source": [
    "print(config.data_dir)\n",
    "if DEBUG:\n",
    "    print(DEBUG)\n",
    "    df_train_offset_db = get_indices(batch_size*100,df_train_offset)\n",
    "    df_val_offset_db = get_indices(batch_size*100,df_val_offset)\n",
    "    df_test_offset_db = get_indices(batch_size*100,df_test_offset)\n",
    "    \n",
    "    train_dataset_db = MozDataset(df_train_offset_db,  config.data_dir, min_length)\n",
    "    val_dataset_db = MozDataset(df_val_offset_db,  config.data_dir, min_length)\n",
    "    test_dataset_db = MozDataset(df_test_offset_db,  config.data_dir, min_length)\n",
    "    #error_dataset = MozErrAnalysisDataset(df_val_offset,  config.data_dir, min_length = config.min_duration)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset_db, num_workers=num_workers,batch_size = batch_size,shuffle = True, pin_memory=True,drop_last = True )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset_db, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory,drop_last = True )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset_db, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,drop_last = True)\n",
    "    #error_loader = torch.utils.data.DataLoader(error_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,drop_last = True)\n",
    "    print(\"Length of train dataset = \" +str(len(train_dataset_db)))\n",
    "    print(\"Length of train loader = \" +str(len(train_loader)))\n",
    "    print(\"Length of val dataset = \" +str(len(val_dataset_db)))\n",
    "    print(\"Length of val loader = \" +str(len(val_loader)))\n",
    "    print(\"Length of test dataset = \" +str(len(test_dataset_db)))\n",
    "    print(\"Length of test loader = \" +str(len(test_loader)))\n",
    "\n",
    "else:\n",
    "    train_dataset = MozDataset(df_train_offset,  config.data_dir, min_length)\n",
    "    val_dataset = MozDataset(df_val_offset,  config.data_dir, min_length)\n",
    "    test_dataset = MozDataset(df_test_offset,  config.data_dir, min_length)\n",
    "    error_dataset = MozErrAnalysisDataset(df_val_offset,  config.data_dir, min_length = config.min_duration)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True, pin_memory=True,drop_last = True )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory,drop_last = True )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,drop_last = True)\n",
    "    error_loader = torch.utils.data.DataLoader(error_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,drop_last = True)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c9626e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4011d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a03173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_itr = iter(train_loader)\n",
    "# a,b = train_itr.next()\n",
    "# print(a.shape)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1bfe9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "#                               window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "#                            sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "# x = spec_layer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4205d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_mod = Model('convnext_small',224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c6b0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mod(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fe8b5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98b0ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model=MyModel('convnext_xlarge_in22k')):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ccb8c5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "epoch = 0batch = 0 of 1145duraation = 0.017692438761393228\n",
      "epoch = 0batch = 500 of 1145duraation = 3.934461541970571\n",
      "epoch = 0batch = 1000 of 1145duraation = 7.849305665493011\n",
      "Epoch: 0, Train Loss: 2.08370777, Train f1: 0.22743477, Val Loss: 2.05008990, Val f1: 0.06051989, overrun_counter -1\n",
      "Saving model to: ../../models/model_e0_2023_04_08_04_43_52.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "test_loss =  2.056332767377665\n",
      "test_f1 =  0.051417095681295016\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.00      0.00      0.00      4364\n",
      "culex pipiens complex       0.00      0.00      0.00      2759\n",
      "           ae aegypti       0.00      0.00      0.00       306\n",
      "       an funestus ss       0.17      1.00      0.30      1801\n",
      "         an squamosus       0.00      0.00      0.00       572\n",
      "          an coustani       0.00      0.00      0.00       152\n",
      "         ma uniformis       0.00      0.00      0.00       218\n",
      "         ma africanus       0.00      0.00      0.00       196\n",
      "\n",
      "             accuracy                           0.17     10368\n",
      "            macro avg       0.02      0.12      0.04     10368\n",
      "         weighted avg       0.03      0.17      0.05     10368\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1batch = 0 of 1145duraation = 0.017733263969421386\n",
      "epoch = 1batch = 500 of 1145duraation = 3.9451319694519045\n",
      "epoch = 1batch = 1000 of 1145duraation = 7.860877092679342\n",
      "..Overrun....no improvement\n",
      "Epoch: 1, Train Loss: 2.09404630, Train f1: 0.16672072, Val Loss: 2.07671198, Val f1: 0.06051989, overrun_counter 0\n",
      "epoch = 2batch = 0 of 1145duraation = 0.016439505418141685\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor contains NaN values in the backbone OP ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-c92fb48e534c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#filepath = \"../../models/model_e2_2023_03_31_15_47_38.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model_epcoh_99 = load_model(filepath,model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mclass_weights\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-ccf8ac817c94>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, test_loader, model, classes, class_weights, num_epochs, n_channels)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m#y_pred_smax = softmax(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-aee697525e3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, train, attention_mask)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#print(\"backbone_op = \",backbone_op)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mbackbone_op_nan_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbackbone_op_nan_check\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"Tensor contains NaN values in the backbone OP \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mnormalized_spec_gram\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mspec_gram\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor contains NaN values in the backbone OP "
     ]
    }
   ],
   "source": [
    "#model =MyModel('convnext_xlarge_in22k')\n",
    "model =MyModel('convnext_xlarge_in22k',224)\n",
    "#filepath = \"../../models/model_e2_2023_03_31_15_47_38.pth\"\n",
    "#model_epcoh_99 = load_model(filepath,model)\n",
    "model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1 = train_model(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f574c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len of all_train_f1 = \"+str(len(all_train_f1)))\n",
    "print(\"len of all_val_f1 = \"+str(len(all_val_f1)))\n",
    "print(\"len of all_val_loss = \"+str(len(all_val_loss)))\n",
    "print(\"len of all_train_loss = \"+str(len(all_train_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950148ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d0feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8749028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2da523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_f1_final =  [v for i, v in enumerate(all_val_f1) if i % 2 == 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bacb5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_loss_final =  [v for i, v in enumerate(all_val_loss) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_f1_final =  [v for i, v in enumerate(all_train_f1) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_train_f1_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b198ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({'train_loss':all_train_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['val_f1'] = all_val_f1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['train_f1'] = all_train_f1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['val_loss'] = all_val_loss_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.fillna(0)\n",
    "plot_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.iloc[129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9594d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(plot_df['train_f1','val_f1']);\n",
    "plt.figure(figsize=(8,6)) \n",
    "sns.lineplot(plot_df[['train_f1','val_f1']])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score - ConvNext Small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) \n",
    "sns.lineplot(plot_df[['train_loss','val_loss']])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss - ConvNext Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('file1.csv')\n",
    "plot_df.to_csv(\"plot_df_convNext_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1316196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fefb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_new = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "val_loader_new = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=2,\n",
    "        num_workers=0, pin_memory=pin_memory  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = df_val_offset\n",
    "model = model_epcoh_10\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "for idx,(x,y) in enumerate(val_dataset):\n",
    "    print(idx)\n",
    "    print(y)\n",
    "    x = x.to('cuda').float()\n",
    "    print(\"x shape = \" +str(x.shape))\n",
    "    #x_new = x.unsqueeze(dim = 1)\n",
    "    print(\"x_new shape = \" +str(x_new.shape))\n",
    "    x_new = x.to('cuda')\n",
    "    y_pred = model(x_new)['prediction']\n",
    "    y_pred_cpu = y_pred.cpu().detach()\n",
    "    preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "    df_erroriloc[idx]['y_hat'] = preds\n",
    "    del x_new\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,15360)\n",
    "x = x.unsqueeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_offset.head()\n",
    "path_temp = \"../data/audio/\"\n",
    "for i,row in df_val_offset.iterrows():\n",
    "    print(\"i = \" +str(i))\n",
    "    print(\"id = \" + str(int(row['id'])))\n",
    "    file = str(int(row['id']))+\".wav\"\n",
    "    print(file)\n",
    "    path = path_temp + file\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "    if inp_rate != config.rate:\n",
    "        import torchaudio.transforms as T\n",
    "        resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[1] < config.rate*min_length:\n",
    "        #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "        f_out = pad_mean(waveform)\n",
    "    else:\n",
    "        f = waveform[0]\n",
    "        f_out = f.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa764dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(df):\n",
    "    \n",
    "    path_name = \"../data/audio/\"\n",
    "    file = df.loc[idx]['id'])}.wav\")\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"\")\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            \n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"returning x of shape ...\" + str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c77c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the model checkpoint as a parameter as input\n",
    "# read the val df\n",
    "#get the tensor rep for the offset.\n",
    "#pass it to the model get add get the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    label.append(np.random.rand(9))\n",
    "    pred.append(np.random.rand(9))\n",
    "print(label)\n",
    "print(pred)\n",
    "print(classification_report(label, pred, target_names= classes, labels= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226923bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.randn(4,9)\n",
    "y_pred.shape\n",
    "#y_pred_np = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_np\n",
    "# y_pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdc544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d731208",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(x,y) in enumerate(test_loader):\n",
    "    print(\"idx = \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a6999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941edb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46926d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
