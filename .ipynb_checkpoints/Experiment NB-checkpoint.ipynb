{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf816be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111573f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dfcd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "#import torch_optimizer as optim \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b8ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e4a0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afba19d",
   "metadata": {},
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, pre_tr_model ,input_size = 4, hidden_size = 768 , num_classes = 8):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.backbone = pre_tr_model\n",
    "        #self.linear = nn.Linear(hidden_size , 1024)\n",
    "        self.output = nn.Linear(1000, num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask = False):\n",
    "        batch_size = 4\n",
    "        output_dict = {'probs':None , 'preds':None}\n",
    "        \n",
    "        backbone_op_reshp = self.backbone(input_ids)\n",
    "        print(backbone_op_reshp.logits.shape)\n",
    "        #linear_output = self.linear(backbone_op_reshp)\n",
    "        #print(linear_output.shape)\n",
    "        output = self.output(backbone_op_reshp['logits'])\n",
    "        #print(output)\n",
    "        #print(\"output shape = \" , output.shape)\n",
    "        out_smax = self.softmax(output)\n",
    "        out = torch.argmax(out_smax , dim = 1)\n",
    "        #print(\"out = \",out)\n",
    "        \n",
    "        output_dict['probs'] = out_smax\n",
    "        output_dict['preds'] = out\n",
    "        #print(\"^^^^^ inside forward^^^^^^^\")\n",
    "        #print(\"output_dict = \", output_dict)\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "print(torch.__version__)\n",
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bffca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, ConvNextV2ForImageClassification,ASTModel\n",
    "# model = model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e48992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_variable is 3\n"
     ]
    }
   ],
   "source": [
    "# Define a variable\n",
    "l = [1,2,3 ,5]\n",
    "my_variable = random.choice(l)\n",
    "\n",
    "# Example of a switch-like behavior using if/elif\n",
    "if my_variable == 1:\n",
    "    # Code to be executed if my_variable is 1\n",
    "    print(\"my_variable is 1\")\n",
    "elif my_variable == 2:\n",
    "    # Code to be executed if my_variable is 2\n",
    "    print(\"my_variable is 2\")\n",
    "elif my_variable == 3:\n",
    "    # Code to be executed if my_variable is 3\n",
    "    print(\"my_variable is 3\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae2f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ApplyAug(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.aug_flag_y = transforms.Compose([\n",
    "            transforms.GaussianBlur(3),\n",
    "            transforms.RandomErasing(),\n",
    "            transforms.Normalize(mean=2.7360104e-05, std=.0061507192)\n",
    "        ])\n",
    "        self.aug_flag_n = transforms.Compose([\n",
    "            transforms.Normalize(mean=2.7360104e-05, std=.0061507192)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, spec_gram, aug_flag):\n",
    "        if aug_flag == \"Y\":\n",
    "            rgb_img_auto_aug = self.aug_flag_y(spec_gram)\n",
    "            return rgb_img_auto_aug\n",
    "        else:\n",
    "            img_tensor = self.aug_flag_n(spec_gram)\n",
    "            return img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d4d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model('convnext_xlarge_in22k',\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.25)\n",
    "\n",
    "x = torch.rand(32,1,224,224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2330c528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_rand shape =  torch.Size([32, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_classes = 8\n",
    "y_rand = torch.rand(batch_size,num_classes)\n",
    "print(\"y_rand shape = \",y_rand.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c369f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def smooth_labels(y_true, smoothing=0.1):\n",
    "    # Convert one-hot encoded true labels to soft labels\n",
    "    confidence = 1.0 - smoothing\n",
    "    print(\"y_true shape = \",y_true.shape)\n",
    "    label_shape = torch.Size((y_true.size(0), y_true.size(1)))\n",
    "    print(\"label shape = \",label_shape.shape)\n",
    "    \n",
    "    y_true = torch.empty(label_shape).fill_(smoothing / (y_true.size(1) - 1)).to(y_true.device)\n",
    "    y_true.scatter_(1, y_true.argmax(dim=1, keepdim=True), confidence)\n",
    "    return y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1223690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true shape =  torch.Size([32, 8])\n"
     ]
    }
   ],
   "source": [
    "y_true_scatt = smooth_labels(y_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09423b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as VT\n",
    "import torchaudio.transforms as AT\n",
    "# Define a list of transformations\n",
    "transformations = [\n",
    "    VT.GaussianBlur(3),\n",
    "    VT.RandomErasing(),\n",
    "    AT.FrequencyMasking(freq_mask_param=100),\n",
    "    AT.TimeMasking(time_mask_param=50)\n",
    "]\n",
    "\n",
    "# Define a probability list for each transformation\n",
    "probabilities = [1, 1, 0.8, 0.8]\n",
    "\n",
    "# Create a RandomApply transformation with different probabilities\n",
    "augment = VT.RandomChoice(transformations, p=probabilities)\n",
    "\n",
    "# Apply the RandomApply transformation to an input image\n",
    "augmented = augment(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6c8c55b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only torch.uint8 image tensors are supported, but found torch.float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_758/1529483729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Apply the transforms to the input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_758/1529483729.py\u001b[0m in \u001b[0;36mcustom_transform\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcustom_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Apply AutoAugmentPolicy.IMAGENET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoAugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoAugmentPolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGENET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Apply audio transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFrequencyMasking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq_mask_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/autoaugment.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msigned\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msigns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0mmagnitude\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_apply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/autoaugment.py\u001b[0m in \u001b[0;36m_apply_op\u001b[0;34m(img, op_name, magnitude, interpolation, fill)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocontrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mop_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Equalize\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mop_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Invert\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mequalize\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m   1519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mequalize\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input image tensor should have 3 or 4 dimensions, but found {img.ndim}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Only torch.uint8 image tensors are supported, but found {img.dtype}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0m_assert_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only torch.uint8 image tensors are supported, but found torch.float32"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchaudio.transforms as AT\n",
    "\n",
    "# Define a custom transformation function that applies the AutoAugmentPolicy.IMAGENET policy\n",
    "# followed by audio transforms\n",
    "def custom_transform(x):\n",
    "    # Apply AutoAugmentPolicy.IMAGENET\n",
    "    x = T.AutoAugment(policy=T.AutoAugmentPolicy.IMAGENET)(x)\n",
    "    # Apply audio transforms\n",
    "    x = AT.FrequencyMasking(freq_mask_param=100)(x)\n",
    "    x = AT.TimeMasking(time_mask_param=50)(x)\n",
    "    return x\n",
    "\n",
    "# Create a RandomApply transformation that applies the custom_transform with probability 1.0\n",
    "transforms = T.RandomApply([custom_transform], p=1.0)\n",
    "\n",
    "# Generate a random input tensor\n",
    "x = torch.rand(32, 1, 1025, 31)\n",
    "\n",
    "# Apply the transforms to the input tensor\n",
    "out = transforms(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055247f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19163270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "print(timm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c667c27",
   "metadata": {},
   "source": [
    "### Transforms Random apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4769172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "torch.Size([8])\n",
      "Tensor created with torch.from_numpy(): tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "Tensor created with torch.tensor(): tensor([1, 2, 3, 4, 5, 6, 7, 8])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create a numpy array of shape (8,)\n",
    "numpy_array = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(numpy_array.shape)\n",
    "# Convert numpy array to a PyTorch tensor using torch.from_numpy()\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "\n",
    "# Convert numpy array to a PyTorch tensor using torch.tensor()\n",
    "tensor_from_tensor = torch.tensor(numpy_array)\n",
    "print(tensor_from_tensor.shape)\n",
    "# Print the resulting tensors\n",
    "print(\"Tensor created with torch.from_numpy():\", tensor_from_numpy)\n",
    "print(\"Tensor created with torch.tensor():\", tensor_from_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d262509e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec_gram shape =  torch.Size([32, 1, 31775])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(32, 1025, 31)\n",
    "spec_gram = x.view(32,1,-1)\n",
    "print(\"spec_gram shape = \",spec_gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba495a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class TimeMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeMask, self).__init__()\n",
    "        self.transformations = nn.Sequential(\n",
    "            transforms.RandomApply([\n",
    "                torchaudio.transforms.TimeMasking(time_mask_param=25)\n",
    "                ], p=0.2),  # Example probability of applying the transformations\n",
    "            # Additional transformations or layers can be added here\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformations(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class FreqMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FreqMask, self).__init__()\n",
    "        self.transformations = nn.Sequential(\n",
    "            transforms.RandomApply([\n",
    "                torchaudio.transforms.FrequencyMasking(freq_mask_param=100),\n",
    "                ], p=0.2),  # Example probability of applying the transformations\n",
    "            # Additional transformations or layers can be added here\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformations(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20598674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SpecAug_All(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpecAug_All, self).__init__()\n",
    "        self.transformations = nn.Sequential(\n",
    "            transforms.RandomApply([\n",
    "                torchaudio.transforms.FrequencyMasking(freq_mask_param=100),\n",
    "                torchaudio.transforms.TimeMasking(time_mask_param=25)\n",
    "                \n",
    "            ], p=0.1),  # Example probability of applying the transformations\n",
    "            # Additional transformations or layers can be added here\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformations(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594caff8",
   "metadata": {},
   "source": [
    "## Mean 0 and Std deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b261b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normalize_batch, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_mean = torch.mean(x, dim=0, keepdim=True)\n",
    "        batch_std = torch.std(x, dim=0, keepdim=True)\n",
    "        epsilon = 1e-8\n",
    "        batch_std = torch.sqrt(batch_std ** 2 + epsilon)\n",
    "        batch_normalized = (x - batch_mean) / batch_std\n",
    "        return batch_normalized\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eff356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def normalize_batch(batch, axis=0):\n",
    "    \"\"\"Normalize a batch of tensors to have mean 0 and std 1 along the specified axis.\"\"\"\n",
    "    batch_mean = torch.mean(batch, dim=axis, keepdim=True)\n",
    "    batch_std = torch.std(batch, dim=axis, keepdim=True)\n",
    "    batch_normalized = (batch - batch_mean) / batch_std\n",
    "    return batch_normalized\n",
    "\n",
    "# Example usage:\n",
    "# Assume 'batch' is a PyTorch tensor of shape (batch_size, channels, height, width)\n",
    "\n",
    "# print(batch.shape)\n",
    "# normalized_batch = normalize_batch(batch, axis=0)\n",
    "# print(normalized_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name ,input_size = 4, hidden_size = 768 , num_classes = 8 , image_size = 224 , batch_size = batch_size):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.25)\n",
    "        self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=True,verbose = False,fmin = 100,fmax = 2000)\n",
    "        #self.linear = nn.Linear(hidden_size , 1024)\n",
    "        self.output = nn.Linear(1000, num_classes)\n",
    "        self.sizer = VT.Resize((image_size,image_size),antialias = True)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.TimeMask_layer = TimeMask()\n",
    "        self.FreqMask_layer = FreqMask()\n",
    "        self.SpecAug_All_layer = SpecAug_All()\n",
    "        self.norm_layer = Normalize_batch()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_ids,train = True ,attention_mask = False):\n",
    "        #this will hold the output\n",
    "        output_dict = {'probs':None , 'preds':None}\n",
    "        max_input_wav,_ = torch.max(input_ids,dim =2)\n",
    "        min_input_wav,_ = torch.min(input_ids,dim =2)\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"shape of input = \",input_ids.shape)\n",
    "            print(\"max_input_wav = \", max_input_wav)\n",
    "            print(\"min_input_wav = \", min_input_wav)\n",
    "            \n",
    "        spec_gram = self.spec_layer(input_ids)\n",
    "        spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "        min_val_spec_gram = torch.min(spec_gram)\n",
    "        max_val_spec_gram = torch.max(spec_gram)\n",
    "        assert not(max_val_spec_gram < 0) , \"neg value in the output of spectrogram\" \n",
    "        assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after spec gram creation.\"\n",
    "        if DEBUG:\n",
    "            print(\"post STFT , spec gram = \", spec_gram)\n",
    "            print(\"post STFT , spec gram max val = \", torch.max(spec_gram))\n",
    "            print(\"post STFT , spec gram min val = \", torch.min(spec_gram))\n",
    "            print(\"post STFT , spec_gram_nan_check = \", spec_gram_nan_check)\n",
    "                 \n",
    "        if train== True:\n",
    "            spec_gram = self.TimeMask_layer(spec_gram)\n",
    "            spec_gram = self.FreqMask_layer(spec_gram)\n",
    "            spec_gram = self.SpecAug_All_layer(spec_gram)\n",
    "                        \n",
    "            \n",
    "        spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "        assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after aug layer Y \"\n",
    "        del spec_gram_nan_check,min_val_spec_gram,max_val_spec_gram,max_input_wav,min_input_wav\n",
    "        if DEBUG:\n",
    "            #print(\"post aug_layer N , spec gram = \", spec_gram)\n",
    "            print(\"post aug_layer flag N , spec gram max val = \", torch.max(spec_gram))\n",
    "            print(\"post aug_layer flag N, spec gram min val = \", torch.min(spec_gram))\n",
    "            \n",
    "            \n",
    "        # now reshape to image_size\n",
    "        spec_gram = spec_gram.view(batch_size,1,-1)\n",
    "        spec_gram = self.sizer(spec_gram)\n",
    "        spec_gram = spec_gram.unsqueeze(dim = 1)\n",
    "        normalized_spec_gram = self.norm_layer(spec_gram)\n",
    "        print(\"normalized_spec_gram = \",normalized_spec_gram)\n",
    "                \n",
    "        if DEBUG:\n",
    "            print(\"post sizer shape of spec_gram = \" , spec_gram.shape)\n",
    "            print(\"post Norm  , normalized_spec_gram max = \", torch.max(normalized_spec_gram))\n",
    "            print(\"post Norm  , normalized_spec_gram min = \", torch.min(normalized_spec_gram))\n",
    "            #now make it 3 channel \n",
    "        #spec_gram = torch.cat((spec_gram, spec_gram, spec_gram), dim=1).to('cuda')\n",
    "        #del mean,std\n",
    "        backbone_op = self.backbone(normalized_spec_gram)\n",
    "        #print(\"backbone_op = \",backbone_op)\n",
    "        backbone_op_nan_check = torch.isnan(backbone_op).any().item()\n",
    "        assert not (backbone_op_nan_check) ,\"Tensor contains NaN values in the backbone OP \"\n",
    "        if DEBUG:\n",
    "            print(\"backbone_op shape \",backbone_op.shape)\n",
    "            print(\"backbone_op = \", backbone_op)\n",
    "        out_smax = self.softmax(backbone_op)\n",
    "        if DEBUG:\n",
    "            print(\"out_smax = \",out_smax)\n",
    "            print(\"out_smax shape = \" , out_smax.shape)\n",
    "        #out_smax = self.softmax(output)\n",
    "        out = torch.argmax(out_smax , dim = 1)\n",
    "        #print(\"out = \",out)\n",
    "        \n",
    "        output_dict['probs'] = out_smax\n",
    "        output_dict['preds'] = out\n",
    "        #print(\"^^^^^ inside forward^^^^^^^\")\n",
    "        del spec_gram\n",
    "        if DEBUG:\n",
    "            print(\"output_dict = \", output_dict)\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f373b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_new = MyModel('convnext_xlarge_in22k')\n",
    "model_new.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a66d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "for i in range(1000):\n",
    "    x = torch.rand(4,1,15360,device = 'cuda')\n",
    "    op = model_new(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tim scheduler\n",
    "#ensure to make the 2 updates in the main tr loop\n",
    "num_epochs = 200\n",
    "optimizer = timm.optim.AdamP(model_new.parameters(), lr=0.01)\n",
    "scheduler = timm.scheduler.CosineLRScheduler(optimizer, t_initial=num_epochs/2,lr_min= .0001,cycle_limit=num_epochs//2 + 1,cycle_decay = .75)\n",
    "sch1 = timm.scheduler.CosineLRScheduler(optimizer, t_initial=num_epochs/2,lr_min= 1e-5,cycle_limit=num_epochs//2 + 1,cycle_decay = .25,warmup_t = 5 ,warmup_lr_init = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875239bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    # Find the minimum and maximum values in the batch\n",
    "    min_val = torch.min(batch)\n",
    "    print(\"min_val = \",min_val)\n",
    "    max_val = torch.max(batch)\n",
    "    print(\"max_val = \",max_val )\n",
    "\n",
    "    # Subtract the minimum value from the batch\n",
    "    batch = batch - min_val\n",
    "\n",
    "    # Divide the batch by the range (max_val - min_val)\n",
    "    range_val = max_val - min_val\n",
    "    print(\"range_val = \",range_val)\n",
    "    batch = batch / range_val\n",
    "\n",
    "    # Add a small constant to avoid dividing by zero\n",
    "    batch = batch + 1e-7\n",
    "    print(\"post operation . Shape = \",batch.shape)\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ecc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_tensor = torch.rand(32,1,224,224)\n",
    "norm_bat = normalize_batch(bat_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a batch of random tensors\n",
    "batch_size = 32\n",
    "input_shape = (1,224, 224)  # Example input shape\n",
    "batch = torch.randn(batch_size, *input_shape)  # Example batch of shape (32, 3, 4)\n",
    "print(\"batch shape \" ,batch.shape)\n",
    "# Calculate mean and standard deviation along specific dimensions\n",
    "mean = batch.mean(dim=(1, 2), keepdim=True)\n",
    "std = batch.std(dim=(1, 2), keepdim=True)\n",
    "# Normalize the batch of tensors using mean and standard deviation\n",
    "normalized_batch = (batch - mean) / std\n",
    "\n",
    "\n",
    "# Print original and normalized batch of tensors\n",
    "#print(\"Original Batch:\\n\", batch)\n",
    "print(\"Normalized Batch:\\n\", normalized_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533d7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(4,1,15360,device = 'cuda')\n",
    "#convnext_xlarge_in22k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd741d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with some NaN values\n",
    "a = torch.tensor([.2, .52,  .35])\n",
    "\n",
    "# Check if any value in the tensor is NaN\n",
    "has_nan = torch.isnan(a).any().item()\n",
    "assert (has_nan) ,\"Tensor contains NaN values.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a602486",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = torch.rand(4,1,15360)\n",
    "spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=True,verbose = False,fmin = 100,fmax = 2000)\n",
    "\n",
    "spec = spec_layer(x_t)\n",
    "print(spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "a = torch.tensor([0.5, 3.1, .15, .20])\n",
    "\n",
    "# Find the maximum value in the tensor\n",
    "max_value = torch.max(a)\n",
    "\n",
    "# Check if the maximum value is greater than 1\n",
    "assert not(max_value) > 1, \"Tensor a does not contain values greater than 1.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(5)\n",
    "assert not(torch.isnan(x1)) , \"No nan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04773ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.isnan(x).any().item()):\n",
    "    print(\"not null\")\n",
    "else:\n",
    "    print(\"null\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1621e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_optimizer as optim\n",
    "\n",
    "model = model_new\n",
    "optimizer = optim.NovoGrad(\n",
    "    model.parameters(),\n",
    "    lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    "    grad_averaging=False,\n",
    "    amsgrad=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.optim import Novograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d51d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a43ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = torch.rand(4,1,2787)\n",
    "\n",
    "print(x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fe6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model_new(x_t.to('cuda') , train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c343bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa7769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4167d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m_o= pre_tr_model(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21741aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load a grayscale image (shape: [1, height, width])\n",
    "gray_img = torch.rand(1, 256, 256)\n",
    "gray_img = (gray_img - gray_img.min()) / (gray_img.max() - gray_img.min())\n",
    "\n",
    "# Convert the grayscale image to an RGB image (shape: [1, 3, height, width])\n",
    "rgb_img = torch.cat((gray_img, gray_img, gray_img), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd14156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "rgb_img.shape\n",
    "img_t= T.ToPILImage(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597bdc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-large-22k-384\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2157cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "o = processor(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b18316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(o['pixel_values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef128941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23369891",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ce53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = timm.create_model('convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047d3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
