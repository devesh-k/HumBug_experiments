{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d0fa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6689acb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3dd8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dli/task/ComParE2022_VecNet/notebooks/DK', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/root/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ade729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "#import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad449e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dli/task/ComParE2022_VecNet/src', '/dli/task/ComParE2022_VecNet/notebooks/DK', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/root/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3959fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba3522f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e52b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ce6e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.21.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.28.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (6.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib>=2.2->seaborn) (1.2.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib>=2.2->seaborn) (59.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa354e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/huggingface/datasets.git\n",
      "  Cloning https://github.com/huggingface/datasets.git to /tmp/pip-req-build-95swt5b1\n",
      "  Running command git clone -q https://github.com/huggingface/datasets.git /tmp/pip-req-build-95swt5b1\n",
      "  Resolved https://github.com/huggingface/datasets.git to commit f401758c5019ede4404994d5d59220125984874d\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (1.3.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (3.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (1.21.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (10.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (2021.11.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (2.26.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (0.11.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.9.1.dev0) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.1.dev0) (4.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.1.dev0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==2.9.1.dev0) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.1.dev0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.1.dev0) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.1.dev0) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.1.dev0) (2022.6.15)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.1.dev0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.1.dev0) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.1.dev0) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.1.dev0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.9.1.dev0) (21.4.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.9.1.dev0) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.9.1.dev0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets==2.9.1.dev0) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.26.0.dev0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jiwer in /opt/conda/lib/python3.8/site-packages (2.5.1)\n",
      "Requirement already satisfied: levenshtein==0.20.2 in /opt/conda/lib/python3.8/site-packages (from jiwer) (0.20.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from levenshtein==0.20.2->jiwer) (2.13.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Hugging face\n",
    "!pip install git+https://github.com/huggingface/datasets.git\n",
    "!pip install transformers\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41d88a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers= 8\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    num_workers=1\n",
    "    \n",
    "     \n",
    "\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27716a4a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d10d05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(config.step_size/config.win_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae493854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "# def get_offsets_df(df, short_audio=False):\n",
    "#     audio_offsets = []\n",
    "#     #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "#     min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "#     step_frac = config.step_size/config.win_size\n",
    "#     for _,row in df.iterrows():\n",
    "#         if row['length'] > min_length:\n",
    "#             stride = step_frac*min_length\n",
    "#             audio_offsets.append({'id':row['id'], 'offset':0, 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "#             num_splits = int((row['length']-min_length)//stride) + 1\n",
    "#             print(\"row id = \" +str(row['id']) +\" audio_duration = \" + str(row['length']) +  \"number of splits = \" + str(num_splits))\n",
    "#             for i in range(1, int((row['length']-min_length)//stride)):\n",
    "#                 print(\"i = \" + str (i) + \" offset = \" + str(int(min_length+(i*stride)*config.rate)))\n",
    "#                 audio_offsets.append({'id': row['id'], 'offset':int((min_length+i*stride)*config.rate), 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "#         elif short_audio:\n",
    "#             audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "#     return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e30b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ccef89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    stride = step_frac*min_length\n",
    "#     print(\"min_length = \" +str(min_length))\n",
    "#     print(\"step_frac = \" +str(step_frac))\n",
    "#     print(\"stride = \" +str(stride))\n",
    "    for _,row in df.iterrows():\n",
    "        #processed_data keeps track of the tensor_values processed thus far\n",
    "        if row['length'] > min_length:\n",
    "            processed_data = 0\n",
    "            #total_data is the total tensor present in the audio\n",
    "            total_data = config.rate*row['length']\n",
    "            #print(\"********\")\n",
    "            count = 0\n",
    "#             print(\"count = \" +str(count))\n",
    "#             print(\"id = \" + str(row['id']) + \" duration = \" +str(row['length']) + \"total x vals = \" + str(total_data))\n",
    "            inner_loop_flag = False\n",
    "            #print(\"going into the inner loop to offset....\")\n",
    "            while(processed_data < total_data):\n",
    "                #print(\"inside inner loop.....\")\n",
    "                start = count*stride*config.rate\n",
    "                #now find out the row_len\n",
    "                if total_data - (start + min_length*config.rate) >= 0:\n",
    "                    #print(\"full chunk \")\n",
    "                    row_len = min_length\n",
    "                    end = start + row_len*config.rate\n",
    "                    audio_offsets.append({'id':row['id'], 'offset':count, 'length': row_len,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "#                     print(\"count = \" +str(count) + \"offset = \" +str(count) + \"start = \" +str(start) + \"end = \" +str(end))\n",
    "#                     print(\"for count.... = \" + str(count) + \"processed data = \" +str(processed_data))\n",
    "                    count+=1\n",
    "                    processed_data = (count*stride)*config.rate\n",
    "                    \n",
    "                else:\n",
    "                    inner_loop_flag = True\n",
    "                    break\n",
    "                    \n",
    "                                                       \n",
    "            #for processing residual data\n",
    "            if(inner_loop_flag):\n",
    "                #print(\"processing residual ....processed \" +str(processed_data) + \" of \" + str(total_data))\n",
    "                start = count*stride*config.rate\n",
    "                resid_durn = round((total_data - processed_data)/config.rate,2)\n",
    "                end = total_data\n",
    "                #print(\"for...\" + str(row['id']) + \" adding the residual data in the data frame with duration = \" + str(resid_durn))\n",
    "                audio_offsets.append({'id':row['id'], 'offset':count, 'length':resid_durn ,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "            \n",
    "        elif short_audio:\n",
    "            start = 0\n",
    "            end = row['length']*config.rate\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind'],'start':0 , 'end':end})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c496b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c0beb0",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47a48ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     df = pd.read_csv(config.data_df_msc_test)\n",
    "# else:\n",
    "df = pd.read_csv(config.data_df)\n",
    "\n",
    "# removing rows which do not have specie information\n",
    "df = df.loc[df['species'].notnull()]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6688ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9440ecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16c976b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retaining ONLY those species that are present in the list of 'classes'\n",
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00da24a",
   "metadata": {},
   "source": [
    "At this stage we have all extracted the data with specie information and have encoded the specie encoding in a col = 'specie_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8fbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "742e7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data- this is as per the humbug paper\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc275061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cadd97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "838992e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "import seaborn as sns\n",
    "sns.countplot(x = 'species', data = df_all , ax = ax , hue = 'gender',palette='dark')\n",
    "#ax.bar_label(ax.containers[0])\n",
    "#ax.bar_label(ax.containers[-1], fmt='Count:\\n%.2f', label_type='center')\n",
    "plt.xticks(rotation=90 )\n",
    "plt.title(\"Distribution of Species \")\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('figure', titlesize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f667518b",
   "metadata": {},
   "source": [
    "### Train-Test split( avoiding sklearn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "116951c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa33aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91caf9a",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4914fbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9afe7c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79750fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dcf78",
   "metadata": {},
   "source": [
    "We've confirmed that there is no recording that is common in Train,Test,val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a3a62",
   "metadata": {},
   "source": [
    "### Next, we perform \"offsets\", spliting each(long) recording into multiple 1.92 secs chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45548436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df = df.loc[df['City'] == 'Paris']\n",
    "#df_train_trunc = df_train.loc[df_train['id'] == 220608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d239ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bdcf528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_train_offset_trunc = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40bf923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_offset_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95259cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration,DEBUG = False ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5625ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "512495bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c83c75e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train offset = 35043\n",
      "length of test offset = 11043\n",
      "length of val offset = 9466\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64ff6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/audio\n"
     ]
    }
   ],
   "source": [
    "print(config.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2546d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239610f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "524f4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True , drop = True)\n",
    "df_test_offset.reset_index(inplace = True , drop= True)\n",
    "df_val_offset.reset_index(inplace = True , drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac992d",
   "metadata": {},
   "source": [
    "### Let's check for data leakage in offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0020ef5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, offset_x, length_x, specie_ind_x, start_x, end_x, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_test_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f57608a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, offset_x, length_x, specie_ind_x, start_x, end_x, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac646a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>start_x</th>\n",
       "      <th>end_x</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, offset_x, length_x, specie_ind_x, start_x, end_x, offset_y, length_y, specie_ind_y, start_y, end_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbe446",
   "metadata": {},
   "source": [
    "### At this stage we've a dataframe of recordin ids and each row corresponds to a 1.92 secs recording or shorter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a1f85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dd75607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32524317 0.56223527 3.61119126 0.62576786 1.97670352 4.1207667\n",
      " 3.00231323 5.25855342]\n"
     ]
    }
   ],
   "source": [
    "#Class imbalance \n",
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafda0a",
   "metadata": {},
   "source": [
    "Let us now get the class distribution for each of the dataframes- train,test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034c2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b066f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = train\n",
      "i = 0\n",
      "13468\n",
      "DF type = train\n",
      "i = 1\n",
      "7791\n",
      "DF type = train\n",
      "i = 2\n",
      "1213\n",
      "DF type = train\n",
      "i = 3\n",
      "7000\n",
      "DF type = train\n",
      "i = 4\n",
      "2216\n",
      "DF type = train\n",
      "i = 5\n",
      "1063\n",
      "DF type = train\n",
      "i = 6\n",
      "1459\n",
      "DF type = train\n",
      "i = 7\n",
      "833\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee3f5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = Val\n",
      "i = 0\n",
      "3908\n",
      "DF type = Val\n",
      "i = 1\n",
      "2175\n",
      "DF type = Val\n",
      "i = 2\n",
      "264\n",
      "DF type = Val\n",
      "i = 3\n",
      "1981\n",
      "DF type = Val\n",
      "i = 4\n",
      "318\n",
      "DF type = Val\n",
      "i = 5\n",
      "260\n",
      "DF type = Val\n",
      "i = 6\n",
      "478\n",
      "DF type = Val\n",
      "i = 7\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ca1241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = test\n",
      "i = 0\n",
      "4717\n",
      "DF type = test\n",
      "i = 1\n",
      "2105\n",
      "DF type = test\n",
      "i = 2\n",
      "484\n",
      "DF type = test\n",
      "i = 3\n",
      "2114\n",
      "DF type = test\n",
      "i = 4\n",
      "559\n",
      "DF type = test\n",
      "i = 5\n",
      "350\n",
      "DF type = test\n",
      "i = 6\n",
      "491\n",
      "DF type = test\n",
      "i = 7\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aafce9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(9.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d7cf848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221103</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221111</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  offset  length  specie_ind    start      end\n",
       "0  221103       0    1.92           7      0.0  15360.0\n",
       "1  221103       1    1.92           7   5120.0  20480.0\n",
       "2  221103       2    1.28           7  10240.0  20480.0\n",
       "3  221111       0    1.92           7      0.0  15360.0\n",
       "4  221111       1    1.92           7   5120.0  20480.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()\n",
    "#df_train_offset.drop(['level_0'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40dcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "429b85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getitem_test(df_train_offset_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5d9ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getitem_test(df , target_samp_rate = 16000):\n",
    "#         #real_idx = idx % len(self.audio_df)\n",
    "#         for _,row in df.iterrows():\n",
    "#             #print(row)\n",
    "#             temp_id = int(row['id'])\n",
    "#             path_var = config.data_dir +\"/\" +str(temp_id)+ str(\".wav\")\n",
    "#             entire_aud, inp_rate = torchaudio.load(path_var)\n",
    "#             print(\"BEFORE RESAMPLING shape of entire  audio = \" +str(entire_aud.shape))\n",
    "#             print(\"processsing file on \" +str(path_var) + \" shape =  \" + str(entire_aud.shape))\n",
    "#             if inp_rate != target_samp_rate:\n",
    "#                 print(\" Original sample rate = \" +str(inp_rate)+ \" resampling ...\")\n",
    "#                 import torchaudio.transforms as T\n",
    "#                 resampler = T.Resample(inp_rate, target_samp_rate, dtype=entire_aud.dtype)\n",
    "#                 entire_aud = resampler(entire_aud)\n",
    "#                 print(\"shape post resampling = \" + str(entire_aud.shape))\n",
    "#                 print(\"processsing file on \" +str(path_var) + \"Post resample shape =  \" + str(entire_aud.shape))\n",
    "#             aud_len = row['length']\n",
    "#             offset = int(row['offset'])\n",
    "#             print(\"offset = \" +str(offset))\n",
    "#             #print(\"sliced val = \" +str(int((offset+config.min_duration)*config.rate)))\n",
    "#             start_pos = int(round(row['start']))\n",
    "#             print(\"start_pos = \" +str(start_pos))\n",
    "#             end_pos =  int(round(row['end']))\n",
    "#             print(\"end_pos = \" +str(end_pos))\n",
    "            \n",
    "#             x = entire_aud[:,start_pos:end_pos]\n",
    "#             print(\"extracted x = \" +str(x))\n",
    "#             print(\"x shape = \" +str(x.shape))\n",
    "#             if aud_len < config.min_duration:\n",
    "#                 #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "#                 print(\"padding on \" +str(path_var))\n",
    "#                 f_out = pad_mean(x)\n",
    "#                 f = f_out.squeeze(0)\n",
    "#                 print(\"returning from padding  SHape = \" +str(f.shape))\n",
    "#                 print(\"f_out_shape post padding= \" +str(f.shape))\n",
    "#             else:\n",
    "#                 f_out = x[0]\n",
    "#                 f_out = f_out.squeeze(0)\n",
    "#                 print(\"f_out_shape = \" +str(f_out.shape))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69e7cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "#df_temp = df_train_offset[df_train_offset['id'] == 221112]\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9bd904",
   "metadata": {},
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25432f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92aebd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../../content/data\"\n",
    "\n",
    "\n",
    "df_train_offset.to_csv(f\"{save_path}/train.csv\",  encoding=\"utf-8\", index=False)\n",
    "df_test_offset.to_csv(f\"{save_path}/test.csv\",  encoding=\"utf-8\", index=False)\n",
    "df_val_offset.to_csv(f\"{save_path}/val.csv\",  encoding=\"utf-8\", index=False)\n",
    "\n",
    "\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)\n",
    "# print(val_df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "009f117c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221103</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221111</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  offset  length  specie_ind    start      end\n",
       "0  221103       0    1.92           7      0.0  15360.0\n",
       "1  221103       1    1.92           7   5120.0  20480.0\n",
       "2  221103       2    1.28           7  10240.0  20480.0\n",
       "3  221111       0    1.92           7      0.0  15360.0\n",
       "4  221111       1    1.92           7   5120.0  20480.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bb4c5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221122</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221122</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>6</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221122</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>6</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221124</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221124</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>6</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  offset  length  specie_ind    start      end\n",
       "0  221122       0    1.92           6      0.0  15360.0\n",
       "1  221122       1    1.92           6   5120.0  20480.0\n",
       "2  221122       2    1.28           6  10240.0  20480.0\n",
       "3  221124       0    1.92           6      0.0  15360.0\n",
       "4  221124       1    1.92           6   5120.0  20480.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_offset.head(\n",
    ")\n",
    "#df_val_offset.drop(['level_0'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74da3bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221150</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221150</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221121</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>6</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>20480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  offset  length  specie_ind    start      end\n",
       "0  221150       0    1.92           0      0.0  15360.0\n",
       "1  221150       1    1.92           0   5120.0  20480.0\n",
       "2  221150       2    1.28           0  10240.0  20480.0\n",
       "3  221121       0    1.92           6      0.0  15360.0\n",
       "4  221121       1    1.92           6   5120.0  20480.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_offset.head()\n",
    "#df_test_offset.drop(['level_0'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ce1867b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'offset', 'length', 'specie_ind', 'start', 'end'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c72466a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'offset', 'length', 'specie_ind', 'start', 'end'], dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_offset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca41dc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'offset', 'length', 'specie_ind', 'start', 'end'], dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_offset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd30eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-49038fabf0a07374/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460d2b1073a442818488c9932ae1aac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6288dd5410840869cc7860e42e5a401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ba11dad3df4d52b92db03174fc7059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbbf4a583464d92aeacddc5000b4245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f65065207c147eb81dbb201521b6ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-49038fabf0a07374/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6a227c2ae94be1aaab1ede4080e643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"../../content/data/train.csv\", \n",
    "    \"validation\": \"../../content/data/val.csv\",\n",
    "    \"test\": \"../../content/data/test.csv\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27b9d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "\n",
    "# print(train_dataset)\n",
    "# print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80c872b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221103"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eaceca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'offset', 'length', 'specie_ind', 'start', 'end']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dea5f8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 221103,\n",
       " 'offset': 0,\n",
       " 'length': 1.92,\n",
       " 'specie_ind': 7,\n",
       " 'start': 0.0,\n",
       " 'end': 15360.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e082d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal_tensor(example, target_resample_rate = 16000):\n",
    "    #print(type(example['id']))\n",
    "    temp_id = example['id']\n",
    "    path_var = os.path.join(\"/\",\"dli\",\"task\",\"ComParE2022_VecNet\",\"data\",\"audio\")\n",
    "    path_var = path_var +\"/\" +str(temp_id)+ str(\".wav\")\n",
    "    entire_aud, inp_rate = torchaudio.load(path_var)\n",
    "    #print(\"shape of entire  audio = \" +str(entire_aud.shape))\n",
    "    #print(\"processsing file on \" +str(path_var) + \" shape =  \" + str(entire_aud.shape))\n",
    "    if inp_rate != target_resample_rate:\n",
    "        #print(\" Original sample rate = \" +str(inp_rate)+ \" resampling ...\")\n",
    "        import torchaudio.transforms as T\n",
    "        resampler = T.Resample(inp_rate, target_resample_rate, dtype=entire_aud.dtype)\n",
    "        entire_aud = resampler(entire_aud)\n",
    "    #print(\"processsing file on \" +str(path_var) + \"Post resample shape =  \" + str(entire_aud.shape))\n",
    "    aud_len = example['length']\n",
    "    #offset = int(example['offset'])\n",
    "    #print(\"offset = \" +str(offset))\n",
    "    #print(\"sliced val = \" +str(int((offset+config.min_duration)*config.rate)))\n",
    "    start_pos = int(round(example['start']))\n",
    "    #print(\"start_pos = \" +str(start_pos))\n",
    "    end_pos =  int(round(example['end']))\n",
    "    #print(\"end_pos = \" +str(end_pos))\n",
    "    x = entire_aud[:,start_pos:end_pos]\n",
    "    #print(\"extracted x = \" +str(x))\n",
    "    #print(\"x shape = \" +str(x.shape))\n",
    "    if aud_len < 1.92:\n",
    "        #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "        #print(\"padding on \" +str(path_var))\n",
    "        f_out = pad_mean(x)\n",
    "        #print(\" after padding . f_out shape = \" +str(f_out.shape))\n",
    "        f = f_out.squeeze(0)\n",
    "        example[\"signal\"] = f\n",
    "        \n",
    "        \n",
    "        #print(\"returning from padding  SHape = \" +str(f_out.shape))\n",
    "    else:\n",
    "        f_out = x[0]\n",
    "        #print(\" f_out shape = \" +str(f_out.shape))\n",
    "        f = f_out.squeeze(0)\n",
    "        example[\"signal\"] = f\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b512869c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'offset', 'length', 'specie_ind', 'start', 'end'],\n",
       "    num_rows: 35043\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9a4073e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795126e747104528b502eb0a53bf6785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/35043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "updated_train = train_dataset.map(get_signal_tensor ,num_proc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42a5245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065da6aedc034fe19f10bff15a815259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/9466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#eval_dataset\n",
    "updated_eval = eval_dataset.map(get_signal_tensor,num_proc = 4 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37c5b029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed220c3280d4f09820a87f0dd1d599b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/11043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "updated_test = test_dataset.map(get_signal_tensor,num_proc = 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d99c11bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'offset', 'length', 'specie_ind', 'start', 'end', 'signal'],\n",
       "    num_rows: 35043\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "969ded54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.rename_column(\"old_column_name_1\", \"new_column_name_1\")\n",
    "updated_train = updated_train.rename_column(\"signal\",\"input_values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "635e448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_train = updated_train.rename_column(\"specie_ind\",\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "339c8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_eval = updated_eval.rename_column(\"specie_ind\",\"labels\")\n",
    "updated_eval = updated_eval.rename_column(\"signal\",\"input_values\")\n",
    "updated_test = updated_test.rename_column(\"specie_ind\",\"labels\")\n",
    "updated_test = updated_test.rename_column(\"signal\",\"input_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41b231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7960df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We need to specify the input and output column\n",
    "# input_column = \"path\"\n",
    "# output_column = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0cb86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, Wav2Vec2Processor, Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96a5cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"facebook/wav2vec2-base-100k-voxpopuli\"\n",
    "pooling_mode = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "638ef10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id  =  {'an arabiensis': 0, 'culex pipiens complex': 1, 'ae aegypti': 2, 'an funestus ss': 3, 'an squamosus': 4, 'an coustani': 5, 'ma uniformis': 6, 'ma africanus': 7}\n",
      "id2label  =  {0: 'an arabiensis', 1: 'culex pipiens complex', 2: 'ae aegypti', 3: 'an funestus ss', 4: 'an squamosus', 5: 'an coustani', 6: 'ma uniformis', 7: 'ma africanus'}\n"
     ]
    }
   ],
   "source": [
    "label2id={label: i for i, label in enumerate(classes)}\n",
    "print(\"label2id  = \", label2id)\n",
    "id2label={i: label for i, label in enumerate(classes)}\n",
    "print(\"id2label  = \", id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d8f95338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d241db81baeb438f83b010f44c1e6e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setting up the model's configuration \n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(classes),\n",
    "    label2id=label2id  ,\n",
    "    id2label=id2label  ,\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "743dbdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e145969bc146b0b26dcdb0427e4509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb8cbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6fc2c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "262cdc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`)\n",
    "            The feature_extractor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e74139fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(feature_extractor=feature_extractor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e179cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_regression = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f4e51392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "\n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0aa5b8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82869f2e59ae4fffae6385c90a4be291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-100k-voxpopuli were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.codevectors', 'project_hid.weight', 'project_q.weight', 'project_q.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-100k-voxpopuli and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c8662d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'offset', 'length', 'labels', 'start', 'end', 'input_values'],\n",
       "    num_rows: 35043\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700d4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5decbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5887628c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../../content/MSC\",\n",
    "    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-base-100k-eating-sound-collection\"\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=512,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=100.0,\n",
    "    #fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "35c1b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        #if self.use_amp:\n",
    "        #with autocast():\n",
    "            #loss = self.compute_loss(model, inputs)\n",
    "        #else:\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        #if self.use_amp:\n",
    "            #self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba5292e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=updated_train,\n",
    "    eval_dataset=updated_eval,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18504690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 35043\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2048\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1700\n",
      "  Number of trainable parameters = 90768008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='379' max='1700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 379/1700 2:05:12 < 7:18:43, 0.05 it/s, Epoch 22.23/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.072200</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.072200</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.073100</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.122500</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.070200</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.071200</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.122200</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.072700</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.071900</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.070700</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.122900</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.071100</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.123200</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.070900</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.070600</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.122300</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.072700</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.072200</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.070500</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.124800</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.071600</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.057363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-17\n",
      "Configuration saved in ../../content/MSC/checkpoint-17/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-17/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-17/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-34\n",
      "Configuration saved in ../../content/MSC/checkpoint-34/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-34/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-34/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-51\n",
      "Configuration saved in ../../content/MSC/checkpoint-51/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-51/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-51/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-68\n",
      "Configuration saved in ../../content/MSC/checkpoint-68/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-68/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-68/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-85\n",
      "Configuration saved in ../../content/MSC/checkpoint-85/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-85/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-85/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-102\n",
      "Configuration saved in ../../content/MSC/checkpoint-102/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-102/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-102/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-119\n",
      "Configuration saved in ../../content/MSC/checkpoint-119/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-119/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-119/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-136\n",
      "Configuration saved in ../../content/MSC/checkpoint-136/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-136/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-136/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-153\n",
      "Configuration saved in ../../content/MSC/checkpoint-153/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-153/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-153/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-170\n",
      "Configuration saved in ../../content/MSC/checkpoint-170/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-170/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-170/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-187\n",
      "Configuration saved in ../../content/MSC/checkpoint-187/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-187/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-187/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-204\n",
      "Configuration saved in ../../content/MSC/checkpoint-204/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-204/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-204/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-221\n",
      "Configuration saved in ../../content/MSC/checkpoint-221/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-221/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-221/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-238\n",
      "Configuration saved in ../../content/MSC/checkpoint-238/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-238/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-238/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-255\n",
      "Configuration saved in ../../content/MSC/checkpoint-255/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-255/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-255/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-272\n",
      "Configuration saved in ../../content/MSC/checkpoint-272/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-272/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-272/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-289\n",
      "Configuration saved in ../../content/MSC/checkpoint-289/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-289/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-289/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-306\n",
      "Configuration saved in ../../content/MSC/checkpoint-306/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-306/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-306/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-323\n",
      "Configuration saved in ../../content/MSC/checkpoint-323/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-323/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-323/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-340\n",
      "Configuration saved in ../../content/MSC/checkpoint-340/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-340/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-340/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-357\n",
      "Configuration saved in ../../content/MSC/checkpoint-357/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-357/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-357/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: end, id, start, length, offset. If end, id, start, length, offset are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9466\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../../content/MSC/checkpoint-374\n",
      "Configuration saved in ../../content/MSC/checkpoint-374/config.json\n",
      "Model weights saved in ../../content/MSC/checkpoint-374/pytorch_model.bin\n",
      "Feature extractor saved in ../../content/MSC/checkpoint-374/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96127ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afabf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cc8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d2ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a897c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb77e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e0821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5294fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d604a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850dcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddeab92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a528a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2ef6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff14c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b641be7",
   "metadata": {},
   "source": [
    "### Prepare Data for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a355392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset_train = Dataset.from_pandas(df_train_offset)\n",
    "dataset_val = Dataset.from_pandas(df_val_offset)\n",
    "dataset_test = Dataset.from_pandas(df_test_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cf159",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 5\n",
    "print(\"x1 = \" +str(x1))\n",
    "x2 = torch.tensor(x1)\n",
    "print(\"x2 = \" +str(x2))\n",
    "print(x2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4c6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
