{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdc9af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dli/task/ComParE2022_VecNet/notebooks/DK', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/root/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c519a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/nnAudio/Spectrogram.py:4: Warning: importing Spectrogram subpackage will be deprecated soon. You should import the feature extractor from the feature subpackage. See actual documentation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "#from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "# import IPython.display as ipd\n",
    "\n",
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "## nnAudio\n",
    "from nnAudio import features , Spectrogram\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0042791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser(description='Trainable_SpecAugment', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "batch_size = 16\n",
    "num_workers= 4\n",
    "pin_memory = True\n",
    "#test_batch_size = 256\n",
    "DEBUG = False\n",
    "num_epochs= 200              \n",
    "USE_SHORT_AUDIO = True\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89ace9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5465cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    stride = step_frac*min_length\n",
    "#     print(\"min_length = \" +str(min_length))\n",
    "#     print(\"step_frac = \" +str(step_frac))\n",
    "#     print(\"stride = \" +str(stride))\n",
    "    for _,row in df.iterrows():\n",
    "        #processed_data keeps track of the tensor_values processed thus far\n",
    "        if row['length'] > min_length:\n",
    "            processed_data = 0\n",
    "            #total_data is the total tensor present in the audio\n",
    "            total_data = config.rate*row['length']\n",
    "            #print(\"********\")\n",
    "            count = 0\n",
    "            #print(\"count = \" +str(count))\n",
    "            #print(\"id = \" + str(row['id']) + \" duration = \" +str(row['length']) + \"total x vals = \" + str(total_data))\n",
    "            inner_loop_flag = False\n",
    "            #print(\"going into the inner loop to offset....\")\n",
    "            while(processed_data < total_data):\n",
    "                #print(\"inside inner loop.....\")\n",
    "                start = count*stride*config.rate\n",
    "                #now find out the row_len\n",
    "                if total_data - (start + min_length*config.rate) >= 0:\n",
    "                    #print(\"full chunk \")\n",
    "                    row_len = min_length\n",
    "                    end = start + row_len*config.rate\n",
    "                    audio_offsets.append({'id':row['id'], 'offset':count, 'length': row_len,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "                    #print(\"count = \" +str(count) + \"offset = \" +str(count) + \"start = \" +str(start) + \"end = \" +str(end))\n",
    "                    #print(\"for count.... = \" + str(count) + \"processed data = \" +str(processed_data))\n",
    "                    count+=1\n",
    "                    processed_data = (count*stride)*config.rate\n",
    "                    \n",
    "                else:\n",
    "                    inner_loop_flag = True\n",
    "                    break\n",
    "                    \n",
    "                                                       \n",
    "            #for processing residual data\n",
    "            if(inner_loop_flag):\n",
    "                #print(\"processing residual ....processed \" +str(processed_data) + \" of \" + str(total_data))\n",
    "                start = count*stride*config.rate\n",
    "                resid_durn = round((total_data - processed_data)/config.rate,2)\n",
    "                end = total_data\n",
    "                #print(\"for...\" + str(row['id']) + \" adding the residual data in the data frame with duration = \" + str(resid_durn))\n",
    "                audio_offsets.append({'id':row['id'], 'offset':count, 'length':resid_durn ,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "            \n",
    "        elif short_audio:\n",
    "            start = 0\n",
    "            end = row['length']*config.rate\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind'],'start':0 , 'end':end})\n",
    "    return pd.DataFrame(audio_offsets)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8918b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ####### Prepare df######\n",
    "\n",
    "def prepare_df(classes ,csv_loc = config.data_df  ):\n",
    "    \"\"\"This function reads a csv and creates a dataframe for further processing.\"\"\"\n",
    "    df = pd.read_csv(csv_loc)\n",
    "    #df = df.loc[df['Grade'].notnull()]\n",
    "    df = df.loc[df['species'].notnull()]\n",
    "    # a new column for specie_index to hold numerical values for specie\n",
    "    df['specie_ind'] = \"NULL_VAL\"\n",
    "    ind = 0\n",
    "    for specie in classes:\n",
    "        print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "        row_indexes=df[df['species']==specie].index \n",
    "        df.loc[row_indexes,'specie_ind']= ind\n",
    "        ind+=1\n",
    "    #remove all the rows where specie is other than the one present in classes\n",
    "    df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "    #filter the data for TZ and cup recordings only\n",
    "    idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "    df_all = df[idx_multiclass]\n",
    "    df_all.reset_index(inplace=True, drop = True )\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5edd54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plt df\n",
    "def plot_df(df):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    import seaborn as sns\n",
    "    sns.countplot(x = 'species', data = df , ax = ax , hue = 'gender',palette='dark')\n",
    "    #ax.bar_label(ax.containers[0])\n",
    "    #ax.bar_label(ax.containers[-1], fmt='Count:\\n%.2f', label_type='center')\n",
    "    plt.xticks(rotation=90 )\n",
    "    plt.title(\"Distribution of Species \")\n",
    "    plt.rc('xtick', labelsize=12)\n",
    "    plt.rc('xtick', labelsize=12)\n",
    "    plt.rc('axes', labelsize=15)\n",
    "    plt.rc('figure', titlesize=15)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3020bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train _test split####\n",
    "def train_test_split(df_all):\n",
    "    np.random.seed(42)\n",
    "    msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "    df_test = df_all[msk_test]\n",
    "    df_train_temp  = df_all[~msk_test]\n",
    "    msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "    df_val = df_train_temp[msk_train]\n",
    "    df_train  = df_train_temp[~msk_train]\n",
    "    return df_train ,df_val ,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6233349",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validate split ####\n",
    "def validate_split(df1 , df2):\n",
    "    df_temp = pd.merge(df1,df2, on = 'id', how = 'inner')\n",
    "    #print(df_temp)\n",
    "    common_elem = len(df_temp)\n",
    "    #print(\"common_elem = \",common_elem)\n",
    "    con = (common_elem == 0)\n",
    "    #print(\"condition = \",con)\n",
    "    assert (con), \"Split has issues\"\n",
    "    print(\"split is a success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2378ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Specie _distribution ###\n",
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c24fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class weights to address imbalance in classes ###\n",
    "def get_class_weights(df):\n",
    "    np.array(df_train_offset.specie_ind)\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df.specie_ind)),y=np.array(np.array(df.specie_ind)))\n",
    "    print(type(class_weights))\n",
    "    print(class_weights.shape)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13ed371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pad_mean #####\n",
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbcdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot confusion Matrix ######\n",
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce1d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normalize_batch, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_mean = torch.mean(x, dim=0, keepdim=True)\n",
    "        batch_std = torch.std(x, dim=0, keepdim=True)\n",
    "        epsilon = 1e-8\n",
    "        batch_std = torch.sqrt(batch_std ** 2 + epsilon)\n",
    "        batch_normalized = (x - batch_mean) / batch_std\n",
    "        return batch_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82df7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa337afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channel=1, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves\n",
    "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
    "        # https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, layer=100):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "    'resnet18': [resnet18, 512],\n",
    "    'resnet34': [resnet34, 512],\n",
    "    'resnet50': [resnet50, 2048],\n",
    "    'resnet101': [resnet101, 2048],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class SupConResNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, name='resnet50', head='mlp', feat_dim=128):\n",
    "        super(SupConResNet, self).__init__()\n",
    "        model_fun, dim_in = model_dict[name]\n",
    "        self.encoder = model_fun()\n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        feat = F.normalize(self.head(feat), dim=1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c44a8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name, image_size = 224):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        #self.backbone = timm.create_model(model_name, pretrained=True, num_classes=8, in_chans=1, drop_path_rate=0.2, global_pool='max', drop_rate=0.25)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "                #### end layer freezing\n",
    "        #self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size),antialias = True)\n",
    "        self.encoder = SupConResNet()\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features= 1)\n",
    "        self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=False,verbose = False).to('cuda')\n",
    "        #self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "        \n",
    "    def forward(self, x,train = True):\n",
    "        # first compute spectrogram\n",
    "        spec_gram = self.spec_layer(x)\n",
    "        output = {}\n",
    "        #print(\"post spec gram shape = \",spec_gram.shape)\n",
    "        spec_gram = self.batch_norm(spec_gram.unsqueeze(dim = 1))\n",
    "        #print(\"post norm shape = \",spec_gram.shape)\n",
    "        spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "        assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after spec gram creation.\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if train == True:\n",
    "                #generate a random number and if condition is met apply aug\n",
    "                ta_transformations_rndm_choice = VT.RandomChoice([AT.FrequencyMasking(freq_mask_param=100),AT.TimeMasking(time_mask_param=50)], p=[.4, .4])\n",
    "                ta_transformations_rndm_apply = VT.RandomApply([AT.FrequencyMasking(freq_mask_param=50),AT.TimeMasking(time_mask_param=25)],p = .2)\n",
    "                spec_gram = ta_transformations_rndm_choice(spec_gram)\n",
    "                spec_gram = ta_transformations_rndm_apply(spec_gram)\n",
    "                spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "                assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after augmentations  \"\n",
    "                aug_bat = [ta_transformations_rndm_choice(spec_gram),ta_transformations_rndm_choice(spec_gram)]\n",
    "                aug_bat = torch.cat(aug_bat , dim = 0)\n",
    "                #print(\"shape of augmented batch = \",aug_bat.shape)\n",
    "                #output['feat'] = aug_bat\n",
    "                \n",
    "        \n",
    "        encoder = self.encoder.to('cuda')\n",
    "        features = encoder(aug_bat)\n",
    "        #print(\"output of encoder shape = \",features.shape)\n",
    "        bsz = x.shape[0]\n",
    "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "        #loss = criterion(features, y)\n",
    "        output['feat'] = features\n",
    "        #x = self.sizer(spec_gram.squeeze(dim = 1))\n",
    "        #print(\"post sizer shape = \",x.shape)\n",
    "        #x = x.unsqueeze(dim = 1)\n",
    "        #print(\"post unsqueeze shape = \",x.shape)\n",
    "        \n",
    "        # then repeat channels\n",
    "        del spec_gram,aug_bat\n",
    "        #backbone_op_nan_check = torch.isnan(x).any().item()\n",
    "        #assert not (backbone_op_nan_check) ,\"Tensor contains NaN values in the backbone OP \"\n",
    "        #print(\"x shape = \" + str(x.shape))\n",
    "        #print(\"x = \" +str(x))\n",
    "        #pred = nn.Softmax(x)\n",
    "        #pred = x\n",
    "        #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "        #print(pred)\n",
    "        #output[\"prediction\"]=  pred \n",
    "        #print(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908d357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Test Model####\n",
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    softmax = nn.Softmax()\n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            y_pred = model(x,train = False)['prediction']\n",
    "            #y_pred_smax = softmax(y_pred)\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9700ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader,test_loader, model ,classes,class_weights,num_epochs,encoder ):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = SupConLoss(temperature= .07).to(device)\n",
    "    lr = .000015\n",
    "    base_optimiser = timm.optim.AdamP(model.parameters(), lr= lr)\n",
    "    look_optimiser = timm.optim.Lookahead(base_optimiser)\n",
    "    cooldown_epoch = 50\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(base_optimiser, t_initial= num_epochs,lr_min= lr/100,warmup_t = 5,warmup_lr_init= lr/10,noise_std=.075)\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    lr_log = []\n",
    "    for e in range(num_epochs + cooldown_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        #tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(train_loader):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 1000 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. x device = \" +str(x.device))\n",
    "                \n",
    "            \n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            x_sum = torch.sum(x,axis = 1)\n",
    "            x_sum.unsqueeze(dim = 1)\n",
    "                                  \n",
    "            with autocast():\n",
    "                output = model(x,train = True)\n",
    "                #y_pred = output['prediction']\n",
    "                #y_pred_smax = softmax(y_pred)\n",
    "                #preds = torch.argmax(y_pred, axis = 1)\n",
    "                #feat = output['feat']\n",
    "                #images = torch.cat(feat ,dim = 0)\n",
    "                #print(\"post concat images shape = \",images.shape)\n",
    "                #encoder = encoder.to(device)\n",
    "                features = output['feat']\n",
    "                loss = criterion(features, y)\n",
    "                \n",
    "            if DEBUG:\n",
    "                print(\"y_pred  = \" +str(y_pred))\n",
    "                print(\"preds = \" +str(preds))\n",
    "            train_loss += loss.item()\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),error_if_nonfinite=False ,max_norm = 1.0 )\n",
    "            base_optimiser.step()\n",
    "            del x\n",
    "            del y\n",
    "            del features\n",
    "        \n",
    "        #lr_log.append(lr)\n",
    "        look_optimiser.sync_lookahead()\n",
    "        epoch_loss = train_loss/len(train_loader)\n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        #acc_metric = best_loss\n",
    "        #best_acc_metric = best_loss\n",
    "        if  epoch_loss < best_loss:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.enc_model_dir,  checkpoint_name))\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader),  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.enc_model_dir,  checkpoint_name)) \n",
    "            current_lr = base_optimiser.param_groups[0]['lr']\n",
    "            print(\"Current LR = \" + '{0:.8f}'.format(current_lr))\n",
    "            best_epoch = e\n",
    "            best_loss = epoch_loss\n",
    "            #best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader),  overrun_counter))\n",
    "        scheduler.step(e+1)\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "240fdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dataste class #####\n",
    "class MozDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        temp_id = int(self.audio_df.loc[idx]['id'])\n",
    "        file_path = os.path.join(\"..\",\"..\",\"data\",\"audio\")\n",
    "        path_var = file_path +\"/\" +str(temp_id)+ str(\".wav\")\n",
    "        entire_aud, inp_rate = torchaudio.load(path_var)\n",
    "        if inp_rate != config.rate:\n",
    "            #print(\" Original sample rate = \" +str(inp_rate)+ \" resampling ...\")\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=entire_aud.dtype)\n",
    "            entire_aud = resampler(entire_aud)\n",
    "            #print(\"processsing file on \" +str(path_var) + \"Post resample shape =  \" + str(entire_aud.shape))\n",
    "        \n",
    "        aud_len = self.audio_df.loc[idx]['length']\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        #print(\"sliced val = \" +str(int((offset+config.min_duration)*config.rate)))\n",
    "        start_pos = int(round(self.audio_df.loc[idx]['start']))\n",
    "        #print(\"start_pos = \" +str(start_pos))\n",
    "        end_pos =  int(round(self.audio_df.loc[idx]['end']))\n",
    "        #print(\"end_pos = \" +str(end_pos))\n",
    "        x = entire_aud[:,start_pos:end_pos]\n",
    "        #print(\"extracted x = \" +str(x))\n",
    "        #print(\"x shape = \" +str(x.shape))\n",
    "        if aud_len < config.min_duration:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            #print(\"padding on \" +str(path_var))\n",
    "            f_out = pad_mean(x)\n",
    "            #print(\"returning from padding  SHape = \" +str(f_out.shape))\n",
    "        else:\n",
    "            f_out = x[0]\n",
    "            f_out = f_out.unsqueeze(0)\n",
    "            \n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            #print(\"offset = \" + str(offset))\n",
    "            #print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(f_out.shape))\n",
    "        \n",
    "        #x_val = x[:,start:end]\n",
    "        #now that we have final x- let's create specgram and add augmentations.\n",
    "                 \n",
    "        return (f_out,self.audio_df.loc[idx]['specie_ind'] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0fcf8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train_model ####\n",
    "#train_loader, val_loader, test_loader,model,classes,df_train_offset ,num_epochs = num_epochs \n",
    "\n",
    "### Get_indices ####\n",
    "def get_indices(num_values ,df ,classes = classes):\n",
    "    new_df = pd.DataFrame()\n",
    "    for ind in range(len(classes)):\n",
    "        #print(\"ind = \", ind)\n",
    "        op = df[df['specie_ind'] == ind]\n",
    "        #print(\"len op = \", len(op))\n",
    "        op_new = op.sample(n = 1)\n",
    "        #print(\"rand_ind = \" , rand_ind)\n",
    "        #([df1, df2], axis=1)\n",
    "        new_df = pd.concat([op_new,new_df],axis = 0)\n",
    "        #print(\"elem = \" , elem)\n",
    "        #new_list.append(elem)\n",
    "    if len(new_df) < num_values:\n",
    "        diff =  num_values - len(new_df)\n",
    "        #print(\"diff = \", diff)\n",
    "        remaining_elems= df.sample(n = diff)\n",
    "        #print(\"len of remaining elems = \", len(remaining_elems))\n",
    "        new_df = pd.concat([remaining_elems,new_df],axis = 0)\n",
    "        \n",
    "    #print(\"new_df = \", new_df)    \n",
    "    new_df_1 = new_df.reset_index(drop = True)\n",
    "    return new_df_1\n",
    "\n",
    "#### Load model ####\n",
    "def load_model(filepath, model=MyModel('convnext_xlarge_in22k')):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa60c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c974543c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60bb114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n",
      "now validating the split post loading and keeping TZ data\n",
      "split is a success\n",
      "split is a success\n",
      "split is a success\n",
      "now validating the split post offset_creation\n",
      "split is a success\n",
      "split is a success\n",
      "split is a success\n",
      "<class 'numpy.ndarray'>\n",
      "(8,)\n",
      "inside main. class_weigths type =  <class 'numpy.ndarray'>\n",
      "Training on cuda:0\n",
      "epoch = 0batch = 0 of 2258duraation = 0.011176848411560058\n",
      "epoch = 0batch = 1000 of 2258duraation = 10.474160011609396\n",
      "epoch = 0batch = 2000 of 2258duraation = 20.9079225619634\n",
      "Epoch: 0, Train Loss: 3.42111968, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e0_2023_05_18_23_18_01.pth\n",
      "Current LR = 0.00000150\n",
      "epoch = 1batch = 0 of 2258duraation = 0.01010580857594808\n",
      "epoch = 1batch = 1000 of 2258duraation = 10.442606449127197\n",
      "epoch = 1batch = 2000 of 2258duraation = 20.873825824260713\n",
      "Epoch: 1, Train Loss: 3.40608480, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e1_2023_05_18_23_41_35.pth\n",
      "Current LR = 0.00000420\n",
      "epoch = 2batch = 0 of 2258duraation = 0.010211960474650065\n",
      "epoch = 2batch = 1000 of 2258duraation = 10.452091765403747\n",
      "epoch = 2batch = 2000 of 2258duraation = 20.876609830061593\n",
      "Epoch: 2, Train Loss: 3.39726234, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e2_2023_05_19_00_05_09.pth\n",
      "Current LR = 0.00000690\n",
      "epoch = 3batch = 0 of 2258duraation = 0.00883558988571167\n",
      "epoch = 3batch = 1000 of 2258duraation = 10.429656330744425\n",
      "epoch = 3batch = 2000 of 2258duraation = 20.845569332440693\n",
      "Epoch: 3, Train Loss: 3.38871085, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e3_2023_05_19_00_28_41.pth\n",
      "Current LR = 0.00000960\n",
      "epoch = 4batch = 0 of 2258duraation = 0.008283162117004394\n",
      "epoch = 4batch = 1000 of 2258duraation = 10.424025940895081\n",
      "epoch = 4batch = 2000 of 2258duraation = 20.834755671024322\n",
      "Epoch: 4, Train Loss: 3.38152798, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e4_2023_05_19_00_52_13.pth\n",
      "Current LR = 0.00001230\n",
      "epoch = 5batch = 0 of 2258duraation = 0.00994723637898763\n",
      "epoch = 5batch = 1000 of 2258duraation = 10.418036476771038\n",
      "epoch = 5batch = 2000 of 2258duraation = 20.824634985129038\n",
      "Epoch: 5, Train Loss: 3.37566666, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e5_2023_05_19_01_15_44.pth\n",
      "Current LR = 0.00001498\n",
      "epoch = 6batch = 0 of 2258duraation = 0.009683704376220703\n",
      "epoch = 6batch = 1000 of 2258duraation = 10.413891422748566\n",
      "epoch = 6batch = 2000 of 2258duraation = 20.816362774372102\n",
      "Epoch: 6, Train Loss: 3.36462321, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e6_2023_05_19_01_39_14.pth\n",
      "Current LR = 0.00001497\n",
      "epoch = 7batch = 0 of 2258duraation = 0.00852655569712321\n",
      "epoch = 7batch = 1000 of 2258duraation = 10.409512281417847\n",
      "epoch = 7batch = 2000 of 2258duraation = 20.805996588865916\n",
      "Epoch: 7, Train Loss: 3.36322932, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e7_2023_05_19_02_02_44.pth\n",
      "Current LR = 0.00001496\n",
      "epoch = 8batch = 0 of 2258duraation = 0.009582138061523438\n",
      "epoch = 8batch = 1000 of 2258duraation = 10.410549084345499\n",
      "epoch = 8batch = 2000 of 2258duraation = 20.811459962526957\n",
      "Epoch: 8, Train Loss: 3.35518993, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e8_2023_05_19_02_26_14.pth\n",
      "Current LR = 0.00001494\n",
      "epoch = 9batch = 0 of 2258duraation = 0.009375603993733723\n",
      "epoch = 9batch = 1000 of 2258duraation = 10.408526726563771\n",
      "epoch = 9batch = 2000 of 2258duraation = 20.807097963492076\n",
      "Epoch: 9, Train Loss: 3.35019965, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e9_2023_05_19_02_49_44.pth\n",
      "Current LR = 0.00001493\n",
      "epoch = 10batch = 0 of 2258duraation = 0.008578960100809734\n",
      "epoch = 10batch = 1000 of 2258duraation = 10.406329425175985\n",
      "epoch = 10batch = 2000 of 2258duraation = 20.804684007167815\n",
      "Epoch: 10, Train Loss: 3.34202112, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e10_2023_05_19_03_13_14.pth\n",
      "Current LR = 0.00001491\n",
      "epoch = 11batch = 0 of 2258duraation = 0.007423965136210123\n",
      "epoch = 11batch = 1000 of 2258duraation = 10.40407231648763\n",
      "epoch = 11batch = 2000 of 2258duraation = 20.800951155026755\n",
      "..Overrun....no improvement\n",
      "Epoch: 11, Train Loss: 3.34516374, overrun_counter 0\n",
      "epoch = 12batch = 0 of 2258duraation = 0.009264194965362548\n",
      "epoch = 12batch = 1000 of 2258duraation = 10.405926994482677\n",
      "epoch = 12batch = 2000 of 2258duraation = 20.800198729832967\n",
      "Epoch: 12, Train Loss: 3.33713538, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e12_2023_05_19_04_00_12.pth\n",
      "Current LR = 0.00001487\n",
      "epoch = 13batch = 0 of 2258duraation = 0.009891494115193685\n",
      "epoch = 13batch = 1000 of 2258duraation = 10.403589149316153\n",
      "epoch = 13batch = 2000 of 2258duraation = 20.799076235294343\n",
      "Epoch: 13, Train Loss: 3.33209969, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e13_2023_05_19_04_23_41.pth\n",
      "Current LR = 0.00001485\n",
      "epoch = 14batch = 0 of 2258duraation = 0.008567269643147786\n",
      "epoch = 14batch = 1000 of 2258duraation = 10.406290837128957\n",
      "epoch = 14batch = 2000 of 2258duraation = 20.80442318916321\n",
      "Epoch: 14, Train Loss: 3.32984751, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e14_2023_05_19_04_47_11.pth\n",
      "Current LR = 0.00001482\n",
      "epoch = 15batch = 0 of 2258duraation = 0.0100037415822347\n",
      "epoch = 15batch = 1000 of 2258duraation = 10.405255583922068\n",
      "epoch = 15batch = 2000 of 2258duraation = 20.800691616535186\n",
      "Epoch: 15, Train Loss: 3.32804258, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e15_2023_05_19_05_10_40.pth\n",
      "Current LR = 0.00001479\n",
      "epoch = 16batch = 0 of 2258duraation = 0.009038440386454264\n",
      "epoch = 16batch = 1000 of 2258duraation = 10.405460917949677\n",
      "epoch = 16batch = 2000 of 2258duraation = 20.803013678391775\n",
      "..Overrun....no improvement\n",
      "Epoch: 16, Train Loss: 3.32975868, overrun_counter 0\n",
      "epoch = 17batch = 0 of 2258duraation = 0.008963962395985922\n",
      "epoch = 17batch = 1000 of 2258duraation = 10.406898013750713\n",
      "epoch = 17batch = 2000 of 2258duraation = 20.800886623064677\n",
      "Epoch: 17, Train Loss: 3.32488659, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e17_2023_05_19_05_57_39.pth\n",
      "Current LR = 0.00001474\n",
      "epoch = 18batch = 0 of 2258duraation = 0.010780437787373861\n",
      "epoch = 18batch = 1000 of 2258duraation = 10.40761212905248\n",
      "epoch = 18batch = 2000 of 2258duraation = 20.800805695851643\n",
      "Epoch: 18, Train Loss: 3.31533454, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e18_2023_05_19_06_21_08.pth\n",
      "Current LR = 0.00001471\n",
      "epoch = 19batch = 0 of 2258duraation = 0.009321037928263347\n",
      "epoch = 19batch = 1000 of 2258duraation = 10.40710899035136\n",
      "epoch = 19batch = 2000 of 2258duraation = 20.8045299132665\n",
      "Epoch: 19, Train Loss: 3.31303385, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e19_2023_05_19_06_44_38.pth\n",
      "Current LR = 0.00001467\n",
      "epoch = 20batch = 0 of 2258duraation = 0.0074590484301249186\n",
      "epoch = 20batch = 1000 of 2258duraation = 10.405795280138651\n",
      "epoch = 20batch = 2000 of 2258duraation = 20.801066092650096\n",
      "Epoch: 20, Train Loss: 3.31021999, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e20_2023_05_19_07_08_07.pth\n",
      "Current LR = 0.00001464\n",
      "epoch = 21batch = 0 of 2258duraation = 0.009482308228810628\n",
      "epoch = 21batch = 1000 of 2258duraation = 10.405210343996684\n",
      "epoch = 21batch = 2000 of 2258duraation = 20.802360570430757\n",
      "Epoch: 21, Train Loss: 3.30731329, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e21_2023_05_19_07_31_37.pth\n",
      "Current LR = 0.00001460\n",
      "epoch = 22batch = 0 of 2258duraation = 0.007931935787200927\n",
      "epoch = 22batch = 1000 of 2258duraation = 10.40105237166087\n",
      "epoch = 22batch = 2000 of 2258duraation = 20.79683357079824\n",
      "..Overrun....no improvement\n",
      "Epoch: 22, Train Loss: 3.31454420, overrun_counter 0\n",
      "epoch = 23batch = 0 of 2258duraation = 0.008895337581634521\n",
      "epoch = 23batch = 1000 of 2258duraation = 10.40715250968933\n",
      "epoch = 23batch = 2000 of 2258duraation = 20.80258947213491\n",
      "Epoch: 23, Train Loss: 3.30607118, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e23_2023_05_19_08_18_35.pth\n",
      "Current LR = 0.00001452\n",
      "epoch = 24batch = 0 of 2258duraation = 0.00850455363591512\n",
      "epoch = 24batch = 1000 of 2258duraation = 10.40382372935613\n",
      "epoch = 24batch = 2000 of 2258duraation = 20.796617885430653\n",
      "Epoch: 24, Train Loss: 3.30114323, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e24_2023_05_19_08_42_04.pth\n",
      "Current LR = 0.00001448\n",
      "epoch = 25batch = 0 of 2258duraation = 0.007736110687255859\n",
      "epoch = 25batch = 1000 of 2258duraation = 10.400165335337322\n",
      "epoch = 25batch = 2000 of 2258duraation = 20.792662342389423\n",
      "..Overrun....no improvement\n",
      "Epoch: 25, Train Loss: 3.30421658, overrun_counter 0\n",
      "epoch = 26batch = 0 of 2258duraation = 0.009867656230926513\n",
      "epoch = 26batch = 1000 of 2258duraation = 10.401545675595601\n",
      "epoch = 26batch = 2000 of 2258duraation = 20.79515623251597\n",
      "Epoch: 26, Train Loss: 3.29364247, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e26_2023_05_19_09_29_01.pth\n",
      "Current LR = 0.00001439\n",
      "epoch = 27batch = 0 of 2258duraation = 0.010003205140431721\n",
      "epoch = 27batch = 1000 of 2258duraation = 10.40389518737793\n",
      "epoch = 27batch = 2000 of 2258duraation = 20.798450716336568\n",
      "..Overrun....no improvement\n",
      "Epoch: 27, Train Loss: 3.29759718, overrun_counter 0\n",
      "epoch = 28batch = 0 of 2258duraation = 0.008672714233398438\n",
      "epoch = 28batch = 1000 of 2258duraation = 10.398880751927694\n",
      "epoch = 28batch = 2000 of 2258duraation = 20.79215016365051\n",
      "..Overrun....no improvement\n",
      "Epoch: 28, Train Loss: 3.29761198, overrun_counter 1\n",
      "epoch = 29batch = 0 of 2258duraation = 0.01037211020787557\n",
      "epoch = 29batch = 1000 of 2258duraation = 10.403981578350066\n",
      "epoch = 29batch = 2000 of 2258duraation = 20.796552220980328\n",
      "Epoch: 29, Train Loss: 3.28842049, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e29_2023_05_19_10_39_27.pth\n",
      "Current LR = 0.00001424\n",
      "epoch = 30batch = 0 of 2258duraation = 0.008378100395202637\n",
      "epoch = 30batch = 1000 of 2258duraation = 10.401570014158885\n",
      "epoch = 30batch = 2000 of 2258duraation = 20.795585481325784\n",
      "..Overrun....no improvement\n",
      "Epoch: 30, Train Loss: 3.28883574, overrun_counter 0\n",
      "epoch = 31batch = 0 of 2258duraation = 0.009362510840098063\n",
      "epoch = 31batch = 1000 of 2258duraation = 10.404094858964283\n",
      "epoch = 31batch = 2000 of 2258duraation = 20.79986548423767\n",
      "Epoch: 31, Train Loss: 3.28266923, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e31_2023_05_19_11_26_25.pth\n",
      "Current LR = 0.00001414\n",
      "epoch = 32batch = 0 of 2258duraation = 0.00862342913945516\n",
      "epoch = 32batch = 1000 of 2258duraation = 10.407423452536266\n",
      "epoch = 32batch = 2000 of 2258duraation = 20.804471770922344\n",
      "Epoch: 32, Train Loss: 3.27772901, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e32_2023_05_19_11_49_54.pth\n",
      "Current LR = 0.00001408\n",
      "epoch = 33batch = 0 of 2258duraation = 0.010621094703674316\n",
      "epoch = 33batch = 1000 of 2258duraation = 10.407975514729818\n",
      "epoch = 33batch = 2000 of 2258duraation = 20.80267363389333\n",
      "Epoch: 33, Train Loss: 3.27521388, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e33_2023_05_19_12_13_24.pth\n",
      "Current LR = 0.00001402\n",
      "epoch = 34batch = 0 of 2258duraation = 0.009773552417755127\n",
      "epoch = 34batch = 1000 of 2258duraation = 10.406895498434702\n",
      "epoch = 34batch = 2000 of 2258duraation = 20.80424726009369\n",
      "..Overrun....no improvement\n",
      "Epoch: 34, Train Loss: 3.27626523, overrun_counter 0\n",
      "epoch = 35batch = 0 of 2258duraation = 0.009322412808736165\n",
      "epoch = 35batch = 1000 of 2258duraation = 10.405995186169942\n",
      "epoch = 35batch = 2000 of 2258duraation = 20.800207821528115\n",
      "..Overrun....no improvement\n",
      "Epoch: 35, Train Loss: 3.27936214, overrun_counter 1\n",
      "epoch = 36batch = 0 of 2258duraation = 0.009533361593882243\n",
      "epoch = 36batch = 1000 of 2258duraation = 10.404518099625905\n",
      "epoch = 36batch = 2000 of 2258duraation = 20.79945650100708\n",
      "Epoch: 36, Train Loss: 3.26665109, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e36_2023_05_19_13_23_51.pth\n",
      "Current LR = 0.00001384\n",
      "epoch = 37batch = 0 of 2258duraation = 0.008351778984069825\n",
      "epoch = 37batch = 1000 of 2258duraation = 10.402375257015228\n",
      "epoch = 37batch = 2000 of 2258duraation = 20.792837862173716\n",
      "..Overrun....no improvement\n",
      "Epoch: 37, Train Loss: 3.26892497, overrun_counter 0\n",
      "epoch = 38batch = 0 of 2258duraation = 0.008707610766092937\n",
      "epoch = 38batch = 1000 of 2258duraation = 10.401750870545705\n",
      "epoch = 38batch = 2000 of 2258duraation = 20.79393977721532\n",
      "Epoch: 38, Train Loss: 3.26529706, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e38_2023_05_19_14_10_48.pth\n",
      "Current LR = 0.00001372\n",
      "epoch = 39batch = 0 of 2258duraation = 0.010416404406229655\n",
      "epoch = 39batch = 1000 of 2258duraation = 10.40261577765147\n",
      "epoch = 39batch = 2000 of 2258duraation = 20.79607877333959\n",
      "Epoch: 39, Train Loss: 3.26455692, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e39_2023_05_19_14_34_17.pth\n",
      "Current LR = 0.00001365\n",
      "epoch = 40batch = 0 of 2258duraation = 0.00890664259592692\n",
      "epoch = 40batch = 1000 of 2258duraation = 10.402843133608501\n",
      "epoch = 40batch = 2000 of 2258duraation = 20.79580521186193\n",
      "Epoch: 40, Train Loss: 3.26152677, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e40_2023_05_19_14_57_46.pth\n",
      "Current LR = 0.00001358\n",
      "epoch = 41batch = 0 of 2258duraation = 0.009342912832895916\n",
      "epoch = 41batch = 1000 of 2258duraation = 10.402626585960387\n",
      "epoch = 41batch = 2000 of 2258duraation = 20.798295040925343\n",
      "Epoch: 41, Train Loss: 3.25405424, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e41_2023_05_19_15_21_15.pth\n",
      "Current LR = 0.00001351\n",
      "epoch = 42batch = 0 of 2258duraation = 0.007384904225667318\n",
      "epoch = 42batch = 1000 of 2258duraation = 10.402317003409069\n",
      "epoch = 42batch = 2000 of 2258duraation = 20.79817111492157\n",
      "..Overrun....no improvement\n",
      "Epoch: 42, Train Loss: 3.26498751, overrun_counter 0\n",
      "epoch = 43batch = 0 of 2258duraation = 0.008155508836110433\n",
      "epoch = 43batch = 1000 of 2258duraation = 10.404411852359772\n",
      "epoch = 43batch = 2000 of 2258duraation = 20.797100428740183\n",
      "..Overrun....no improvement\n",
      "Epoch: 43, Train Loss: 3.25891191, overrun_counter 1\n",
      "epoch = 44batch = 0 of 2258duraation = 0.00924601952234904\n",
      "epoch = 44batch = 1000 of 2258duraation = 10.403727213541666\n",
      "epoch = 44batch = 2000 of 2258duraation = 20.79899738629659\n",
      "..Overrun....no improvement\n",
      "Epoch: 44, Train Loss: 3.25463814, overrun_counter 2\n",
      "epoch = 45batch = 0 of 2258duraation = 0.007580268383026123\n",
      "epoch = 45batch = 1000 of 2258duraation = 10.404288828372955\n",
      "epoch = 45batch = 2000 of 2258duraation = 20.797517999013266\n",
      "Epoch: 45, Train Loss: 3.24018783, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e45_2023_05_19_16_55_10.pth\n",
      "Current LR = 0.00001322\n",
      "epoch = 46batch = 0 of 2258duraation = 0.008706104755401612\n",
      "epoch = 46batch = 1000 of 2258duraation = 10.401900525887807\n",
      "epoch = 46batch = 2000 of 2258duraation = 20.794723097483317\n",
      "..Overrun....no improvement\n",
      "Epoch: 46, Train Loss: 3.24829882, overrun_counter 0\n",
      "epoch = 47batch = 0 of 2258duraation = 0.008177932103474934\n",
      "epoch = 47batch = 1000 of 2258duraation = 10.402273042996724\n",
      "epoch = 47batch = 2000 of 2258duraation = 20.79619103272756\n",
      "..Overrun....no improvement\n",
      "Epoch: 47, Train Loss: 3.25023683, overrun_counter 1\n",
      "epoch = 48batch = 0 of 2258duraation = 0.0079970121383667\n",
      "epoch = 48batch = 1000 of 2258duraation = 10.401382478078206\n",
      "epoch = 48batch = 2000 of 2258duraation = 20.79655339717865\n",
      "Epoch: 48, Train Loss: 3.23761685, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e48_2023_05_19_18_05_36.pth\n",
      "Current LR = 0.00001299\n",
      "epoch = 49batch = 0 of 2258duraation = 0.009127875169118246\n",
      "epoch = 49batch = 1000 of 2258duraation = 10.405783967177074\n",
      "epoch = 49batch = 2000 of 2258duraation = 20.801660390694938\n",
      "..Overrun....no improvement\n",
      "Epoch: 49, Train Loss: 3.24374681, overrun_counter 0\n",
      "epoch = 50batch = 0 of 2258duraation = 0.008250665664672852\n",
      "epoch = 50batch = 1000 of 2258duraation = 10.40421677827835\n",
      "epoch = 50batch = 2000 of 2258duraation = 20.79792054891586\n",
      "..Overrun....no improvement\n",
      "Epoch: 50, Train Loss: 3.23770360, overrun_counter 1\n",
      "epoch = 51batch = 0 of 2258duraation = 0.010348014036814372\n",
      "epoch = 51batch = 1000 of 2258duraation = 10.403879495461782\n",
      "epoch = 51batch = 2000 of 2258duraation = 20.79836104710897\n",
      "..Overrun....no improvement\n",
      "Epoch: 51, Train Loss: 3.24090816, overrun_counter 2\n",
      "epoch = 52batch = 0 of 2258duraation = 0.008653255303700765\n",
      "epoch = 52batch = 1000 of 2258duraation = 10.403786230087281\n",
      "epoch = 52batch = 2000 of 2258duraation = 20.797467132409412\n",
      "Epoch: 52, Train Loss: 3.23239014, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e52_2023_05_19_19_39_32.pth\n",
      "Current LR = 0.00001266\n",
      "epoch = 53batch = 0 of 2258duraation = 0.00908597707748413\n",
      "epoch = 53batch = 1000 of 2258duraation = 10.401771831512452\n",
      "epoch = 53batch = 2000 of 2258duraation = 20.795911780993144\n",
      "..Overrun....no improvement\n",
      "Epoch: 53, Train Loss: 3.24006610, overrun_counter 0\n",
      "epoch = 54batch = 0 of 2258duraation = 0.009301368395487468\n",
      "epoch = 54batch = 1000 of 2258duraation = 10.404424512386322\n",
      "epoch = 54batch = 2000 of 2258duraation = 20.797992185751596\n",
      "Epoch: 54, Train Loss: 3.22863858, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e54_2023_05_19_20_26_29.pth\n",
      "Current LR = 0.00001249\n",
      "epoch = 55batch = 0 of 2258duraation = 0.008294375737508138\n",
      "epoch = 55batch = 1000 of 2258duraation = 10.40332107146581\n",
      "epoch = 55batch = 2000 of 2258duraation = 20.79677441517512\n",
      "..Overrun....no improvement\n",
      "Epoch: 55, Train Loss: 3.23093145, overrun_counter 0\n",
      "epoch = 56batch = 0 of 2258duraation = 0.008583549658457439\n",
      "epoch = 56batch = 1000 of 2258duraation = 10.403687187035878\n",
      "epoch = 56batch = 2000 of 2258duraation = 20.79963603814443\n",
      "Epoch: 56, Train Loss: 3.22439890, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e56_2023_05_19_21_13_27.pth\n",
      "Current LR = 0.00001231\n",
      "epoch = 57batch = 0 of 2258duraation = 0.009462042649586996\n",
      "epoch = 57batch = 1000 of 2258duraation = 10.404787528514863\n",
      "epoch = 57batch = 2000 of 2258duraation = 20.799828640619914\n",
      "Epoch: 57, Train Loss: 3.22277577, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e57_2023_05_19_21_36_57.pth\n",
      "Current LR = 0.00001222\n",
      "epoch = 58batch = 0 of 2258duraation = 0.009872941176096599\n",
      "epoch = 58batch = 1000 of 2258duraation = 10.401748156547546\n",
      "epoch = 58batch = 2000 of 2258duraation = 20.796064889431\n",
      "Epoch: 58, Train Loss: 3.21940271, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e58_2023_05_19_22_00_26.pth\n",
      "Current LR = 0.00001213\n",
      "epoch = 59batch = 0 of 2258duraation = 0.008970125516255697\n",
      "epoch = 59batch = 1000 of 2258duraation = 10.401551524798075\n",
      "epoch = 59batch = 2000 of 2258duraation = 20.791962643464405\n",
      "Epoch: 59, Train Loss: 3.21522968, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e59_2023_05_19_22_23_54.pth\n",
      "Current LR = 0.00001203\n",
      "epoch = 60batch = 0 of 2258duraation = 0.00921479066212972\n",
      "epoch = 60batch = 1000 of 2258duraation = 10.40077288945516\n",
      "epoch = 60batch = 2000 of 2258duraation = 20.792776783307392\n",
      "Epoch: 60, Train Loss: 3.21263985, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e60_2023_05_19_22_47_23.pth\n",
      "Current LR = 0.00001194\n",
      "epoch = 61batch = 0 of 2258duraation = 0.00831592877705892\n",
      "epoch = 61batch = 1000 of 2258duraation = 10.401990878582001\n",
      "epoch = 61batch = 2000 of 2258duraation = 20.795981431007384\n",
      "Epoch: 61, Train Loss: 3.20483535, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e61_2023_05_19_23_10_52.pth\n",
      "Current LR = 0.00001184\n",
      "epoch = 62batch = 0 of 2258duraation = 0.008295170466105143\n",
      "epoch = 62batch = 1000 of 2258duraation = 10.403319958845774\n",
      "epoch = 62batch = 2000 of 2258duraation = 20.79381191333135\n",
      "Epoch: 62, Train Loss: 3.20448628, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e62_2023_05_19_23_34_21.pth\n",
      "Current LR = 0.00001175\n",
      "epoch = 63batch = 0 of 2258duraation = 0.007883421579996745\n",
      "epoch = 63batch = 1000 of 2258duraation = 10.403769151369731\n",
      "epoch = 63batch = 2000 of 2258duraation = 20.797993576526643\n",
      "..Overrun....no improvement\n",
      "Epoch: 63, Train Loss: 3.20909066, overrun_counter 0\n",
      "epoch = 64batch = 0 of 2258duraation = 0.008518826961517335\n",
      "epoch = 64batch = 1000 of 2258duraation = 10.403878104686736\n",
      "epoch = 64batch = 2000 of 2258duraation = 20.796424643198648\n",
      "..Overrun....no improvement\n",
      "Epoch: 64, Train Loss: 3.20636422, overrun_counter 1\n",
      "epoch = 65batch = 0 of 2258duraation = 0.007989776134490967\n",
      "epoch = 65batch = 1000 of 2258duraation = 10.403212428092957\n",
      "epoch = 65batch = 2000 of 2258duraation = 20.796691914399464\n",
      "Epoch: 65, Train Loss: 3.20122277, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e65_2023_05_20_00_44_47.pth\n",
      "Current LR = 0.00001145\n",
      "epoch = 66batch = 0 of 2258duraation = 0.010495245456695557\n",
      "epoch = 66batch = 1000 of 2258duraation = 10.402392601966858\n",
      "epoch = 66batch = 2000 of 2258duraation = 20.794074098269146\n",
      "..Overrun....no improvement\n",
      "Epoch: 66, Train Loss: 3.20569503, overrun_counter 0\n",
      "epoch = 67batch = 0 of 2258duraation = 0.009506519635518391\n",
      "epoch = 67batch = 1000 of 2258duraation = 10.40132429599762\n",
      "epoch = 67batch = 2000 of 2258duraation = 20.792169761657714\n",
      "Epoch: 67, Train Loss: 3.19274599, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e67_2023_05_20_01_31_44.pth\n",
      "Current LR = 0.00001125\n",
      "epoch = 68batch = 0 of 2258duraation = 0.008652098973592122\n",
      "epoch = 68batch = 1000 of 2258duraation = 10.401498730977377\n",
      "epoch = 68batch = 2000 of 2258duraation = 20.797440489133198\n",
      "..Overrun....no improvement\n",
      "Epoch: 68, Train Loss: 3.19342453, overrun_counter 0\n",
      "epoch = 69batch = 0 of 2258duraation = 0.009229262669881185\n",
      "epoch = 69batch = 1000 of 2258duraation = 10.405356887976328\n",
      "epoch = 69batch = 2000 of 2258duraation = 20.798952770233154\n",
      "..Overrun....no improvement\n",
      "Epoch: 69, Train Loss: 3.19657044, overrun_counter 1\n",
      "epoch = 70batch = 0 of 2258duraation = 0.008046575387318929\n",
      "epoch = 70batch = 1000 of 2258duraation = 10.400087996323903\n",
      "epoch = 70batch = 2000 of 2258duraation = 20.795386918385823\n",
      "..Overrun....no improvement\n",
      "Epoch: 70, Train Loss: 3.19351301, overrun_counter 2\n",
      "epoch = 71batch = 0 of 2258duraation = 0.009296985467274983\n",
      "epoch = 71batch = 1000 of 2258duraation = 10.404821340243021\n",
      "epoch = 71batch = 2000 of 2258duraation = 20.80077784061432\n",
      "Epoch: 71, Train Loss: 3.18252086, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e71_2023_05_20_03_05_39.pth\n",
      "Current LR = 0.00001084\n",
      "epoch = 72batch = 0 of 2258duraation = 0.0086090087890625\n",
      "epoch = 72batch = 1000 of 2258duraation = 10.401903943220775\n",
      "epoch = 72batch = 2000 of 2258duraation = 20.791629032293955\n",
      "Epoch: 72, Train Loss: 3.17213370, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e72_2023_05_20_03_29_08.pth\n",
      "Current LR = 0.00001074\n",
      "epoch = 73batch = 0 of 2258duraation = 0.007296208540598551\n",
      "epoch = 73batch = 1000 of 2258duraation = 10.39898130496343\n",
      "epoch = 73batch = 2000 of 2258duraation = 20.79129331111908\n",
      "..Overrun....no improvement\n",
      "Epoch: 73, Train Loss: 3.18372962, overrun_counter 0\n",
      "epoch = 74batch = 0 of 2258duraation = 0.008652019500732421\n",
      "epoch = 74batch = 1000 of 2258duraation = 10.402584858735402\n",
      "epoch = 74batch = 2000 of 2258duraation = 20.797323993841808\n",
      "..Overrun....no improvement\n",
      "Epoch: 74, Train Loss: 3.17628622, overrun_counter 1\n",
      "epoch = 75batch = 0 of 2258duraation = 0.010302293300628661\n",
      "epoch = 75batch = 1000 of 2258duraation = 10.408191402753195\n",
      "epoch = 75batch = 2000 of 2258duraation = 20.80491771697998\n",
      "Epoch: 75, Train Loss: 3.17095873, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e75_2023_05_20_04_39_34.pth\n",
      "Current LR = 0.00001042\n",
      "epoch = 76batch = 0 of 2258duraation = 0.007893021901448567\n",
      "epoch = 76batch = 1000 of 2258duraation = 10.403997286160786\n",
      "epoch = 76batch = 2000 of 2258duraation = 20.801534628868104\n",
      "..Overrun....no improvement\n",
      "Epoch: 76, Train Loss: 3.17284849, overrun_counter 0\n",
      "epoch = 77batch = 0 of 2258duraation = 0.010743006070454916\n",
      "epoch = 77batch = 1000 of 2258duraation = 10.408547755082449\n",
      "epoch = 77batch = 2000 of 2258duraation = 20.804697775840758\n",
      "Epoch: 77, Train Loss: 3.15922421, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e77_2023_05_20_05_26_33.pth\n",
      "Current LR = 0.00001020\n",
      "epoch = 78batch = 0 of 2258duraation = 0.00905006726582845\n",
      "epoch = 78batch = 1000 of 2258duraation = 10.40506196419398\n",
      "epoch = 78batch = 2000 of 2258duraation = 20.799115570386252\n",
      "..Overrun....no improvement\n",
      "Epoch: 78, Train Loss: 3.17767653, overrun_counter 0\n",
      "epoch = 79batch = 0 of 2258duraation = 0.009269948800404866\n",
      "epoch = 79batch = 1000 of 2258duraation = 10.406556403636932\n",
      "epoch = 79batch = 2000 of 2258duraation = 20.802191547552745\n",
      "..Overrun....no improvement\n",
      "Epoch: 79, Train Loss: 3.18219254, overrun_counter 1\n",
      "epoch = 80batch = 0 of 2258duraation = 0.0078078826268514\n",
      "epoch = 80batch = 1000 of 2258duraation = 10.408687158425648\n",
      "epoch = 80batch = 2000 of 2258duraation = 20.804227968056995\n",
      "..Overrun....no improvement\n",
      "Epoch: 80, Train Loss: 3.16392610, overrun_counter 2\n",
      "epoch = 81batch = 0 of 2258duraation = 0.01056233247121175\n",
      "epoch = 81batch = 1000 of 2258duraation = 10.406790133317312\n",
      "epoch = 81batch = 2000 of 2258duraation = 20.804068303108217\n",
      "Epoch: 81, Train Loss: 3.15564298, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e81_2023_05_20_07_00_29.pth\n",
      "Current LR = 0.00000976\n",
      "epoch = 82batch = 0 of 2258duraation = 0.008729477723439535\n",
      "epoch = 82batch = 1000 of 2258duraation = 10.402023458480835\n",
      "epoch = 82batch = 2000 of 2258duraation = 20.792408641179403\n",
      "..Overrun....no improvement\n",
      "Epoch: 82, Train Loss: 3.17467896, overrun_counter 0\n",
      "epoch = 83batch = 0 of 2258duraation = 0.008790679772694905\n",
      "epoch = 83batch = 1000 of 2258duraation = 10.401960968971252\n",
      "epoch = 83batch = 2000 of 2258duraation = 20.79152843554815\n",
      "Epoch: 83, Train Loss: 3.15326181, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e83_2023_05_20_07_47_27.pth\n",
      "Current LR = 0.00000953\n",
      "epoch = 84batch = 0 of 2258duraation = 0.007311511039733887\n",
      "epoch = 84batch = 1000 of 2258duraation = 10.404753422737121\n",
      "epoch = 84batch = 2000 of 2258duraation = 20.80247937043508\n",
      "Epoch: 84, Train Loss: 3.14669067, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e84_2023_05_20_08_10_56.pth\n",
      "Current LR = 0.00000942\n",
      "epoch = 85batch = 0 of 2258duraation = 0.009151589870452882\n",
      "epoch = 85batch = 1000 of 2258duraation = 10.408317685127258\n",
      "epoch = 85batch = 2000 of 2258duraation = 20.805627449353537\n",
      "..Overrun....no improvement\n",
      "Epoch: 85, Train Loss: 3.15080032, overrun_counter 0\n",
      "epoch = 86batch = 0 of 2258duraation = 0.009254443645477294\n",
      "epoch = 86batch = 1000 of 2258duraation = 10.406400163968405\n",
      "epoch = 86batch = 2000 of 2258duraation = 20.8047633488973\n",
      "Epoch: 86, Train Loss: 3.14206295, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e86_2023_05_20_08_57_55.pth\n",
      "Current LR = 0.00000919\n",
      "epoch = 87batch = 0 of 2258duraation = 0.00972286860148112\n",
      "epoch = 87batch = 1000 of 2258duraation = 10.408330313364665\n",
      "epoch = 87batch = 2000 of 2258duraation = 20.8050843556722\n",
      "..Overrun....no improvement\n",
      "Epoch: 87, Train Loss: 3.14710167, overrun_counter 0\n",
      "epoch = 88batch = 0 of 2258duraation = 0.007267498970031738\n",
      "epoch = 88batch = 1000 of 2258duraation = 10.403105628490447\n",
      "epoch = 88batch = 2000 of 2258duraation = 20.79932781457901\n",
      "..Overrun....no improvement\n",
      "Epoch: 88, Train Loss: 3.15022875, overrun_counter 1\n",
      "epoch = 89batch = 0 of 2258duraation = 0.009305608272552491\n",
      "epoch = 89batch = 1000 of 2258duraation = 10.404897741476695\n",
      "epoch = 89batch = 2000 of 2258duraation = 20.801354622840883\n",
      "Epoch: 89, Train Loss: 3.13896461, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e89_2023_05_20_10_08_22.pth\n",
      "Current LR = 0.00000885\n",
      "epoch = 90batch = 0 of 2258duraation = 0.008172500133514404\n",
      "epoch = 90batch = 1000 of 2258duraation = 10.406419682502747\n",
      "epoch = 90batch = 2000 of 2258duraation = 20.805161833763123\n",
      "..Overrun....no improvement\n",
      "Epoch: 90, Train Loss: 3.14316529, overrun_counter 0\n",
      "epoch = 91batch = 0 of 2258duraation = 0.00961753527323405\n",
      "epoch = 91batch = 1000 of 2258duraation = 10.409378910064698\n",
      "epoch = 91batch = 2000 of 2258duraation = 20.809119590123494\n",
      "..Overrun....no improvement\n",
      "Epoch: 91, Train Loss: 3.14397147, overrun_counter 1\n",
      "epoch = 92batch = 0 of 2258duraation = 0.01064067284266154\n",
      "epoch = 92batch = 1000 of 2258duraation = 10.411088037490845\n",
      "epoch = 92batch = 2000 of 2258duraation = 20.809066526095073\n",
      "..Overrun....no improvement\n",
      "Epoch: 92, Train Loss: 3.14473428, overrun_counter 2\n",
      "epoch = 93batch = 0 of 2258duraation = 0.010386852423350017\n",
      "epoch = 93batch = 1000 of 2258duraation = 10.41197802623113\n",
      "epoch = 93batch = 2000 of 2258duraation = 20.80692612330119\n",
      "Epoch: 93, Train Loss: 3.13107485, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e93_2023_05_20_11_42_21.pth\n",
      "Current LR = 0.00000839\n",
      "epoch = 94batch = 0 of 2258duraation = 0.007561747233072917\n",
      "epoch = 94batch = 1000 of 2258duraation = 10.406364138921102\n",
      "epoch = 94batch = 2000 of 2258duraation = 20.80304601987203\n",
      "..Overrun....no improvement\n",
      "Epoch: 94, Train Loss: 3.13126373, overrun_counter 0\n",
      "epoch = 95batch = 0 of 2258duraation = 0.009648585319519043\n",
      "epoch = 95batch = 1000 of 2258duraation = 10.40698964993159\n",
      "epoch = 95batch = 2000 of 2258duraation = 20.805836431185405\n",
      "Epoch: 95, Train Loss: 3.12833303, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e95_2023_05_20_12_29_19.pth\n",
      "Current LR = 0.00000816\n",
      "epoch = 96batch = 0 of 2258duraation = 0.009499784310658772\n",
      "epoch = 96batch = 1000 of 2258duraation = 10.411034679412841\n",
      "epoch = 96batch = 2000 of 2258duraation = 20.81153992811839\n",
      "Epoch: 96, Train Loss: 3.11964305, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e96_2023_05_20_12_52_50.pth\n",
      "Current LR = 0.00000804\n",
      "epoch = 97batch = 0 of 2258duraation = 0.008962464332580567\n",
      "epoch = 97batch = 1000 of 2258duraation = 10.405946878592173\n",
      "epoch = 97batch = 2000 of 2258duraation = 20.806123379866282\n",
      "..Overrun....no improvement\n",
      "Epoch: 97, Train Loss: 3.14135237, overrun_counter 0\n",
      "epoch = 98batch = 0 of 2258duraation = 0.0087920347849528\n",
      "epoch = 98batch = 1000 of 2258duraation = 10.409788080056508\n",
      "epoch = 98batch = 2000 of 2258duraation = 20.810171631971993\n",
      "Epoch: 98, Train Loss: 3.11946443, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e98_2023_05_20_13_39_49.pth\n",
      "Current LR = 0.00000781\n",
      "epoch = 99batch = 0 of 2258duraation = 0.008782235781351726\n",
      "epoch = 99batch = 1000 of 2258duraation = 10.40957959095637\n",
      "epoch = 99batch = 2000 of 2258duraation = 20.80974061091741\n",
      "Epoch: 99, Train Loss: 3.11832240, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e99_2023_05_20_14_03_19.pth\n",
      "Current LR = 0.00000769\n",
      "epoch = 100batch = 0 of 2258duraation = 0.009743777910868327\n",
      "epoch = 100batch = 1000 of 2258duraation = 10.41043131351471\n",
      "epoch = 100batch = 2000 of 2258duraation = 20.808209323883055\n",
      "..Overrun....no improvement\n",
      "Epoch: 100, Train Loss: 3.13192187, overrun_counter 0\n",
      "epoch = 101batch = 0 of 2258duraation = 0.009141409397125244\n",
      "epoch = 101batch = 1000 of 2258duraation = 10.411055858929952\n",
      "epoch = 101batch = 2000 of 2258duraation = 20.810407058397928\n",
      "Epoch: 101, Train Loss: 3.11408730, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e101_2023_05_20_14_50_18.pth\n",
      "Current LR = 0.00000746\n",
      "epoch = 102batch = 0 of 2258duraation = 0.010118810335795085\n",
      "epoch = 102batch = 1000 of 2258duraation = 10.41000653107961\n",
      "epoch = 102batch = 2000 of 2258duraation = 20.81098895072937\n",
      "..Overrun....no improvement\n",
      "Epoch: 102, Train Loss: 3.11693308, overrun_counter 0\n",
      "epoch = 103batch = 0 of 2258duraation = 0.010085880756378174\n",
      "epoch = 103batch = 1000 of 2258duraation = 10.410735730330149\n",
      "epoch = 103batch = 2000 of 2258duraation = 20.81197861035665\n",
      "..Overrun....no improvement\n",
      "Epoch: 103, Train Loss: 3.12406934, overrun_counter 1\n",
      "epoch = 104batch = 0 of 2258duraation = 0.009503014882405599\n",
      "epoch = 104batch = 1000 of 2258duraation = 10.408825266361237\n",
      "epoch = 104batch = 2000 of 2258duraation = 20.81061780055364\n",
      "Epoch: 104, Train Loss: 3.11388484, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e104_2023_05_20_16_00_48.pth\n",
      "Current LR = 0.00000711\n",
      "epoch = 105batch = 0 of 2258duraation = 0.009040276209513346\n",
      "epoch = 105batch = 1000 of 2258duraation = 10.40907353957494\n",
      "epoch = 105batch = 2000 of 2258duraation = 20.808740039666493\n",
      "Epoch: 105, Train Loss: 3.09827308, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e105_2023_05_20_16_24_18.pth\n",
      "Current LR = 0.00000699\n",
      "epoch = 106batch = 0 of 2258duraation = 0.009249484539031983\n",
      "epoch = 106batch = 1000 of 2258duraation = 10.412327734629313\n",
      "epoch = 106batch = 2000 of 2258duraation = 20.81437864700953\n",
      "..Overrun....no improvement\n",
      "Epoch: 106, Train Loss: 3.10151148, overrun_counter 0\n",
      "epoch = 107batch = 0 of 2258duraation = 0.009798085689544678\n",
      "epoch = 107batch = 1000 of 2258duraation = 10.412876971562703\n",
      "epoch = 107batch = 2000 of 2258duraation = 20.81440045038859\n",
      "..Overrun....no improvement\n",
      "Epoch: 107, Train Loss: 3.11023445, overrun_counter 1\n",
      "epoch = 108batch = 0 of 2258duraation = 0.00942216714223226\n",
      "epoch = 108batch = 1000 of 2258duraation = 10.410149717330933\n",
      "epoch = 108batch = 2000 of 2258duraation = 20.813452939192455\n",
      "..Overrun....no improvement\n",
      "Epoch: 108, Train Loss: 3.09932331, overrun_counter 2\n",
      "epoch = 109batch = 0 of 2258duraation = 0.008112959067026774\n",
      "epoch = 109batch = 1000 of 2258duraation = 10.411817105611165\n",
      "epoch = 109batch = 2000 of 2258duraation = 20.814923199017844\n",
      "Epoch: 109, Train Loss: 3.09126786, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e109_2023_05_20_17_58_18.pth\n",
      "Current LR = 0.00000653\n",
      "epoch = 110batch = 0 of 2258duraation = 0.00856686035792033\n",
      "epoch = 110batch = 1000 of 2258duraation = 10.410309338569641\n"
     ]
    }
   ],
   "source": [
    "csv_loc = os.path.join(\"..\",\"..\",\"data\",\"metadata\",\"neurips_2021_zenodo_0_0_1.csv\")\n",
    "df = prepare_df(classes = classes,csv_loc = csv_loc)\n",
    "plot_df(df)\n",
    "df_train ,df_val ,df_test = train_test_split(df)\n",
    "print(\"now validating the split post loading and keeping TZ data\")\n",
    "validate_split(df_train ,df_val)\n",
    "validate_split(df_train ,df_test)\n",
    "validate_split(df_test ,df_val)\n",
    "df_train_offset = get_offsets_df(df_train, short_audio=True)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=True)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=True)\n",
    "df_train_offset.reset_index(inplace = True , drop = True)\n",
    "df_test_offset.reset_index(inplace = True , drop = True)\n",
    "df_val_offset.reset_index(inplace = True , drop = True)\n",
    "print(\"now validating the split post offset_creation\")\n",
    "validate_split(df_train_offset ,df_val_offset)\n",
    "validate_split(df_train_offset ,df_test_offset)\n",
    "validate_split(df_test_offset ,df_val_offset)\n",
    "\n",
    "class_weights = get_class_weights(df_train_offset)\n",
    "print(\"inside main. class_weigths type = \", type(class_weights))\n",
    "model =MyModel('convnext_xlarge_in22k',224)\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "\n",
    "train_dataset = MozDataset(df_train_offset,  config.data_dir, min_length)\n",
    "val_dataset = MozDataset(df_val_offset,  config.data_dir, min_length)\n",
    "test_dataset = MozDataset(df_test_offset,  config.data_dir, min_length)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True, pin_memory=True )\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory, shuffle = True )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,shuffle = True)\n",
    "\n",
    "encoder = SupConResNet()\n",
    "\n",
    "#train_loader, val_loader,test_loader, model ,class_weights, classes = classes, num_epochs = num_epochs ,n_channels = 1\n",
    "#train_loader, val_loader,test_loader, model, classes ,df,num_epochs = num_epochs ,n_channels = 1\n",
    "tr_model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1 = train_model(train_loader, val_loader, test_loader,model,classes,class_weights ,num_epochs,encoder )\n",
    "\n",
    "print(\"ALL DONE!!!!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b712c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f0a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9ff03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3891a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
