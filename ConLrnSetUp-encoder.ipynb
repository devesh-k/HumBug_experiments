{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c199120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dli/task/ComParE2022_VecNet/notebooks/DK', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/root/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3840b54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/nnAudio/Spectrogram.py:4: Warning: importing Spectrogram subpackage will be deprecated soon. You should import the feature extractor from the feature subpackage. See actual documentation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "#from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "# import IPython.display as ipd\n",
    "\n",
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "## nnAudio\n",
    "from nnAudio import features , Spectrogram\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6ab0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser(description='Trainable_SpecAugment', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "batch_size = 128\n",
    "num_workers= 4\n",
    "pin_memory = True\n",
    "#test_batch_size = 256\n",
    "DEBUG = False\n",
    "num_epochs= 600              \n",
    "USE_SHORT_AUDIO = True\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950dba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98a6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    stride = step_frac*min_length\n",
    "#     print(\"min_length = \" +str(min_length))\n",
    "#     print(\"step_frac = \" +str(step_frac))\n",
    "#     print(\"stride = \" +str(stride))\n",
    "    for _,row in df.iterrows():\n",
    "        #processed_data keeps track of the tensor_values processed thus far\n",
    "        if row['length'] > min_length:\n",
    "            processed_data = 0\n",
    "            #total_data is the total tensor present in the audio\n",
    "            total_data = config.rate*row['length']\n",
    "            #print(\"********\")\n",
    "            count = 0\n",
    "            #print(\"count = \" +str(count))\n",
    "            #print(\"id = \" + str(row['id']) + \" duration = \" +str(row['length']) + \"total x vals = \" + str(total_data))\n",
    "            inner_loop_flag = False\n",
    "            #print(\"going into the inner loop to offset....\")\n",
    "            while(processed_data < total_data):\n",
    "                #print(\"inside inner loop.....\")\n",
    "                start = count*stride*config.rate\n",
    "                #now find out the row_len\n",
    "                if total_data - (start + min_length*config.rate) >= 0:\n",
    "                    #print(\"full chunk \")\n",
    "                    row_len = min_length\n",
    "                    end = start + row_len*config.rate\n",
    "                    audio_offsets.append({'id':row['id'], 'offset':count, 'length': row_len,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "                    #print(\"count = \" +str(count) + \"offset = \" +str(count) + \"start = \" +str(start) + \"end = \" +str(end))\n",
    "                    #print(\"for count.... = \" + str(count) + \"processed data = \" +str(processed_data))\n",
    "                    count+=1\n",
    "                    processed_data = (count*stride)*config.rate\n",
    "                    \n",
    "                else:\n",
    "                    inner_loop_flag = True\n",
    "                    break\n",
    "                    \n",
    "                                                       \n",
    "            #for processing residual data\n",
    "            if(inner_loop_flag):\n",
    "                #print(\"processing residual ....processed \" +str(processed_data) + \" of \" + str(total_data))\n",
    "                start = count*stride*config.rate\n",
    "                resid_durn = round((total_data - processed_data)/config.rate,2)\n",
    "                end = total_data\n",
    "                #print(\"for...\" + str(row['id']) + \" adding the residual data in the data frame with duration = \" + str(resid_durn))\n",
    "                audio_offsets.append({'id':row['id'], 'offset':count, 'length':resid_durn ,'specie_ind': row['specie_ind'],'start':start,'end':end})\n",
    "            \n",
    "        elif short_audio:\n",
    "            start = 0\n",
    "            end = row['length']*config.rate\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind'],'start':0 , 'end':end})\n",
    "    return pd.DataFrame(audio_offsets)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa3a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ####### Prepare df######\n",
    "\n",
    "def prepare_df(classes ,csv_loc = config.data_df  ):\n",
    "    \"\"\"This function reads a csv and creates a dataframe for further processing.\"\"\"\n",
    "    df = pd.read_csv(csv_loc)\n",
    "    #df = df.loc[df['Grade'].notnull()]\n",
    "    df = df.loc[df['species'].notnull()]\n",
    "    # a new column for specie_index to hold numerical values for specie\n",
    "    df['specie_ind'] = \"NULL_VAL\"\n",
    "    ind = 0\n",
    "    for specie in classes:\n",
    "        print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "        row_indexes=df[df['species']==specie].index \n",
    "        df.loc[row_indexes,'specie_ind']= ind\n",
    "        ind+=1\n",
    "    #remove all the rows where specie is other than the one present in classes\n",
    "    df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "    #filter the data for TZ and cup recordings only\n",
    "    idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "    df_all = df[idx_multiclass]\n",
    "    df_all.reset_index(inplace=True, drop = True )\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2741cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plt df\n",
    "def plot_df(df):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    import seaborn as sns\n",
    "    sns.countplot(x = 'species', data = df , ax = ax , hue = 'gender',palette='dark')\n",
    "    #ax.bar_label(ax.containers[0])\n",
    "    #ax.bar_label(ax.containers[-1], fmt='Count:\\n%.2f', label_type='center')\n",
    "    plt.xticks(rotation=90 )\n",
    "    plt.title(\"Distribution of Species \")\n",
    "    plt.rc('xtick', labelsize=12)\n",
    "    plt.rc('xtick', labelsize=12)\n",
    "    plt.rc('axes', labelsize=15)\n",
    "    plt.rc('figure', titlesize=15)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09548fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train _test split####\n",
    "def train_test_split(df_all):\n",
    "    np.random.seed(42)\n",
    "    msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "    df_test = df_all[msk_test]\n",
    "    df_train_temp  = df_all[~msk_test]\n",
    "    msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "    df_val = df_train_temp[msk_train]\n",
    "    df_train  = df_train_temp[~msk_train]\n",
    "    return df_train ,df_val ,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39c1105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validate split ####\n",
    "def validate_split(df1 , df2):\n",
    "    df_temp = pd.merge(df1,df2, on = 'id', how = 'inner')\n",
    "    #print(df_temp)\n",
    "    common_elem = len(df_temp)\n",
    "    #print(\"common_elem = \",common_elem)\n",
    "    con = (common_elem == 0)\n",
    "    #print(\"condition = \",con)\n",
    "    assert (con), \"Split has issues\"\n",
    "    print(\"split is a success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff2c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Specie _distribution ###\n",
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fa0ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class weights to address imbalance in classes ###\n",
    "def get_class_weights(df):\n",
    "    np.array(df_train_offset.specie_ind)\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df.specie_ind)),y=np.array(np.array(df.specie_ind)))\n",
    "    print(type(class_weights))\n",
    "    print(class_weights.shape)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c1a5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pad_mean #####\n",
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e8ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot confusion Matrix ######\n",
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b370e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normalize_batch, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_mean = torch.mean(x, dim=0, keepdim=True)\n",
    "        batch_std = torch.std(x, dim=0, keepdim=True)\n",
    "        epsilon = 1e-8\n",
    "        batch_std = torch.sqrt(batch_std ** 2 + epsilon)\n",
    "        batch_normalized = (x - batch_mean) / batch_std\n",
    "        return batch_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6fe8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcb675df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channel=1, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves\n",
    "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
    "        # https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, layer=100):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "    'resnet18': [resnet18, 512],\n",
    "    'resnet34': [resnet34, 512],\n",
    "    'resnet50': [resnet50, 2048],\n",
    "    'resnet101': [resnet101, 2048],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class SupConResNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, name='resnet18', head='mlp', feat_dim=512):\n",
    "        super(SupConResNet, self).__init__()\n",
    "        model_fun, dim_in = model_dict[name]\n",
    "        self.encoder = model_fun()\n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        feat = F.normalize(self.head(feat), dim=1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d50516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name, image_size = 224):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        #self.backbone = timm.create_model(model_name, pretrained=True, num_classes=8, in_chans=1, drop_path_rate=0.2, global_pool='max', drop_rate=0.25)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "                #### end layer freezing\n",
    "        #self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size),antialias = True)\n",
    "        self.encoder = SupConResNet()\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features= 1)\n",
    "        self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=False,verbose = False).to('cuda')\n",
    "        #self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "        \n",
    "    def forward(self, x,train = True):\n",
    "        # first compute spectrogram\n",
    "        spec_gram = self.spec_layer(x)\n",
    "        output = {}\n",
    "        #print(\"post spec gram shape = \",spec_gram.shape)\n",
    "        spec_gram = self.batch_norm(spec_gram.unsqueeze(dim = 1))\n",
    "        #print(\"post norm shape = \",spec_gram.shape)\n",
    "        spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "        assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after spec gram creation.\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if train == True:\n",
    "                #generate a random number and if condition is met apply aug\n",
    "                ta_transformations_rndm_choice = VT.RandomChoice([AT.FrequencyMasking(freq_mask_param=100),AT.TimeMasking(time_mask_param=50)], p=[.4, .4])\n",
    "                ta_transformations_rndm_apply = VT.RandomApply([AT.FrequencyMasking(freq_mask_param=50),AT.TimeMasking(time_mask_param=25)],p = .2)\n",
    "                spec_gram = ta_transformations_rndm_choice(spec_gram)\n",
    "                spec_gram = ta_transformations_rndm_apply(spec_gram)\n",
    "                spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "                assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after augmentations  \"\n",
    "                aug_bat = [ta_transformations_rndm_choice(spec_gram),ta_transformations_rndm_choice(spec_gram)]\n",
    "                aug_bat = torch.cat(aug_bat , dim = 0)\n",
    "                #print(\"shape of augmented batch = \",aug_bat.shape)\n",
    "                #output['feat'] = aug_bat\n",
    "                \n",
    "        \n",
    "        encoder = self.encoder.to('cuda')\n",
    "        features = encoder(aug_bat)\n",
    "        #print(\"output of encoder shape = \",features.shape)\n",
    "        bsz = x.shape[0]\n",
    "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "        #loss = criterion(features, y)\n",
    "        output['feat'] = features\n",
    "        #x = self.sizer(spec_gram.squeeze(dim = 1))\n",
    "        #print(\"post sizer shape = \",x.shape)\n",
    "        #x = x.unsqueeze(dim = 1)\n",
    "        #print(\"post unsqueeze shape = \",x.shape)\n",
    "        \n",
    "        # then repeat channels\n",
    "        del spec_gram,aug_bat\n",
    "        #backbone_op_nan_check = torch.isnan(x).any().item()\n",
    "        #assert not (backbone_op_nan_check) ,\"Tensor contains NaN values in the backbone OP \"\n",
    "        #print(\"x shape = \" + str(x.shape))\n",
    "        #print(\"x = \" +str(x))\n",
    "        #pred = nn.Softmax(x)\n",
    "        #pred = x\n",
    "        #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "        #print(pred)\n",
    "        #output[\"prediction\"]=  pred \n",
    "        #print(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06f5ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Test Model####\n",
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    softmax = nn.Softmax()\n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            y_pred = model(x,train = False)['prediction']\n",
    "            #y_pred_smax = softmax(y_pred)\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ecc89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader,test_loader, model ,classes,class_weights,num_epochs,encoder ):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = SupConLoss(temperature= .07).to(device)\n",
    "    lr = .000015\n",
    "    base_optimiser = timm.optim.AdamP(model.parameters(), lr= lr)\n",
    "    look_optimiser = timm.optim.Lookahead(base_optimiser)\n",
    "    cooldown_epoch = 50\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(base_optimiser, t_initial= num_epochs,lr_min= lr/100,warmup_t = 5,warmup_lr_init= lr/10,noise_std=.075)\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    lr_log = []\n",
    "    for e in range(num_epochs + cooldown_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        #tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(train_loader):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 100 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. x device = \" +str(x.device))\n",
    "                \n",
    "            \n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            x_sum = torch.sum(x,axis = 1)\n",
    "            x_sum.unsqueeze(dim = 1)\n",
    "                                  \n",
    "            with autocast():\n",
    "                output = model(x,train = True)\n",
    "                #y_pred = output['prediction']\n",
    "                #y_pred_smax = softmax(y_pred)\n",
    "                #preds = torch.argmax(y_pred, axis = 1)\n",
    "                #feat = output['feat']\n",
    "                #images = torch.cat(feat ,dim = 0)\n",
    "                #print(\"post concat images shape = \",images.shape)\n",
    "                #encoder = encoder.to(device)\n",
    "                features = output['feat']\n",
    "                loss = criterion(features, y)\n",
    "                \n",
    "            if DEBUG:\n",
    "                print(\"y_pred  = \" +str(y_pred))\n",
    "                print(\"preds = \" +str(preds))\n",
    "            train_loss += loss.item()\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),error_if_nonfinite=False ,max_norm = 1.0 )\n",
    "            base_optimiser.step()\n",
    "            del x\n",
    "            del y\n",
    "            del features\n",
    "        \n",
    "        #lr_log.append(lr)\n",
    "        look_optimiser.sync_lookahead()\n",
    "        epoch_loss = train_loss/len(train_loader)\n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        #acc_metric = best_loss\n",
    "        #best_acc_metric = best_loss\n",
    "        if  epoch_loss < best_loss:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.enc_model_dir,  checkpoint_name))\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader),  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.enc_model_dir,  checkpoint_name)) \n",
    "            current_lr = base_optimiser.param_groups[0]['lr']\n",
    "            print(\"Current LR = \" + '{0:.8f}'.format(current_lr))\n",
    "            best_epoch = e\n",
    "            best_loss = epoch_loss\n",
    "            #best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader),  overrun_counter))\n",
    "        scheduler.step(e+1)\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59cdfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dataste class #####\n",
    "class MozDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        temp_id = int(self.audio_df.loc[idx]['id'])\n",
    "        file_path = os.path.join(\"..\",\"..\",\"data\",\"audio\")\n",
    "        path_var = file_path +\"/\" +str(temp_id)+ str(\".wav\")\n",
    "        entire_aud, inp_rate = torchaudio.load(path_var)\n",
    "        if inp_rate != config.rate:\n",
    "            #print(\" Original sample rate = \" +str(inp_rate)+ \" resampling ...\")\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=entire_aud.dtype)\n",
    "            entire_aud = resampler(entire_aud)\n",
    "            #print(\"processsing file on \" +str(path_var) + \"Post resample shape =  \" + str(entire_aud.shape))\n",
    "        \n",
    "        aud_len = self.audio_df.loc[idx]['length']\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        #print(\"sliced val = \" +str(int((offset+config.min_duration)*config.rate)))\n",
    "        start_pos = int(round(self.audio_df.loc[idx]['start']))\n",
    "        #print(\"start_pos = \" +str(start_pos))\n",
    "        end_pos =  int(round(self.audio_df.loc[idx]['end']))\n",
    "        #print(\"end_pos = \" +str(end_pos))\n",
    "        x = entire_aud[:,start_pos:end_pos]\n",
    "        #print(\"extracted x = \" +str(x))\n",
    "        #print(\"x shape = \" +str(x.shape))\n",
    "        if aud_len < config.min_duration:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            #print(\"padding on \" +str(path_var))\n",
    "            f_out = pad_mean(x)\n",
    "            #print(\"returning from padding  SHape = \" +str(f_out.shape))\n",
    "        else:\n",
    "            f_out = x[0]\n",
    "            f_out = f_out.unsqueeze(0)\n",
    "            \n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            #print(\"offset = \" + str(offset))\n",
    "            #print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(f_out.shape))\n",
    "        \n",
    "        #x_val = x[:,start:end]\n",
    "        #now that we have final x- let's create specgram and add augmentations.\n",
    "                 \n",
    "        return (f_out,self.audio_df.loc[idx]['specie_ind'] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bbbb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train_model ####\n",
    "#train_loader, val_loader, test_loader,model,classes,df_train_offset ,num_epochs = num_epochs \n",
    "\n",
    "### Get_indices ####\n",
    "def get_indices(num_values ,df ,classes = classes):\n",
    "    new_df = pd.DataFrame()\n",
    "    for ind in range(len(classes)):\n",
    "        #print(\"ind = \", ind)\n",
    "        op = df[df['specie_ind'] == ind]\n",
    "        #print(\"len op = \", len(op))\n",
    "        op_new = op.sample(n = 1)\n",
    "        #print(\"rand_ind = \" , rand_ind)\n",
    "        #([df1, df2], axis=1)\n",
    "        new_df = pd.concat([op_new,new_df],axis = 0)\n",
    "        #print(\"elem = \" , elem)\n",
    "        #new_list.append(elem)\n",
    "    if len(new_df) < num_values:\n",
    "        diff =  num_values - len(new_df)\n",
    "        #print(\"diff = \", diff)\n",
    "        remaining_elems= df.sample(n = diff)\n",
    "        #print(\"len of remaining elems = \", len(remaining_elems))\n",
    "        new_df = pd.concat([remaining_elems,new_df],axis = 0)\n",
    "        \n",
    "    #print(\"new_df = \", new_df)    \n",
    "    new_df_1 = new_df.reset_index(drop = True)\n",
    "    return new_df_1\n",
    "\n",
    "#### Load model ####\n",
    "def load_model(filepath, model=MyModel('convnext_xlarge_in22k')):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811b231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "901060de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3921014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n",
      "now validating the split post loading and keeping TZ data\n",
      "split is a success\n",
      "split is a success\n",
      "split is a success\n",
      "now validating the split post offset_creation\n",
      "split is a success\n",
      "split is a success\n",
      "split is a success\n",
      "<class 'numpy.ndarray'>\n",
      "(8,)\n",
      "inside main. class_weigths type =  <class 'numpy.ndarray'>\n",
      "Training on cuda:0\n",
      "epoch = 0batch = 0 of 283duraation = 0.05708500544230143\n",
      "epoch = 0batch = 100 of 283duraation = 2.14768515030543\n",
      "epoch = 0batch = 200 of 283duraation = 4.173191154003144\n",
      "Epoch: 0, Train Loss: 5.59042564, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e0_2023_05_29_19_27_31.pth\n",
      "Current LR = 0.00000150\n",
      "epoch = 1batch = 0 of 283duraation = 0.057416709264119466\n",
      "epoch = 1batch = 100 of 283duraation = 2.0862447659174603\n",
      "epoch = 1batch = 200 of 283duraation = 4.11257305542628\n",
      "Epoch: 1, Train Loss: 5.53714797, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e1_2023_05_29_19_33_18.pth\n",
      "Current LR = 0.00000420\n",
      "epoch = 2batch = 0 of 283duraation = 0.0566900094350179\n",
      "epoch = 2batch = 100 of 283duraation = 2.082304294904073\n",
      "epoch = 2batch = 200 of 283duraation = 4.108022773265839\n",
      "Epoch: 2, Train Loss: 5.53615272, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e2_2023_05_29_19_39_05.pth\n",
      "Current LR = 0.00000690\n",
      "epoch = 3batch = 0 of 283duraation = 0.0609013557434082\n",
      "epoch = 3batch = 100 of 283duraation = 2.0862722595532737\n",
      "epoch = 3batch = 200 of 283duraation = 4.109226961930593\n",
      "Epoch: 3, Train Loss: 5.53557631, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e3_2023_05_29_19_44_52.pth\n",
      "Current LR = 0.00000960\n",
      "epoch = 4batch = 0 of 283duraation = 0.056093970934549965\n",
      "epoch = 4batch = 100 of 283duraation = 2.0833375970522563\n",
      "epoch = 4batch = 200 of 283duraation = 4.106665182113647\n",
      "Epoch: 4, Train Loss: 5.53503342, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e4_2023_05_29_19_50_38.pth\n",
      "Current LR = 0.00001230\n",
      "epoch = 5batch = 0 of 283duraation = 0.05745320320129395\n",
      "epoch = 5batch = 100 of 283duraation = 2.0827592571576434\n",
      "epoch = 5batch = 200 of 283duraation = 4.105956908067068\n",
      "Epoch: 5, Train Loss: 5.53462864, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e5_2023_05_29_19_56_25.pth\n",
      "Current LR = 0.00001500\n",
      "epoch = 6batch = 0 of 283duraation = 0.05797937711079915\n",
      "epoch = 6batch = 100 of 283duraation = 2.0834632317225137\n",
      "epoch = 6batch = 200 of 283duraation = 4.106956692536672\n",
      "Epoch: 6, Train Loss: 5.53434401, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e6_2023_05_29_20_02_11.pth\n",
      "Current LR = 0.00001500\n",
      "epoch = 7batch = 0 of 283duraation = 0.05226131280263265\n",
      "epoch = 7batch = 100 of 283duraation = 2.0794364253679913\n",
      "epoch = 7batch = 200 of 283duraation = 4.103018736839294\n",
      "Epoch: 7, Train Loss: 5.53393455, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e7_2023_05_29_20_07_58.pth\n",
      "Current LR = 0.00001500\n",
      "epoch = 8batch = 0 of 283duraation = 0.056856822967529294\n",
      "epoch = 8batch = 100 of 283duraation = 2.084661928812663\n",
      "epoch = 8batch = 200 of 283duraation = 4.109570848941803\n",
      "Epoch: 8, Train Loss: 5.53366834, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e8_2023_05_29_20_13_44.pth\n",
      "Current LR = 0.00001499\n",
      "epoch = 9batch = 0 of 283duraation = 0.053755104541778564\n",
      "epoch = 9batch = 100 of 283duraation = 2.0804387370745343\n",
      "epoch = 9batch = 200 of 283duraation = 4.105578970909119\n",
      "..Overrun....no improvement\n",
      "Epoch: 9, Train Loss: 5.53383236, overrun_counter 0\n",
      "epoch = 10batch = 0 of 283duraation = 0.05545932054519653\n",
      "epoch = 10batch = 100 of 283duraation = 2.0821598768234253\n",
      "epoch = 10batch = 200 of 283duraation = 4.106738694508871\n",
      "..Overrun....no improvement\n",
      "Epoch: 10, Train Loss: 5.53412974, overrun_counter 1\n",
      "epoch = 11batch = 0 of 283duraation = 0.05461487372716268\n",
      "epoch = 11batch = 100 of 283duraation = 2.080066974957784\n",
      "epoch = 11batch = 200 of 283duraation = 4.103920209407806\n",
      "..Overrun....no improvement\n",
      "Epoch: 11, Train Loss: 5.53393316, overrun_counter 2\n",
      "epoch = 12batch = 0 of 283duraation = 0.058330535888671875\n",
      "epoch = 12batch = 100 of 283duraation = 2.083183141549428\n",
      "epoch = 12batch = 200 of 283duraation = 4.104807615280151\n",
      "..Overrun....no improvement\n",
      "Epoch: 12, Train Loss: 5.53421746, overrun_counter 3\n",
      "epoch = 13batch = 0 of 283duraation = 0.055627715587615964\n",
      "epoch = 13batch = 100 of 283duraation = 2.0823817292849225\n",
      "epoch = 13batch = 200 of 283duraation = 4.105240420500437\n",
      "..Overrun....no improvement\n",
      "Epoch: 13, Train Loss: 5.53385801, overrun_counter 4\n",
      "epoch = 14batch = 0 of 283duraation = 0.05728705724080404\n",
      "epoch = 14batch = 100 of 283duraation = 2.0822823723157247\n",
      "epoch = 14batch = 200 of 283duraation = 4.105651044845581\n",
      "Epoch: 14, Train Loss: 5.53362659, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e14_2023_05_29_20_48_22.pth\n",
      "Current LR = 0.00001498\n",
      "epoch = 15batch = 0 of 283duraation = 0.05982118844985962\n",
      "epoch = 15batch = 100 of 283duraation = 2.086156169573466\n",
      "epoch = 15batch = 200 of 283duraation = 4.108668148517609\n",
      "..Overrun....no improvement\n",
      "Epoch: 15, Train Loss: 5.53378560, overrun_counter 0\n",
      "epoch = 16batch = 0 of 283duraation = 0.05343536535898844\n",
      "epoch = 16batch = 100 of 283duraation = 2.0777942220369976\n",
      "epoch = 16batch = 200 of 283duraation = 4.101223969459534\n",
      "Epoch: 16, Train Loss: 5.53340143, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e16_2023_05_29_20_59_55.pth\n",
      "Current LR = 0.00001497\n",
      "epoch = 17batch = 0 of 283duraation = 0.05440433422724406\n",
      "epoch = 17batch = 100 of 283duraation = 2.083251090844472\n",
      "epoch = 17batch = 200 of 283duraation = 4.105840345223744\n",
      "..Overrun....no improvement\n",
      "Epoch: 17, Train Loss: 5.53386952, overrun_counter 0\n",
      "epoch = 18batch = 0 of 283duraation = 0.058854103088378906\n",
      "epoch = 18batch = 100 of 283duraation = 2.0863587657610574\n",
      "epoch = 18batch = 200 of 283duraation = 4.111271007855733\n",
      "..Overrun....no improvement\n",
      "Epoch: 18, Train Loss: 5.53386999, overrun_counter 1\n",
      "epoch = 19batch = 0 of 283duraation = 0.06233384609222412\n",
      "epoch = 19batch = 100 of 283duraation = 2.0879638314247133\n",
      "epoch = 19batch = 200 of 283duraation = 4.110481170813243\n",
      "..Overrun....no improvement\n",
      "Epoch: 19, Train Loss: 5.53392125, overrun_counter 2\n",
      "epoch = 20batch = 0 of 283duraation = 0.053468469778696695\n",
      "epoch = 20batch = 100 of 283duraation = 2.0778032978375753\n",
      "epoch = 20batch = 200 of 283duraation = 4.09962721268336\n",
      "..Overrun....no improvement\n",
      "Epoch: 20, Train Loss: 5.53382567, overrun_counter 3\n",
      "epoch = 21batch = 0 of 283duraation = 0.06716449658075968\n",
      "epoch = 21batch = 100 of 283duraation = 2.095582409699758\n",
      "epoch = 21batch = 200 of 283duraation = 4.12052454551061\n",
      "..Overrun....no improvement\n",
      "Epoch: 21, Train Loss: 5.53391166, overrun_counter 4\n",
      "epoch = 22batch = 0 of 283duraation = 0.057739790280659994\n",
      "epoch = 22batch = 100 of 283duraation = 2.083562954266866\n",
      "epoch = 22batch = 200 of 283duraation = 4.107831108570099\n",
      "..Overrun....no improvement\n",
      "Epoch: 22, Train Loss: 5.53376489, overrun_counter 5\n",
      "epoch = 23batch = 0 of 283duraation = 0.054575872421264646\n",
      "epoch = 23batch = 100 of 283duraation = 2.0807838956514995\n",
      "epoch = 23batch = 200 of 283duraation = 4.105258063475291\n",
      "..Overrun....no improvement\n",
      "Epoch: 23, Train Loss: 5.53376333, overrun_counter 6\n",
      "epoch = 24batch = 0 of 283duraation = 0.05392438968022664\n",
      "epoch = 24batch = 100 of 283duraation = 2.0805880784988404\n",
      "epoch = 24batch = 200 of 283duraation = 4.104642808437347\n",
      "..Overrun....no improvement\n",
      "Epoch: 24, Train Loss: 5.53392498, overrun_counter 7\n",
      "epoch = 25batch = 0 of 283duraation = 0.060801124572753905\n",
      "epoch = 25batch = 100 of 283duraation = 2.086544140179952\n",
      "epoch = 25batch = 200 of 283duraation = 4.110717558860779\n",
      "..Overrun....no improvement\n",
      "Epoch: 25, Train Loss: 5.53408480, overrun_counter 8\n",
      "epoch = 26batch = 0 of 283duraation = 0.058802203337351484\n",
      "epoch = 26batch = 100 of 283duraation = 2.084976545969645\n",
      "epoch = 26batch = 200 of 283duraation = 4.108999836444855\n",
      "..Overrun....no improvement\n",
      "Epoch: 26, Train Loss: 5.53348531, overrun_counter 9\n",
      "epoch = 27batch = 0 of 283duraation = 0.05465431213378906\n",
      "epoch = 27batch = 100 of 283duraation = 2.082284080982208\n",
      "epoch = 27batch = 200 of 283duraation = 4.106675044695536\n",
      "..Overrun....no improvement\n",
      "Epoch: 27, Train Loss: 5.53352280, overrun_counter 10\n",
      "epoch = 28batch = 0 of 283duraation = 0.05574313004811605\n",
      "epoch = 28batch = 100 of 283duraation = 2.082564675807953\n",
      "epoch = 28batch = 200 of 283duraation = 4.10642573038737\n",
      "..Overrun....no improvement\n",
      "Epoch: 28, Train Loss: 5.53377259, overrun_counter 11\n",
      "epoch = 29batch = 0 of 283duraation = 0.05629169940948486\n",
      "epoch = 29batch = 100 of 283duraation = 2.083442536989848\n",
      "epoch = 29batch = 200 of 283duraation = 4.108686820665995\n",
      "..Overrun....no improvement\n",
      "Epoch: 29, Train Loss: 5.53380470, overrun_counter 12\n",
      "epoch = 30batch = 0 of 283duraation = 0.058140822251637775\n",
      "epoch = 30batch = 100 of 283duraation = 2.0848491470019024\n",
      "epoch = 30batch = 200 of 283duraation = 4.10809991757075\n",
      "..Overrun....no improvement\n",
      "Epoch: 30, Train Loss: 5.53416443, overrun_counter 13\n",
      "epoch = 31batch = 0 of 283duraation = 0.05625433127085368\n",
      "epoch = 31batch = 100 of 283duraation = 2.080468809604645\n",
      "epoch = 31batch = 200 of 283duraation = 4.105158130327861\n",
      "Epoch: 31, Train Loss: 5.53317749, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e31_2023_05_29_22_26_31.pth\n",
      "Current LR = 0.00001490\n",
      "epoch = 32batch = 0 of 283duraation = 0.058217028776804604\n",
      "epoch = 32batch = 100 of 283duraation = 2.084960679213206\n",
      "epoch = 32batch = 200 of 283duraation = 4.109068874518076\n",
      "..Overrun....no improvement\n",
      "Epoch: 32, Train Loss: 5.53389140, overrun_counter 0\n",
      "epoch = 33batch = 0 of 283duraation = 0.05589714845021566\n",
      "epoch = 33batch = 100 of 283duraation = 2.0817600806554157\n",
      "epoch = 33batch = 200 of 283duraation = 4.106004174550375\n",
      "..Overrun....no improvement\n",
      "Epoch: 33, Train Loss: 5.53354589, overrun_counter 1\n",
      "epoch = 34batch = 0 of 283duraation = 0.056376894315083824\n",
      "epoch = 34batch = 100 of 283duraation = 2.0859068115552266\n",
      "epoch = 34batch = 200 of 283duraation = 4.108659903208415\n",
      "..Overrun....no improvement\n",
      "Epoch: 34, Train Loss: 5.53348609, overrun_counter 2\n",
      "epoch = 35batch = 0 of 283duraation = 0.05352207819620768\n",
      "epoch = 35batch = 100 of 283duraation = 2.080335867404938\n",
      "epoch = 35batch = 200 of 283duraation = 4.10513463417689\n",
      "Epoch: 35, Train Loss: 5.53305267, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e35_2023_05_29_22_49_37.pth\n",
      "Current LR = 0.00001488\n",
      "epoch = 36batch = 0 of 283duraation = 0.05839505195617676\n",
      "epoch = 36batch = 100 of 283duraation = 2.085850195089976\n",
      "epoch = 36batch = 200 of 283duraation = 4.110178510348002\n",
      "..Overrun....no improvement\n",
      "Epoch: 36, Train Loss: 5.53328574, overrun_counter 0\n",
      "epoch = 37batch = 0 of 283duraation = 0.05416580438613892\n",
      "epoch = 37batch = 100 of 283duraation = 2.080737034479777\n",
      "epoch = 37batch = 200 of 283duraation = 4.103798147042593\n",
      "..Overrun....no improvement\n",
      "Epoch: 37, Train Loss: 5.53356932, overrun_counter 1\n",
      "epoch = 38batch = 0 of 283duraation = 0.06350491046905518\n",
      "epoch = 38batch = 100 of 283duraation = 2.089304180939992\n",
      "epoch = 38batch = 200 of 283duraation = 4.112182573477427\n",
      "..Overrun....no improvement\n",
      "Epoch: 38, Train Loss: 5.53307493, overrun_counter 2\n",
      "epoch = 39batch = 0 of 283duraation = 0.05981631278991699\n",
      "epoch = 39batch = 100 of 283duraation = 2.085331976413727\n",
      "epoch = 39batch = 200 of 283duraation = 4.110132090250651\n",
      "..Overrun....no improvement\n",
      "Epoch: 39, Train Loss: 5.53341318, overrun_counter 3\n",
      "epoch = 40batch = 0 of 283duraation = 0.05933604637781779\n",
      "epoch = 40batch = 100 of 283duraation = 2.086501959959666\n",
      "epoch = 40batch = 200 of 283duraation = 4.111445279916127\n",
      "..Overrun....no improvement\n",
      "Epoch: 40, Train Loss: 5.53341397, overrun_counter 4\n",
      "epoch = 41batch = 0 of 283duraation = 0.05910499493281047\n",
      "epoch = 41batch = 100 of 283duraation = 2.085478063424428\n",
      "epoch = 41batch = 200 of 283duraation = 4.11052321990331\n",
      "..Overrun....no improvement\n",
      "Epoch: 41, Train Loss: 5.53386863, overrun_counter 5\n",
      "epoch = 42batch = 0 of 283duraation = 0.05969982147216797\n",
      "epoch = 42batch = 100 of 283duraation = 2.0864577015240986\n",
      "epoch = 42batch = 200 of 283duraation = 4.1107874790827434\n",
      "..Overrun....no improvement\n",
      "Epoch: 42, Train Loss: 5.53361381, overrun_counter 6\n",
      "epoch = 43batch = 0 of 283duraation = 0.059321399529774985\n",
      "epoch = 43batch = 100 of 283duraation = 2.0868605295817058\n",
      "epoch = 43batch = 200 of 283duraation = 4.110206949710846\n",
      "..Overrun....no improvement\n",
      "Epoch: 43, Train Loss: 5.53396208, overrun_counter 7\n",
      "epoch = 44batch = 0 of 283duraation = 0.05723494291305542\n",
      "epoch = 44batch = 100 of 283duraation = 2.082352630297343\n",
      "epoch = 44batch = 200 of 283duraation = 4.106892263889312\n",
      "..Overrun....no improvement\n",
      "Epoch: 44, Train Loss: 5.53335947, overrun_counter 8\n",
      "epoch = 45batch = 0 of 283duraation = 0.060140629609425865\n",
      "epoch = 45batch = 100 of 283duraation = 2.08772873878479\n",
      "epoch = 45batch = 200 of 283duraation = 4.112436620394389\n",
      "Epoch: 45, Train Loss: 5.53288808, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e45_2023_05_29_23_47_22.pth\n",
      "Current LR = 0.00001479\n",
      "epoch = 46batch = 0 of 283duraation = 0.05387000242869059\n",
      "epoch = 46batch = 100 of 283duraation = 2.0824382662773133\n",
      "epoch = 46batch = 200 of 283duraation = 4.1077612002690636\n",
      "..Overrun....no improvement\n",
      "Epoch: 46, Train Loss: 5.53290772, overrun_counter 0\n",
      "epoch = 47batch = 0 of 283duraation = 0.051143987973531084\n",
      "epoch = 47batch = 100 of 283duraation = 2.0764702121416727\n",
      "epoch = 47batch = 200 of 283duraation = 4.100382701555888\n",
      "Epoch: 47, Train Loss: 5.53261229, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e47_2023_05_29_23_58_57.pth\n",
      "Current LR = 0.00001478\n",
      "epoch = 48batch = 0 of 283duraation = 0.05958439906438192\n",
      "epoch = 48batch = 100 of 283duraation = 2.0874936024347943\n",
      "epoch = 48batch = 200 of 283duraation = 4.113260662555694\n",
      "..Overrun....no improvement\n",
      "Epoch: 48, Train Loss: 5.53341700, overrun_counter 0\n",
      "epoch = 49batch = 0 of 283duraation = 0.058034042517344155\n",
      "epoch = 49batch = 100 of 283duraation = 2.0856168985366823\n",
      "epoch = 49batch = 200 of 283duraation = 4.110292132695516\n",
      "..Overrun....no improvement\n",
      "Epoch: 49, Train Loss: 5.53318711, overrun_counter 1\n",
      "epoch = 50batch = 0 of 283duraation = 0.057957005500793454\n",
      "epoch = 50batch = 100 of 283duraation = 2.0820636987686156\n",
      "epoch = 50batch = 200 of 283duraation = 4.105659266312917\n",
      "..Overrun....no improvement\n",
      "Epoch: 50, Train Loss: 5.53265969, overrun_counter 2\n",
      "epoch = 51batch = 0 of 283duraation = 0.058269186814626055\n",
      "epoch = 51batch = 100 of 283duraation = 2.084378763039907\n",
      "epoch = 51batch = 200 of 283duraation = 4.109019927183787\n",
      "..Overrun....no improvement\n",
      "Epoch: 51, Train Loss: 5.53318350, overrun_counter 3\n",
      "epoch = 52batch = 0 of 283duraation = 0.06119044621785482\n",
      "epoch = 52batch = 100 of 283duraation = 2.0878655831019084\n",
      "epoch = 52batch = 200 of 283duraation = 4.112348091602326\n",
      "..Overrun....no improvement\n",
      "Epoch: 52, Train Loss: 5.53288639, overrun_counter 4\n",
      "epoch = 53batch = 0 of 283duraation = 0.057288622856140135\n",
      "epoch = 53batch = 100 of 283duraation = 2.0841457804044086\n",
      "epoch = 53batch = 200 of 283duraation = 4.1084525903066\n",
      "..Overrun....no improvement\n",
      "Epoch: 53, Train Loss: 5.53310483, overrun_counter 5\n",
      "epoch = 54batch = 0 of 283duraation = 0.05585840145746867\n",
      "epoch = 54batch = 100 of 283duraation = 2.0822959899902345\n",
      "epoch = 54batch = 200 of 283duraation = 4.105344013373057\n",
      "..Overrun....no improvement\n",
      "Epoch: 54, Train Loss: 5.53338388, overrun_counter 6\n",
      "epoch = 55batch = 0 of 283duraation = 0.05763685703277588\n",
      "epoch = 55batch = 100 of 283duraation = 2.0837326725323995\n",
      "epoch = 55batch = 200 of 283duraation = 4.107600498199463\n",
      "..Overrun....no improvement\n",
      "Epoch: 55, Train Loss: 5.53302805, overrun_counter 7\n",
      "epoch = 56batch = 0 of 283duraation = 0.05723288059234619\n",
      "epoch = 56batch = 100 of 283duraation = 2.0837855458259584\n",
      "epoch = 56batch = 200 of 283duraation = 4.106158459186554\n",
      "..Overrun....no improvement\n",
      "Epoch: 56, Train Loss: 5.53310948, overrun_counter 8\n",
      "epoch = 57batch = 0 of 283duraation = 0.05839063326517741\n",
      "epoch = 57batch = 100 of 283duraation = 2.0820884823799135\n",
      "epoch = 57batch = 200 of 283duraation = 4.1068390011787415\n",
      "..Overrun....no improvement\n",
      "Epoch: 57, Train Loss: 5.53321578, overrun_counter 9\n",
      "epoch = 58batch = 0 of 283duraation = 0.05720860958099365\n",
      "epoch = 58batch = 100 of 283duraation = 2.0825584967931112\n",
      "epoch = 58batch = 200 of 283duraation = 4.106016135215759\n",
      "..Overrun....no improvement\n",
      "Epoch: 58, Train Loss: 5.53293799, overrun_counter 10\n",
      "epoch = 59batch = 0 of 283duraation = 0.05616474151611328\n",
      "epoch = 59batch = 100 of 283duraation = 2.0810046474138897\n",
      "epoch = 59batch = 200 of 283duraation = 4.10613204240799\n",
      "..Overrun....no improvement\n",
      "Epoch: 59, Train Loss: 5.53268385, overrun_counter 11\n",
      "epoch = 60batch = 0 of 283duraation = 0.05640977621078491\n",
      "epoch = 60batch = 100 of 283duraation = 2.0831498781840008\n",
      "epoch = 60batch = 200 of 283duraation = 4.107279189427694\n",
      "..Overrun....no improvement\n",
      "Epoch: 60, Train Loss: 5.53334505, overrun_counter 12\n",
      "epoch = 61batch = 0 of 283duraation = 0.05720444917678833\n",
      "epoch = 61batch = 100 of 283duraation = 2.082481010754903\n",
      "epoch = 61batch = 200 of 283duraation = 4.105981214841207\n",
      "Epoch: 61, Train Loss: 5.53232292, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e61_2023_05_30_01_19_47.pth\n",
      "Current LR = 0.00001462\n",
      "epoch = 62batch = 0 of 283duraation = 0.05629278421401977\n",
      "epoch = 62batch = 100 of 283duraation = 2.0805736740430194\n",
      "epoch = 62batch = 200 of 283duraation = 4.103687063852946\n",
      "..Overrun....no improvement\n",
      "Epoch: 62, Train Loss: 5.53268993, overrun_counter 0\n",
      "epoch = 63batch = 0 of 283duraation = 0.05634729464848836\n",
      "epoch = 63batch = 100 of 283duraation = 2.07896667321523\n",
      "epoch = 63batch = 200 of 283duraation = 4.10122592051824\n",
      "..Overrun....no improvement\n",
      "Epoch: 63, Train Loss: 5.53276647, overrun_counter 1\n",
      "epoch = 64batch = 0 of 283duraation = 0.06044078270594279\n",
      "epoch = 64batch = 100 of 283duraation = 2.0868298689524334\n",
      "epoch = 64batch = 200 of 283duraation = 4.111695452531179\n",
      "..Overrun....no improvement\n",
      "Epoch: 64, Train Loss: 5.53320505, overrun_counter 2\n",
      "epoch = 65batch = 0 of 283duraation = 0.056623681386311846\n",
      "epoch = 65batch = 100 of 283duraation = 2.08319149017334\n",
      "epoch = 65batch = 200 of 283duraation = 4.107366633415222\n",
      "..Overrun....no improvement\n",
      "Epoch: 65, Train Loss: 5.53289347, overrun_counter 3\n",
      "epoch = 66batch = 0 of 283duraation = 0.056044975916544594\n",
      "epoch = 66batch = 100 of 283duraation = 2.0831018249193827\n",
      "epoch = 66batch = 200 of 283duraation = 4.1059926629066466\n",
      "..Overrun....no improvement\n",
      "Epoch: 66, Train Loss: 5.53312714, overrun_counter 4\n",
      "epoch = 67batch = 0 of 283duraation = 0.05362461010615031\n",
      "epoch = 67batch = 100 of 283duraation = 2.079169193903605\n",
      "epoch = 67batch = 200 of 283duraation = 4.102401594320933\n",
      "..Overrun....no improvement\n",
      "Epoch: 67, Train Loss: 5.53303582, overrun_counter 5\n",
      "epoch = 68batch = 0 of 283duraation = 0.05565122365951538\n",
      "epoch = 68batch = 100 of 283duraation = 2.08073201974233\n",
      "epoch = 68batch = 200 of 283duraation = 4.103789714972178\n",
      "..Overrun....no improvement\n",
      "Epoch: 68, Train Loss: 5.53359629, overrun_counter 6\n",
      "epoch = 69batch = 0 of 283duraation = 0.0521631916364034\n",
      "epoch = 69batch = 100 of 283duraation = 2.0801507472991942\n",
      "epoch = 69batch = 200 of 283duraation = 4.104129103819529\n",
      "..Overrun....no improvement\n",
      "Epoch: 69, Train Loss: 5.53368871, overrun_counter 7\n",
      "epoch = 70batch = 0 of 283duraation = 0.056895661354064944\n",
      "epoch = 70batch = 100 of 283duraation = 2.082306909561157\n",
      "epoch = 70batch = 200 of 283duraation = 4.104015759627024\n",
      "..Overrun....no improvement\n",
      "Epoch: 70, Train Loss: 5.53370200, overrun_counter 8\n",
      "epoch = 71batch = 0 of 283duraation = 0.054969159762064616\n",
      "epoch = 71batch = 100 of 283duraation = 2.0816723783810933\n",
      "epoch = 71batch = 200 of 283duraation = 4.104024147987365\n",
      "..Overrun....no improvement\n",
      "Epoch: 71, Train Loss: 5.53387754, overrun_counter 9\n",
      "epoch = 72batch = 0 of 283duraation = 0.053525781631469725\n",
      "epoch = 72batch = 100 of 283duraation = 2.0822782119115195\n",
      "epoch = 72batch = 200 of 283duraation = 4.10524928967158\n",
      "..Overrun....no improvement\n",
      "Epoch: 72, Train Loss: 5.53303165, overrun_counter 10\n",
      "epoch = 73batch = 0 of 283duraation = 0.0591845711072286\n",
      "epoch = 73batch = 100 of 283duraation = 2.0865980863571165\n",
      "epoch = 73batch = 200 of 283duraation = 4.1120959401130674\n",
      "..Overrun....no improvement\n",
      "Epoch: 73, Train Loss: 5.53325214, overrun_counter 11\n",
      "epoch = 74batch = 0 of 283duraation = 0.05779381593068441\n",
      "epoch = 74batch = 100 of 283duraation = 2.0843365669250487\n",
      "epoch = 74batch = 200 of 283duraation = 4.107110067208608\n",
      "..Overrun....no improvement\n",
      "Epoch: 74, Train Loss: 5.53306882, overrun_counter 12\n",
      "epoch = 75batch = 0 of 283duraation = 0.057377549012502034\n",
      "epoch = 75batch = 100 of 283duraation = 2.086557344595591\n",
      "epoch = 75batch = 200 of 283duraation = 4.109509785970052\n",
      "..Overrun....no improvement\n",
      "Epoch: 75, Train Loss: 5.53367760, overrun_counter 13\n",
      "epoch = 76batch = 0 of 283duraation = 0.05320543845494588\n",
      "epoch = 76batch = 100 of 283duraation = 2.080834635098775\n",
      "epoch = 76batch = 200 of 283duraation = 4.104473650455475\n",
      "..Overrun....no improvement\n",
      "Epoch: 76, Train Loss: 5.53377735, overrun_counter 14\n",
      "epoch = 77batch = 0 of 283duraation = 0.05630552371342977\n",
      "epoch = 77batch = 100 of 283duraation = 2.0823855996131897\n",
      "epoch = 77batch = 200 of 283duraation = 4.108094747861227\n",
      "..Overrun....no improvement\n",
      "Epoch: 77, Train Loss: 5.53329485, overrun_counter 15\n",
      "epoch = 78batch = 0 of 283duraation = 0.057750121752421064\n",
      "epoch = 78batch = 100 of 283duraation = 2.0850162943204245\n",
      "epoch = 78batch = 200 of 283duraation = 4.10955046415329\n",
      "..Overrun....no improvement\n",
      "Epoch: 78, Train Loss: 5.53346418, overrun_counter 16\n",
      "epoch = 79batch = 0 of 283duraation = 0.05668399333953857\n",
      "epoch = 79batch = 100 of 283duraation = 2.08143972158432\n",
      "epoch = 79batch = 200 of 283duraation = 4.104502618312836\n",
      "..Overrun....no improvement\n",
      "Epoch: 79, Train Loss: 5.53377075, overrun_counter 17\n",
      "epoch = 80batch = 0 of 283duraation = 0.0594424565633138\n",
      "epoch = 80batch = 100 of 283duraation = 2.0860240896542868\n",
      "epoch = 80batch = 200 of 283duraation = 4.110778760910034\n",
      "..Overrun....no improvement\n",
      "Epoch: 80, Train Loss: 5.53370170, overrun_counter 18\n",
      "epoch = 81batch = 0 of 283duraation = 0.056558688481648765\n",
      "epoch = 81batch = 100 of 283duraation = 2.0815850297609964\n",
      "epoch = 81batch = 200 of 283duraation = 4.104761544863383\n",
      "..Overrun....no improvement\n",
      "Epoch: 81, Train Loss: 5.53318440, overrun_counter 19\n",
      "epoch = 82batch = 0 of 283duraation = 0.05401047468185425\n",
      "epoch = 82batch = 100 of 283duraation = 2.08254132270813\n",
      "epoch = 82batch = 200 of 283duraation = 4.107222398122151\n",
      "..Overrun....no improvement\n",
      "Epoch: 82, Train Loss: 5.53418780, overrun_counter 20\n",
      "epoch = 83batch = 0 of 283duraation = 0.0607974370320638\n",
      "epoch = 83batch = 100 of 283duraation = 2.087878096103668\n",
      "epoch = 83batch = 200 of 283duraation = 4.112821006774903\n",
      "..Overrun....no improvement\n",
      "Epoch: 83, Train Loss: 5.53347985, overrun_counter 21\n",
      "epoch = 84batch = 0 of 283duraation = 0.05607089598973592\n",
      "epoch = 84batch = 100 of 283duraation = 2.0830477992693583\n",
      "epoch = 84batch = 200 of 283duraation = 4.108164950211843\n",
      "..Overrun....no improvement\n",
      "Epoch: 84, Train Loss: 5.53322387, overrun_counter 22\n",
      "epoch = 85batch = 0 of 283duraation = 0.054314116636912026\n",
      "epoch = 85batch = 100 of 283duraation = 2.0804732481638593\n",
      "epoch = 85batch = 200 of 283duraation = 4.1043462236722315\n",
      "..Overrun....no improvement\n",
      "Epoch: 85, Train Loss: 5.53318371, overrun_counter 23\n",
      "epoch = 86batch = 0 of 283duraation = 0.057757492860158285\n",
      "epoch = 86batch = 100 of 283duraation = 2.0865914185841876\n",
      "epoch = 86batch = 200 of 283duraation = 4.110947799682617\n",
      "..Overrun....no improvement\n",
      "Epoch: 86, Train Loss: 5.53280792, overrun_counter 24\n",
      "epoch = 87batch = 0 of 283duraation = 0.06125788688659668\n",
      "epoch = 87batch = 100 of 283duraation = 2.0856353958447773\n",
      "epoch = 87batch = 200 of 283duraation = 4.111474152406057\n",
      "..Overrun....no improvement\n",
      "Epoch: 87, Train Loss: 5.53383040, overrun_counter 25\n",
      "epoch = 88batch = 0 of 283duraation = 0.055293869972229\n",
      "epoch = 88batch = 100 of 283duraation = 2.0799532771110534\n",
      "epoch = 88batch = 200 of 283duraation = 4.104201718171438\n",
      "..Overrun....no improvement\n",
      "Epoch: 88, Train Loss: 5.53293170, overrun_counter 26\n",
      "epoch = 89batch = 0 of 283duraation = 0.05510799884796143\n",
      "epoch = 89batch = 100 of 283duraation = 2.080858651796977\n",
      "epoch = 89batch = 200 of 283duraation = 4.1044211149215695\n",
      "..Overrun....no improvement\n",
      "Epoch: 89, Train Loss: 5.53283573, overrun_counter 27\n",
      "epoch = 90batch = 0 of 283duraation = 0.05732300281524658\n",
      "epoch = 90batch = 100 of 283duraation = 2.0827588478724164\n",
      "epoch = 90batch = 200 of 283duraation = 4.105517264207204\n",
      "..Overrun....no improvement\n",
      "Epoch: 90, Train Loss: 5.53306228, overrun_counter 28\n",
      "epoch = 91batch = 0 of 283duraation = 0.0577880342801412\n",
      "epoch = 91batch = 100 of 283duraation = 2.083544898033142\n",
      "epoch = 91batch = 200 of 283duraation = 4.106479477882385\n",
      "..Overrun....no improvement\n",
      "Epoch: 91, Train Loss: 5.53391865, overrun_counter 29\n",
      "epoch = 92batch = 0 of 283duraation = 0.054122026761372885\n",
      "epoch = 92batch = 100 of 283duraation = 2.0793250163396197\n",
      "epoch = 92batch = 200 of 283duraation = 4.102921585241954\n",
      "..Overrun....no improvement\n",
      "Epoch: 92, Train Loss: 5.53343836, overrun_counter 30\n",
      "epoch = 93batch = 0 of 283duraation = 0.0582603136698405\n",
      "epoch = 93batch = 100 of 283duraation = 2.0838021794954935\n",
      "epoch = 93batch = 200 of 283duraation = 4.105792152881622\n",
      "..Overrun....no improvement\n",
      "Epoch: 93, Train Loss: 5.53339184, overrun_counter 31\n",
      "epoch = 94batch = 0 of 283duraation = 0.06340009371439616\n",
      "epoch = 94batch = 100 of 283duraation = 2.0875691970189414\n",
      "epoch = 94batch = 200 of 283duraation = 4.111659308274587\n",
      "..Overrun....no improvement\n",
      "Epoch: 94, Train Loss: 5.53321473, overrun_counter 32\n",
      "epoch = 95batch = 0 of 283duraation = 0.049803392092386885\n",
      "epoch = 95batch = 100 of 283duraation = 2.07664076089859\n",
      "epoch = 95batch = 200 of 283duraation = 4.100566097100576\n",
      "..Overrun....no improvement\n",
      "Epoch: 95, Train Loss: 5.53296878, overrun_counter 33\n",
      "epoch = 96batch = 0 of 283duraation = 0.05694100062052409\n",
      "epoch = 96batch = 100 of 283duraation = 2.083758310476939\n",
      "epoch = 96batch = 200 of 283duraation = 4.106661732991537\n",
      "..Overrun....no improvement\n",
      "Epoch: 96, Train Loss: 5.53337163, overrun_counter 34\n",
      "epoch = 97batch = 0 of 283duraation = 0.05493559837341309\n",
      "epoch = 97batch = 100 of 283duraation = 2.0810261686642963\n",
      "epoch = 97batch = 200 of 283duraation = 4.103097275892893\n",
      "..Overrun....no improvement\n",
      "Epoch: 97, Train Loss: 5.53298497, overrun_counter 35\n",
      "epoch = 98batch = 0 of 283duraation = 0.06003926992416382\n",
      "epoch = 98batch = 100 of 283duraation = 2.0833402911822003\n",
      "epoch = 98batch = 200 of 283duraation = 4.106566774845123\n",
      "..Overrun....no improvement\n",
      "Epoch: 98, Train Loss: 5.53278696, overrun_counter 36\n",
      "epoch = 99batch = 0 of 283duraation = 0.05830676158269246\n",
      "epoch = 99batch = 100 of 283duraation = 2.0850463191668194\n",
      "epoch = 99batch = 200 of 283duraation = 4.108116642634074\n",
      "..Overrun....no improvement\n",
      "Epoch: 99, Train Loss: 5.53298667, overrun_counter 37\n",
      "epoch = 100batch = 0 of 283duraation = 0.056917826334635414\n",
      "epoch = 100batch = 100 of 283duraation = 2.082967547575633\n",
      "epoch = 100batch = 200 of 283duraation = 4.105585134029388\n",
      "..Overrun....no improvement\n",
      "Epoch: 100, Train Loss: 5.53258956, overrun_counter 38\n",
      "epoch = 101batch = 0 of 283duraation = 0.05735606749852498\n",
      "epoch = 101batch = 100 of 283duraation = 2.083366350332896\n",
      "epoch = 101batch = 200 of 283duraation = 4.1087632576624555\n",
      "..Overrun....no improvement\n",
      "Epoch: 101, Train Loss: 5.53346482, overrun_counter 39\n",
      "epoch = 102batch = 0 of 283duraation = 0.059313360850016275\n",
      "epoch = 102batch = 100 of 283duraation = 2.0851529677708944\n",
      "epoch = 102batch = 200 of 283duraation = 4.10871471563975\n",
      "..Overrun....no improvement\n",
      "Epoch: 102, Train Loss: 5.53368189, overrun_counter 40\n",
      "epoch = 103batch = 0 of 283duraation = 0.06264702876408895\n",
      "epoch = 103batch = 100 of 283duraation = 2.0894242405891417\n",
      "epoch = 103batch = 200 of 283duraation = 4.114658749103546\n",
      "..Overrun....no improvement\n",
      "Epoch: 103, Train Loss: 5.53269544, overrun_counter 41\n",
      "epoch = 104batch = 0 of 283duraation = 0.05698919693628947\n",
      "epoch = 104batch = 100 of 283duraation = 2.0825270652770995\n",
      "epoch = 104batch = 200 of 283duraation = 4.107951096693674\n",
      "..Overrun....no improvement\n",
      "Epoch: 104, Train Loss: 5.53385501, overrun_counter 42\n",
      "epoch = 105batch = 0 of 283duraation = 0.05355591376622518\n",
      "epoch = 105batch = 100 of 283duraation = 2.080506753921509\n",
      "epoch = 105batch = 200 of 283duraation = 4.103902912139892\n",
      "..Overrun....no improvement\n",
      "Epoch: 105, Train Loss: 5.53301811, overrun_counter 43\n",
      "epoch = 106batch = 0 of 283duraation = 0.058549122015635176\n",
      "epoch = 106batch = 100 of 283duraation = 2.0841171582539877\n",
      "epoch = 106batch = 200 of 283duraation = 4.1077587882677715\n",
      "..Overrun....no improvement\n",
      "Epoch: 106, Train Loss: 5.53254154, overrun_counter 44\n",
      "epoch = 107batch = 0 of 283duraation = 0.06033221483230591\n",
      "epoch = 107batch = 100 of 283duraation = 2.08920787970225\n",
      "epoch = 107batch = 200 of 283duraation = 4.112915309270223\n",
      "..Overrun....no improvement\n",
      "Epoch: 107, Train Loss: 5.53293369, overrun_counter 45\n",
      "epoch = 108batch = 0 of 283duraation = 0.06013437906901042\n",
      "epoch = 108batch = 100 of 283duraation = 2.0861898263295493\n",
      "epoch = 108batch = 200 of 283duraation = 4.109347303708394\n",
      "..Overrun....no improvement\n",
      "Epoch: 108, Train Loss: 5.53275605, overrun_counter 46\n",
      "epoch = 109batch = 0 of 283duraation = 0.0565355658531189\n",
      "epoch = 109batch = 100 of 283duraation = 2.082432496547699\n",
      "epoch = 109batch = 200 of 283duraation = 4.1069182793299355\n",
      "Epoch: 109, Train Loss: 5.53219450, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e109_2023_05_30_05_56_48.pth\n",
      "Current LR = 0.00001382\n",
      "epoch = 110batch = 0 of 283duraation = 0.05576397180557251\n",
      "epoch = 110batch = 100 of 283duraation = 2.0823883056640624\n",
      "epoch = 110batch = 200 of 283duraation = 4.107691224416097\n",
      "..Overrun....no improvement\n",
      "Epoch: 110, Train Loss: 5.53259168, overrun_counter 0\n",
      "epoch = 111batch = 0 of 283duraation = 0.05660327275594076\n",
      "epoch = 111batch = 100 of 283duraation = 2.084341331322988\n",
      "epoch = 111batch = 200 of 283duraation = 4.108527640501658\n",
      "..Overrun....no improvement\n",
      "Epoch: 111, Train Loss: 5.53283448, overrun_counter 1\n",
      "epoch = 112batch = 0 of 283duraation = 0.059790988763173424\n",
      "epoch = 112batch = 100 of 283duraation = 2.0870855768521626\n",
      "epoch = 112batch = 200 of 283duraation = 4.11130405664444\n",
      "..Overrun....no improvement\n",
      "Epoch: 112, Train Loss: 5.53301504, overrun_counter 2\n",
      "epoch = 113batch = 0 of 283duraation = 0.057337812582651776\n",
      "epoch = 113batch = 100 of 283duraation = 2.082245075702667\n",
      "epoch = 113batch = 200 of 283duraation = 4.107726641496023\n",
      "..Overrun....no improvement\n",
      "Epoch: 113, Train Loss: 5.53268800, overrun_counter 3\n",
      "epoch = 114batch = 0 of 283duraation = 0.05684745709101359\n",
      "epoch = 114batch = 100 of 283duraation = 2.081869029998779\n",
      "epoch = 114batch = 200 of 283duraation = 4.1058426141738895\n",
      "..Overrun....no improvement\n",
      "Epoch: 114, Train Loss: 5.53308230, overrun_counter 4\n",
      "epoch = 115batch = 0 of 283duraation = 0.059835290908813475\n",
      "epoch = 115batch = 100 of 283duraation = 2.0868395368258157\n",
      "epoch = 115batch = 200 of 283duraation = 4.11096263329188\n",
      "..Overrun....no improvement\n",
      "Epoch: 115, Train Loss: 5.53267693, overrun_counter 5\n",
      "epoch = 116batch = 0 of 283duraation = 0.06031011740366618\n",
      "epoch = 116batch = 100 of 283duraation = 2.0863670825958254\n",
      "epoch = 116batch = 200 of 283duraation = 4.109664293130239\n",
      "..Overrun....no improvement\n",
      "Epoch: 116, Train Loss: 5.53270023, overrun_counter 6\n",
      "epoch = 117batch = 0 of 283duraation = 0.05440184275309245\n",
      "epoch = 117batch = 100 of 283duraation = 2.080478628476461\n",
      "epoch = 117batch = 200 of 283duraation = 4.104322199026743\n",
      "..Overrun....no improvement\n",
      "Epoch: 117, Train Loss: 5.53322034, overrun_counter 7\n",
      "epoch = 118batch = 0 of 283duraation = 0.06152623891830444\n",
      "epoch = 118batch = 100 of 283duraation = 2.0869837403297424\n",
      "epoch = 118batch = 200 of 283duraation = 4.111222755908966\n",
      "..Overrun....no improvement\n",
      "Epoch: 118, Train Loss: 5.53272538, overrun_counter 8\n",
      "epoch = 119batch = 0 of 283duraation = 0.05740053653717041\n",
      "epoch = 119batch = 100 of 283duraation = 2.0828750252723696\n",
      "epoch = 119batch = 200 of 283duraation = 4.108410608768463\n",
      "..Overrun....no improvement\n",
      "Epoch: 119, Train Loss: 5.53278968, overrun_counter 9\n",
      "epoch = 120batch = 0 of 283duraation = 0.051272785663604735\n",
      "epoch = 120batch = 100 of 283duraation = 2.078692905108134\n",
      "epoch = 120batch = 200 of 283duraation = 4.102070665359497\n",
      "Epoch: 120, Train Loss: 5.53186446, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e120_2023_05_30_07_00_18.pth\n",
      "Current LR = 0.00001358\n",
      "epoch = 121batch = 0 of 283duraation = 0.054960302511850995\n",
      "epoch = 121batch = 100 of 283duraation = 2.0816606720288595\n",
      "epoch = 121batch = 200 of 283duraation = 4.1045940597852075\n",
      "..Overrun....no improvement\n",
      "Epoch: 121, Train Loss: 5.53310562, overrun_counter 0\n",
      "epoch = 122batch = 0 of 283duraation = 0.05640348196029663\n",
      "epoch = 122batch = 100 of 283duraation = 2.0825518012046813\n",
      "epoch = 122batch = 200 of 283duraation = 4.105407782395681\n",
      "..Overrun....no improvement\n",
      "Epoch: 122, Train Loss: 5.53322438, overrun_counter 1\n",
      "epoch = 123batch = 0 of 283duraation = 0.05760526657104492\n",
      "epoch = 123batch = 100 of 283duraation = 2.082645865281423\n",
      "epoch = 123batch = 200 of 283duraation = 4.105013163884481\n",
      "..Overrun....no improvement\n",
      "Epoch: 123, Train Loss: 5.53283656, overrun_counter 2\n",
      "epoch = 124batch = 0 of 283duraation = 0.05801737705866496\n",
      "epoch = 124batch = 100 of 283duraation = 2.0843876163164774\n",
      "epoch = 124batch = 200 of 283duraation = 4.108950833479564\n",
      "..Overrun....no improvement\n",
      "Epoch: 124, Train Loss: 5.53278563, overrun_counter 3\n",
      "epoch = 125batch = 0 of 283duraation = 0.05389941136042277\n",
      "epoch = 125batch = 100 of 283duraation = 2.0791011889775595\n",
      "epoch = 125batch = 200 of 283duraation = 4.103462886810303\n",
      "..Overrun....no improvement\n",
      "Epoch: 125, Train Loss: 5.53277484, overrun_counter 4\n",
      "epoch = 126batch = 0 of 283duraation = 0.05912801027297974\n",
      "epoch = 126batch = 100 of 283duraation = 2.084514653682709\n",
      "epoch = 126batch = 200 of 283duraation = 4.108614834149678\n",
      "..Overrun....no improvement\n",
      "Epoch: 126, Train Loss: 5.53269889, overrun_counter 5\n",
      "epoch = 127batch = 0 of 283duraation = 0.05666963259379069\n",
      "epoch = 127batch = 100 of 283duraation = 2.0827433347702025\n",
      "epoch = 127batch = 200 of 283duraation = 4.105458025137583\n",
      "..Overrun....no improvement\n",
      "Epoch: 127, Train Loss: 5.53318949, overrun_counter 6\n",
      "epoch = 128batch = 0 of 283duraation = 0.05837453206380208\n",
      "epoch = 128batch = 100 of 283duraation = 2.084338092803955\n",
      "epoch = 128batch = 200 of 283duraation = 4.108991122245788\n",
      "..Overrun....no improvement\n",
      "Epoch: 128, Train Loss: 5.53330806, overrun_counter 7\n",
      "epoch = 129batch = 0 of 283duraation = 0.055112016201019284\n",
      "epoch = 129batch = 100 of 283duraation = 2.0818044384320578\n",
      "epoch = 129batch = 200 of 283duraation = 4.105438947677612\n",
      "..Overrun....no improvement\n",
      "Epoch: 129, Train Loss: 5.53299438, overrun_counter 8\n",
      "epoch = 130batch = 0 of 283duraation = 0.058817851543426516\n",
      "epoch = 130batch = 100 of 283duraation = 2.084124143918355\n",
      "epoch = 130batch = 200 of 283duraation = 4.1113938093185425\n",
      "..Overrun....no improvement\n",
      "Epoch: 130, Train Loss: 5.53273812, overrun_counter 9\n",
      "epoch = 131batch = 0 of 283duraation = 0.05185678005218506\n",
      "epoch = 131batch = 100 of 283duraation = 2.0763177355130513\n",
      "epoch = 131batch = 200 of 283duraation = 4.099396030108134\n",
      "..Overrun....no improvement\n",
      "Epoch: 131, Train Loss: 5.53270842, overrun_counter 10\n",
      "epoch = 132batch = 0 of 283duraation = 0.05816283623377482\n",
      "epoch = 132batch = 100 of 283duraation = 2.0838741660118103\n",
      "epoch = 132batch = 200 of 283duraation = 4.108662454287211\n",
      "..Overrun....no improvement\n",
      "Epoch: 132, Train Loss: 5.53279932, overrun_counter 11\n",
      "epoch = 133batch = 0 of 283duraation = 0.05513174533843994\n",
      "epoch = 133batch = 100 of 283duraation = 2.080111491680145\n",
      "epoch = 133batch = 200 of 283duraation = 4.103814073403677\n",
      "..Overrun....no improvement\n",
      "Epoch: 133, Train Loss: 5.53266689, overrun_counter 12\n",
      "epoch = 134batch = 0 of 283duraation = 0.05957545836766561\n",
      "epoch = 134batch = 100 of 283duraation = 2.0855376879374186\n",
      "epoch = 134batch = 200 of 283duraation = 4.108837926387787\n",
      "..Overrun....no improvement\n",
      "Epoch: 134, Train Loss: 5.53292054, overrun_counter 13\n",
      "epoch = 135batch = 0 of 283duraation = 0.05526717106501262\n",
      "epoch = 135batch = 100 of 283duraation = 2.0808611353238424\n",
      "epoch = 135batch = 200 of 283duraation = 4.105334460735321\n",
      "..Overrun....no improvement\n",
      "Epoch: 135, Train Loss: 5.53265091, overrun_counter 14\n",
      "epoch = 136batch = 0 of 283duraation = 0.05764114061991374\n",
      "epoch = 136batch = 100 of 283duraation = 2.084569211800893\n",
      "epoch = 136batch = 200 of 283duraation = 4.108201618989309\n",
      "..Overrun....no improvement\n",
      "Epoch: 136, Train Loss: 5.53245962, overrun_counter 15\n",
      "epoch = 137batch = 0 of 283duraation = 0.056301601727803546\n",
      "epoch = 137batch = 100 of 283duraation = 2.083005058765411\n",
      "epoch = 137batch = 200 of 283duraation = 4.106213772296906\n",
      "..Overrun....no improvement\n",
      "Epoch: 137, Train Loss: 5.53300659, overrun_counter 16\n",
      "epoch = 138batch = 0 of 283duraation = 0.05293815135955811\n",
      "epoch = 138batch = 100 of 283duraation = 2.0788193027178448\n",
      "epoch = 138batch = 200 of 283duraation = 4.104395922025045\n",
      "Epoch: 138, Train Loss: 5.53181554, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e138_2023_05_30_08_44_11.pth\n",
      "Current LR = 0.00001314\n",
      "epoch = 139batch = 0 of 283duraation = 0.05578655401865641\n",
      "epoch = 139batch = 100 of 283duraation = 2.082674765586853\n",
      "epoch = 139batch = 200 of 283duraation = 4.105348511536916\n",
      "..Overrun....no improvement\n",
      "Epoch: 139, Train Loss: 5.53254499, overrun_counter 0\n",
      "epoch = 140batch = 0 of 283duraation = 0.05777120590209961\n",
      "epoch = 140batch = 100 of 283duraation = 2.0839304089546205\n",
      "epoch = 140batch = 200 of 283duraation = 4.10856112241745\n",
      "..Overrun....no improvement\n",
      "Epoch: 140, Train Loss: 5.53263243, overrun_counter 1\n",
      "epoch = 141batch = 0 of 283duraation = 0.05540141264597575\n",
      "epoch = 141batch = 100 of 283duraation = 2.080127811431885\n",
      "epoch = 141batch = 200 of 283duraation = 4.10502948363622\n",
      "..Overrun....no improvement\n",
      "Epoch: 141, Train Loss: 5.53246249, overrun_counter 2\n",
      "epoch = 142batch = 0 of 283duraation = 0.055825360616048175\n",
      "epoch = 142batch = 100 of 283duraation = 2.081114315986633\n",
      "epoch = 142batch = 200 of 283duraation = 4.104606187343597\n",
      "..Overrun....no improvement\n",
      "Epoch: 142, Train Loss: 5.53358056, overrun_counter 3\n",
      "epoch = 143batch = 0 of 283duraation = 0.05762339433034261\n",
      "epoch = 143batch = 100 of 283duraation = 2.0843716422716776\n",
      "epoch = 143batch = 200 of 283duraation = 4.109636437892914\n",
      "..Overrun....no improvement\n",
      "Epoch: 143, Train Loss: 5.53243792, overrun_counter 4\n",
      "epoch = 144batch = 0 of 283duraation = 0.0555533766746521\n",
      "epoch = 144batch = 100 of 283duraation = 2.0821749607721967\n",
      "epoch = 144batch = 200 of 283duraation = 4.106660024325053\n",
      "..Overrun....no improvement\n",
      "Epoch: 144, Train Loss: 5.53276648, overrun_counter 5\n",
      "epoch = 145batch = 0 of 283duraation = 0.059014610449473065\n",
      "epoch = 145batch = 100 of 283duraation = 2.0852019786834717\n",
      "epoch = 145batch = 200 of 283duraation = 4.108664528528849\n",
      "..Overrun....no improvement\n",
      "Epoch: 145, Train Loss: 5.53271683, overrun_counter 6\n",
      "epoch = 146batch = 0 of 283duraation = 0.05402638912200928\n",
      "epoch = 146batch = 100 of 283duraation = 2.079240330060323\n",
      "epoch = 146batch = 200 of 283duraation = 4.103415544827779\n",
      "..Overrun....no improvement\n",
      "Epoch: 146, Train Loss: 5.53184864, overrun_counter 7\n",
      "epoch = 147batch = 0 of 283duraation = 0.05773884852727254\n",
      "epoch = 147batch = 100 of 283duraation = 2.085676038265228\n",
      "epoch = 147batch = 200 of 283duraation = 4.112602436542511\n",
      "..Overrun....no improvement\n",
      "Epoch: 147, Train Loss: 5.53207712, overrun_counter 8\n",
      "epoch = 148batch = 0 of 283duraation = 0.051999310652414955\n",
      "epoch = 148batch = 100 of 283duraation = 2.0793485562006633\n",
      "epoch = 148batch = 200 of 283duraation = 4.102497390906016\n",
      "..Overrun....no improvement\n",
      "Epoch: 148, Train Loss: 5.53205523, overrun_counter 9\n",
      "epoch = 149batch = 0 of 283duraation = 0.058552877108256025\n",
      "epoch = 149batch = 100 of 283duraation = 2.087448271115621\n",
      "epoch = 149batch = 200 of 283duraation = 4.110243968168894\n",
      "..Overrun....no improvement\n",
      "Epoch: 149, Train Loss: 5.53245200, overrun_counter 10\n",
      "epoch = 150batch = 0 of 283duraation = 0.05163246790568034\n",
      "epoch = 150batch = 100 of 283duraation = 2.0777921040852863\n",
      "epoch = 150batch = 200 of 283duraation = 4.102541462580363\n",
      "..Overrun....no improvement\n",
      "Epoch: 150, Train Loss: 5.53212224, overrun_counter 11\n",
      "epoch = 151batch = 0 of 283duraation = 0.06005420287450155\n",
      "epoch = 151batch = 100 of 283duraation = 2.086336394151052\n",
      "epoch = 151batch = 200 of 283duraation = 4.108558547496796\n",
      "Epoch: 151, Train Loss: 5.53098106, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e151_2023_05_30_09_59_14.pth\n",
      "Current LR = 0.00001280\n",
      "epoch = 152batch = 0 of 283duraation = 0.05296872456868489\n",
      "epoch = 152batch = 100 of 283duraation = 2.078813135623932\n",
      "epoch = 152batch = 200 of 283duraation = 4.103595447540283\n",
      "..Overrun....no improvement\n",
      "Epoch: 152, Train Loss: 5.53196125, overrun_counter 0\n",
      "epoch = 153batch = 0 of 283duraation = 0.060542599360148115\n",
      "epoch = 153batch = 100 of 283duraation = 2.087550330162048\n",
      "epoch = 153batch = 200 of 283duraation = 4.113002638022105\n",
      "..Overrun....no improvement\n",
      "Epoch: 153, Train Loss: 5.53223159, overrun_counter 1\n",
      "epoch = 154batch = 0 of 283duraation = 0.0539335568745931\n",
      "epoch = 154batch = 100 of 283duraation = 2.0812385121981305\n",
      "epoch = 154batch = 200 of 283duraation = 4.105514868100484\n",
      "..Overrun....no improvement\n",
      "Epoch: 154, Train Loss: 5.53205828, overrun_counter 2\n",
      "epoch = 155batch = 0 of 283duraation = 0.05768880844116211\n",
      "epoch = 155batch = 100 of 283duraation = 2.0846969922383627\n",
      "epoch = 155batch = 200 of 283duraation = 4.108628459771475\n",
      "..Overrun....no improvement\n",
      "Epoch: 155, Train Loss: 5.53134464, overrun_counter 3\n",
      "epoch = 156batch = 0 of 283duraation = 0.05379438797632853\n",
      "epoch = 156batch = 100 of 283duraation = 2.078857429822286\n",
      "epoch = 156batch = 200 of 283duraation = 4.102541331450144\n",
      "..Overrun....no improvement\n",
      "Epoch: 156, Train Loss: 5.53181955, overrun_counter 4\n",
      "epoch = 157batch = 0 of 283duraation = 0.06058950026830037\n",
      "epoch = 157batch = 100 of 283duraation = 2.0870318611462912\n",
      "epoch = 157batch = 200 of 283duraation = 4.1118557214736935\n",
      "Epoch: 157, Train Loss: 5.53086791, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e157_2023_05_30_10_33_52.pth\n",
      "Current LR = 0.00001263\n",
      "epoch = 158batch = 0 of 283duraation = 0.05633754332860311\n",
      "epoch = 158batch = 100 of 283duraation = 2.0809375802675882\n",
      "epoch = 158batch = 200 of 283duraation = 4.104062843322754\n",
      "..Overrun....no improvement\n",
      "Epoch: 158, Train Loss: 5.53250036, overrun_counter 0\n",
      "epoch = 159batch = 0 of 283duraation = 0.0583282470703125\n",
      "epoch = 159batch = 100 of 283duraation = 2.0847682555516562\n",
      "epoch = 159batch = 200 of 283duraation = 4.109807821114858\n",
      "..Overrun....no improvement\n",
      "Epoch: 159, Train Loss: 5.53169963, overrun_counter 1\n",
      "epoch = 160batch = 0 of 283duraation = 0.06164209445317586\n",
      "epoch = 160batch = 100 of 283duraation = 2.0886839707692464\n",
      "epoch = 160batch = 200 of 283duraation = 4.111377509435018\n",
      "..Overrun....no improvement\n",
      "Epoch: 160, Train Loss: 5.53248748, overrun_counter 2\n",
      "epoch = 161batch = 0 of 283duraation = 0.05775471925735474\n",
      "epoch = 161batch = 100 of 283duraation = 2.0839250961939495\n",
      "epoch = 161batch = 200 of 283duraation = 4.107757389545441\n",
      "..Overrun....no improvement\n",
      "Epoch: 161, Train Loss: 5.53279406, overrun_counter 3\n",
      "epoch = 162batch = 0 of 283duraation = 0.06104228893915812\n",
      "epoch = 162batch = 100 of 283duraation = 2.0866500536600747\n",
      "epoch = 162batch = 200 of 283duraation = 4.109694349765777\n",
      "..Overrun....no improvement\n",
      "Epoch: 162, Train Loss: 5.53202824, overrun_counter 4\n",
      "epoch = 163batch = 0 of 283duraation = 0.05815204381942749\n",
      "epoch = 163batch = 100 of 283duraation = 2.084044090906779\n",
      "epoch = 163batch = 200 of 283duraation = 4.107538934548696\n",
      "..Overrun....no improvement\n",
      "Epoch: 163, Train Loss: 5.53275418, overrun_counter 5\n",
      "epoch = 164batch = 0 of 283duraation = 0.05803256432215373\n",
      "epoch = 164batch = 100 of 283duraation = 2.0834335764249166\n",
      "epoch = 164batch = 200 of 283duraation = 4.106622687975565\n",
      "..Overrun....no improvement\n",
      "Epoch: 164, Train Loss: 5.53307837, overrun_counter 6\n",
      "epoch = 165batch = 0 of 283duraation = 0.059846977392832436\n",
      "epoch = 165batch = 100 of 283duraation = 2.086314074198405\n",
      "epoch = 165batch = 200 of 283duraation = 4.110465796788533\n",
      "..Overrun....no improvement\n",
      "Epoch: 165, Train Loss: 5.53297562, overrun_counter 7\n",
      "epoch = 166batch = 0 of 283duraation = 0.05848085085550944\n",
      "epoch = 166batch = 100 of 283duraation = 2.084225646654765\n",
      "epoch = 166batch = 200 of 283duraation = 4.107946300506592\n",
      "..Overrun....no improvement\n",
      "Epoch: 166, Train Loss: 5.53369396, overrun_counter 8\n",
      "epoch = 167batch = 0 of 283duraation = 0.056477248668670654\n",
      "epoch = 167batch = 100 of 283duraation = 2.082905701796214\n",
      "epoch = 167batch = 200 of 283duraation = 4.106810037295023\n",
      "..Overrun....no improvement\n",
      "Epoch: 167, Train Loss: 5.53383955, overrun_counter 9\n",
      "epoch = 168batch = 0 of 283duraation = 0.055861679712931316\n",
      "epoch = 168batch = 100 of 283duraation = 2.082053569952647\n",
      "epoch = 168batch = 200 of 283duraation = 4.106305746237437\n",
      "..Overrun....no improvement\n",
      "Epoch: 168, Train Loss: 5.53351532, overrun_counter 10\n",
      "epoch = 169batch = 0 of 283duraation = 0.05978024403254191\n",
      "epoch = 169batch = 100 of 283duraation = 2.086749792098999\n",
      "epoch = 169batch = 200 of 283duraation = 4.109352966149648\n",
      "..Overrun....no improvement\n",
      "Epoch: 169, Train Loss: 5.53309482, overrun_counter 11\n",
      "epoch = 170batch = 0 of 283duraation = 0.05416359901428223\n",
      "epoch = 170batch = 100 of 283duraation = 2.080316468079885\n",
      "epoch = 170batch = 200 of 283duraation = 4.103452773888906\n",
      "..Overrun....no improvement\n",
      "Epoch: 170, Train Loss: 5.53253344, overrun_counter 12\n",
      "epoch = 171batch = 0 of 283duraation = 0.054629882176717125\n",
      "epoch = 171batch = 100 of 283duraation = 2.0818551818529767\n",
      "epoch = 171batch = 200 of 283duraation = 4.105825734138489\n",
      "..Overrun....no improvement\n",
      "Epoch: 171, Train Loss: 5.53340547, overrun_counter 13\n",
      "epoch = 172batch = 0 of 283duraation = 0.05842939217885335\n",
      "epoch = 172batch = 100 of 283duraation = 2.083822286128998\n",
      "epoch = 172batch = 200 of 283duraation = 4.108630009492239\n",
      "..Overrun....no improvement\n",
      "Epoch: 172, Train Loss: 5.53239735, overrun_counter 14\n",
      "epoch = 173batch = 0 of 283duraation = 0.05692571401596069\n",
      "epoch = 173batch = 100 of 283duraation = 2.0823294798533123\n",
      "epoch = 173batch = 200 of 283duraation = 4.105725328127543\n",
      "..Overrun....no improvement\n",
      "Epoch: 173, Train Loss: 5.53204171, overrun_counter 15\n",
      "epoch = 174batch = 0 of 283duraation = 0.05635711749394735\n",
      "epoch = 174batch = 100 of 283duraation = 2.083787421385447\n",
      "epoch = 174batch = 200 of 283duraation = 4.108055865764618\n",
      "..Overrun....no improvement\n",
      "Epoch: 174, Train Loss: 5.53181393, overrun_counter 16\n",
      "epoch = 175batch = 0 of 283duraation = 0.056908992926279704\n",
      "epoch = 175batch = 100 of 283duraation = 2.082261685530345\n",
      "epoch = 175batch = 200 of 283duraation = 4.105702296892802\n",
      "..Overrun....no improvement\n",
      "Epoch: 175, Train Loss: 5.53285784, overrun_counter 17\n",
      "epoch = 176batch = 0 of 283duraation = 0.05461620887120565\n",
      "epoch = 176batch = 100 of 283duraation = 2.0806105772654218\n",
      "epoch = 176batch = 200 of 283duraation = 4.1040844321250916\n",
      "..Overrun....no improvement\n",
      "Epoch: 176, Train Loss: 5.53248177, overrun_counter 18\n",
      "epoch = 177batch = 0 of 283duraation = 0.06043541431427002\n",
      "epoch = 177batch = 100 of 283duraation = 2.086039733886719\n",
      "epoch = 177batch = 200 of 283duraation = 4.1093129277229306\n",
      "..Overrun....no improvement\n",
      "Epoch: 177, Train Loss: 5.53231810, overrun_counter 19\n",
      "epoch = 178batch = 0 of 283duraation = 0.059400888284047444\n",
      "epoch = 178batch = 100 of 283duraation = 2.0862112482388815\n",
      "epoch = 178batch = 200 of 283duraation = 4.109571588039398\n",
      "..Overrun....no improvement\n",
      "Epoch: 178, Train Loss: 5.53340265, overrun_counter 20\n",
      "epoch = 179batch = 0 of 283duraation = 0.055148681004842125\n",
      "epoch = 179batch = 100 of 283duraation = 2.079798702398936\n",
      "epoch = 179batch = 200 of 283duraation = 4.105163470904032\n",
      "..Overrun....no improvement\n",
      "Epoch: 179, Train Loss: 5.53345290, overrun_counter 21\n",
      "epoch = 180batch = 0 of 283duraation = 0.05413413445154826\n",
      "epoch = 180batch = 100 of 283duraation = 2.0806620995203655\n",
      "epoch = 180batch = 200 of 283duraation = 4.1046027302742\n",
      "..Overrun....no improvement\n",
      "Epoch: 180, Train Loss: 5.53322962, overrun_counter 22\n",
      "epoch = 181batch = 0 of 283duraation = 0.057635295391082766\n",
      "epoch = 181batch = 100 of 283duraation = 2.085309104124705\n",
      "epoch = 181batch = 200 of 283duraation = 4.1106640537579855\n",
      "..Overrun....no improvement\n",
      "Epoch: 181, Train Loss: 5.53260384, overrun_counter 23\n",
      "epoch = 182batch = 0 of 283duraation = 0.054836455980936685\n",
      "epoch = 182batch = 100 of 283duraation = 2.0827541828155516\n",
      "epoch = 182batch = 200 of 283duraation = 4.106057929992676\n",
      "..Overrun....no improvement\n",
      "Epoch: 182, Train Loss: 5.53197931, overrun_counter 24\n",
      "epoch = 183batch = 0 of 283duraation = 0.05821456511815389\n",
      "epoch = 183batch = 100 of 283duraation = 2.082914737860362\n",
      "epoch = 183batch = 200 of 283duraation = 4.106765298048655\n",
      "..Overrun....no improvement\n",
      "Epoch: 183, Train Loss: 5.53368090, overrun_counter 25\n",
      "epoch = 184batch = 0 of 283duraation = 0.05339283943176269\n",
      "epoch = 184batch = 100 of 283duraation = 2.080588893095652\n",
      "epoch = 184batch = 200 of 283duraation = 4.105430161952972\n",
      "..Overrun....no improvement\n",
      "Epoch: 184, Train Loss: 5.53315538, overrun_counter 26\n",
      "epoch = 185batch = 0 of 283duraation = 0.06271502176920572\n",
      "epoch = 185batch = 100 of 283duraation = 2.0908334533373516\n",
      "epoch = 185batch = 200 of 283duraation = 4.11516752243042\n",
      "..Overrun....no improvement\n",
      "Epoch: 185, Train Loss: 5.53224430, overrun_counter 27\n",
      "epoch = 186batch = 0 of 283duraation = 0.051839244365692136\n",
      "epoch = 186batch = 100 of 283duraation = 2.0786041100819905\n",
      "epoch = 186batch = 200 of 283duraation = 4.102201306819916\n",
      "..Overrun....no improvement\n",
      "Epoch: 186, Train Loss: 5.53247293, overrun_counter 28\n",
      "epoch = 187batch = 0 of 283duraation = 0.05445059140523275\n",
      "epoch = 187batch = 100 of 283duraation = 2.0800103982289633\n",
      "epoch = 187batch = 200 of 283duraation = 4.1046020746231076\n",
      "..Overrun....no improvement\n",
      "Epoch: 187, Train Loss: 5.53190534, overrun_counter 29\n",
      "epoch = 188batch = 0 of 283duraation = 0.05823125044504802\n",
      "epoch = 188batch = 100 of 283duraation = 2.0846909165382383\n",
      "epoch = 188batch = 200 of 283duraation = 4.108458681901296\n",
      "..Overrun....no improvement\n",
      "Epoch: 188, Train Loss: 5.53270269, overrun_counter 30\n",
      "epoch = 189batch = 0 of 283duraation = 0.059438542524973555\n",
      "epoch = 189batch = 100 of 283duraation = 2.084484839439392\n",
      "epoch = 189batch = 200 of 283duraation = 4.107756388187409\n",
      "..Overrun....no improvement\n",
      "Epoch: 189, Train Loss: 5.53335675, overrun_counter 31\n",
      "epoch = 190batch = 0 of 283duraation = 0.056585319836934406\n",
      "epoch = 190batch = 100 of 283duraation = 2.0831971883773805\n",
      "epoch = 190batch = 200 of 283duraation = 4.108805123964945\n",
      "..Overrun....no improvement\n",
      "Epoch: 190, Train Loss: 5.53357249, overrun_counter 32\n",
      "epoch = 191batch = 0 of 283duraation = 0.053078047434488934\n",
      "epoch = 191batch = 100 of 283duraation = 2.079342325528463\n",
      "epoch = 191batch = 200 of 283duraation = 4.103864840666453\n",
      "..Overrun....no improvement\n",
      "Epoch: 191, Train Loss: 5.53288620, overrun_counter 33\n",
      "epoch = 192batch = 0 of 283duraation = 0.05496360460917155\n",
      "epoch = 192batch = 100 of 283duraation = 2.0805026133855185\n",
      "epoch = 192batch = 200 of 283duraation = 4.1050862789154055\n",
      "..Overrun....no improvement\n",
      "Epoch: 192, Train Loss: 5.53252837, overrun_counter 34\n",
      "epoch = 193batch = 0 of 283duraation = 0.05553413232167562\n",
      "epoch = 193batch = 100 of 283duraation = 2.0825055718421934\n",
      "epoch = 193batch = 200 of 283duraation = 4.107451403141022\n",
      "..Overrun....no improvement\n",
      "Epoch: 193, Train Loss: 5.53346039, overrun_counter 35\n",
      "epoch = 194batch = 0 of 283duraation = 0.055798049767812094\n",
      "epoch = 194batch = 100 of 283duraation = 2.0818445404370625\n",
      "epoch = 194batch = 200 of 283duraation = 4.108232299486796\n",
      "..Overrun....no improvement\n",
      "Epoch: 194, Train Loss: 5.53318520, overrun_counter 36\n",
      "epoch = 195batch = 0 of 283duraation = 0.05498524904251099\n",
      "epoch = 195batch = 100 of 283duraation = 2.0834168553352357\n",
      "epoch = 195batch = 200 of 283duraation = 4.107491246859232\n",
      "..Overrun....no improvement\n",
      "Epoch: 195, Train Loss: 5.53298556, overrun_counter 37\n",
      "epoch = 196batch = 0 of 283duraation = 0.05567684968312581\n",
      "epoch = 196batch = 100 of 283duraation = 2.0840162714322408\n",
      "epoch = 196batch = 200 of 283duraation = 4.109264175097148\n",
      "..Overrun....no improvement\n",
      "Epoch: 196, Train Loss: 5.53294824, overrun_counter 38\n",
      "epoch = 197batch = 0 of 283duraation = 0.06043533086776733\n",
      "epoch = 197batch = 100 of 283duraation = 2.0879348953564962\n",
      "epoch = 197batch = 200 of 283duraation = 4.112277340888977\n",
      "..Overrun....no improvement\n",
      "Epoch: 197, Train Loss: 5.53129937, overrun_counter 39\n",
      "epoch = 198batch = 0 of 283duraation = 0.052447521686553956\n",
      "epoch = 198batch = 100 of 283duraation = 2.078612438837687\n",
      "epoch = 198batch = 200 of 283duraation = 4.1030410091082254\n",
      "..Overrun....no improvement\n",
      "Epoch: 198, Train Loss: 5.53293168, overrun_counter 40\n",
      "epoch = 199batch = 0 of 283duraation = 0.05588853359222412\n",
      "epoch = 199batch = 100 of 283duraation = 2.0825631578763324\n",
      "epoch = 199batch = 200 of 283duraation = 4.108658464749654\n",
      "..Overrun....no improvement\n",
      "Epoch: 199, Train Loss: 5.53154937, overrun_counter 41\n",
      "epoch = 200batch = 0 of 283duraation = 0.05903229316075643\n",
      "epoch = 200batch = 100 of 283duraation = 2.086178783575694\n",
      "epoch = 200batch = 200 of 283duraation = 4.110749669869741\n",
      "..Overrun....no improvement\n",
      "Epoch: 200, Train Loss: 5.53154438, overrun_counter 42\n",
      "epoch = 201batch = 0 of 283duraation = 0.05948104063669841\n",
      "epoch = 201batch = 100 of 283duraation = 2.0882424314816794\n",
      "epoch = 201batch = 200 of 283duraation = 4.113305469353993\n",
      "..Overrun....no improvement\n",
      "Epoch: 201, Train Loss: 5.53185415, overrun_counter 43\n",
      "epoch = 202batch = 0 of 283duraation = 0.057679057121276855\n",
      "epoch = 202batch = 100 of 283duraation = 2.0841383298238116\n",
      "epoch = 202batch = 200 of 283duraation = 4.109494797388712\n",
      "..Overrun....no improvement\n",
      "Epoch: 202, Train Loss: 5.53224777, overrun_counter 44\n",
      "epoch = 203batch = 0 of 283duraation = 0.06078102986017863\n",
      "epoch = 203batch = 100 of 283duraation = 2.0864484190940855\n",
      "epoch = 203batch = 200 of 283duraation = 4.112398461500804\n",
      "..Overrun....no improvement\n",
      "Epoch: 203, Train Loss: 5.53203246, overrun_counter 45\n",
      "epoch = 204batch = 0 of 283duraation = 0.05779596567153931\n",
      "epoch = 204batch = 100 of 283duraation = 2.084912359714508\n",
      "epoch = 204batch = 200 of 283duraation = 4.1106648723284405\n",
      "..Overrun....no improvement\n",
      "Epoch: 204, Train Loss: 5.53286297, overrun_counter 46\n",
      "epoch = 205batch = 0 of 283duraation = 0.05659123659133911\n",
      "epoch = 205batch = 100 of 283duraation = 2.083840584754944\n",
      "epoch = 205batch = 200 of 283duraation = 4.1094651619593305\n",
      "..Overrun....no improvement\n",
      "Epoch: 205, Train Loss: 5.53244766, overrun_counter 47\n",
      "epoch = 206batch = 0 of 283duraation = 0.0544382373491923\n",
      "epoch = 206batch = 100 of 283duraation = 2.080537434418996\n",
      "epoch = 206batch = 200 of 283duraation = 4.106098997592926\n",
      "..Overrun....no improvement\n",
      "Epoch: 206, Train Loss: 5.53179921, overrun_counter 48\n",
      "epoch = 207batch = 0 of 283duraation = 0.05947595040003459\n",
      "epoch = 207batch = 100 of 283duraation = 2.0848668813705444\n",
      "epoch = 207batch = 200 of 283duraation = 4.1090172290802\n",
      "..Overrun....no improvement\n",
      "Epoch: 207, Train Loss: 5.53269243, overrun_counter 49\n",
      "epoch = 208batch = 0 of 283duraation = 0.05675975879033406\n",
      "epoch = 208batch = 100 of 283duraation = 2.0824994921684263\n",
      "epoch = 208batch = 200 of 283duraation = 4.1076465527216595\n",
      "..Overrun....no improvement\n",
      "Epoch: 208, Train Loss: 5.53307869, overrun_counter 50\n",
      "epoch = 209batch = 0 of 283duraation = 0.05660764376322428\n",
      "epoch = 209batch = 100 of 283duraation = 2.08464693625768\n",
      "epoch = 209batch = 200 of 283duraation = 4.109504866600036\n",
      "..Overrun....no improvement\n",
      "Epoch: 209, Train Loss: 5.53272326, overrun_counter 51\n",
      "epoch = 210batch = 0 of 283duraation = 0.05678311983744303\n",
      "epoch = 210batch = 100 of 283duraation = 2.0849554340044656\n",
      "epoch = 210batch = 200 of 283duraation = 4.109356109301249\n",
      "..Overrun....no improvement\n",
      "Epoch: 210, Train Loss: 5.53263825, overrun_counter 52\n",
      "epoch = 211batch = 0 of 283duraation = 0.05745753049850464\n",
      "epoch = 211batch = 100 of 283duraation = 2.0849473396937053\n",
      "epoch = 211batch = 200 of 283duraation = 4.110523267587026\n",
      "..Overrun....no improvement\n",
      "Epoch: 211, Train Loss: 5.53166751, overrun_counter 53\n",
      "epoch = 212batch = 0 of 283duraation = 0.05705188512802124\n",
      "epoch = 212batch = 100 of 283duraation = 2.084804344177246\n",
      "epoch = 212batch = 200 of 283duraation = 4.110013242562612\n",
      "..Overrun....no improvement\n",
      "Epoch: 212, Train Loss: 5.53204522, overrun_counter 54\n",
      "epoch = 213batch = 0 of 283duraation = 0.05711454153060913\n",
      "epoch = 213batch = 100 of 283duraation = 2.086168865362803\n",
      "epoch = 213batch = 200 of 283duraation = 4.111574943860372\n",
      "..Overrun....no improvement\n",
      "Epoch: 213, Train Loss: 5.53260419, overrun_counter 55\n",
      "epoch = 214batch = 0 of 283duraation = 0.057218666871388754\n",
      "epoch = 214batch = 100 of 283duraation = 2.0816955884297688\n",
      "epoch = 214batch = 200 of 283duraation = 4.107344206174215\n",
      "..Overrun....no improvement\n",
      "Epoch: 214, Train Loss: 5.53206305, overrun_counter 56\n",
      "epoch = 215batch = 0 of 283duraation = 0.05802706480026245\n",
      "epoch = 215batch = 100 of 283duraation = 2.0832630515098574\n",
      "epoch = 215batch = 200 of 283duraation = 4.108813126881917\n",
      "..Overrun....no improvement\n",
      "Epoch: 215, Train Loss: 5.53145005, overrun_counter 57\n",
      "epoch = 216batch = 0 of 283duraation = 0.05909312168757121\n",
      "epoch = 216batch = 100 of 283duraation = 2.0888423204421995\n",
      "epoch = 216batch = 200 of 283duraation = 4.116191061337789\n",
      "..Overrun....no improvement\n",
      "Epoch: 216, Train Loss: 5.53206652, overrun_counter 58\n",
      "epoch = 217batch = 0 of 283duraation = 0.05866999626159668\n",
      "epoch = 217batch = 100 of 283duraation = 2.0903217792510986\n",
      "epoch = 217batch = 200 of 283duraation = 4.119801417986552\n",
      "..Overrun....no improvement\n",
      "Epoch: 217, Train Loss: 5.53319139, overrun_counter 59\n",
      "epoch = 218batch = 0 of 283duraation = 0.055996771653493246\n",
      "epoch = 218batch = 100 of 283duraation = 2.0889978329340617\n",
      "epoch = 218batch = 200 of 283duraation = 4.118377280235291\n",
      "..Overrun....no improvement\n",
      "Epoch: 218, Train Loss: 5.53226437, overrun_counter 60\n",
      "epoch = 219batch = 0 of 283duraation = 0.06128172477086385\n",
      "epoch = 219batch = 100 of 283duraation = 2.092611809571584\n",
      "epoch = 219batch = 200 of 283duraation = 4.118847139676412\n",
      "..Overrun....no improvement\n",
      "Epoch: 219, Train Loss: 5.53438673, overrun_counter 61\n",
      "epoch = 220batch = 0 of 283duraation = 0.05687205791473389\n",
      "epoch = 220batch = 100 of 283duraation = 2.0831552505493165\n",
      "epoch = 220batch = 200 of 283duraation = 4.107692654927572\n",
      "..Overrun....no improvement\n",
      "Epoch: 220, Train Loss: 5.53352169, overrun_counter 62\n",
      "epoch = 221batch = 0 of 283duraation = 0.052470052242279054\n",
      "epoch = 221batch = 100 of 283duraation = 2.081425201892853\n",
      "epoch = 221batch = 200 of 283duraation = 4.105986456076304\n",
      "..Overrun....no improvement\n",
      "Epoch: 221, Train Loss: 5.53167940, overrun_counter 63\n",
      "epoch = 222batch = 0 of 283duraation = 0.056841512521107994\n",
      "epoch = 222batch = 100 of 283duraation = 2.0836044748624167\n",
      "epoch = 222batch = 200 of 283duraation = 4.107698094844818\n",
      "..Overrun....no improvement\n",
      "Epoch: 222, Train Loss: 5.53240603, overrun_counter 64\n",
      "epoch = 223batch = 0 of 283duraation = 0.055492130915323894\n",
      "epoch = 223batch = 100 of 283duraation = 2.0807121276855467\n",
      "epoch = 223batch = 200 of 283duraation = 4.107195520401001\n",
      "..Overrun....no improvement\n",
      "Epoch: 223, Train Loss: 5.53215675, overrun_counter 65\n",
      "epoch = 224batch = 0 of 283duraation = 0.05603112777074178\n",
      "epoch = 224batch = 100 of 283duraation = 2.0820383151372273\n",
      "epoch = 224batch = 200 of 283duraation = 4.106754930814107\n",
      "..Overrun....no improvement\n",
      "Epoch: 224, Train Loss: 5.53203643, overrun_counter 66\n",
      "epoch = 225batch = 0 of 283duraation = 0.05970145463943481\n",
      "epoch = 225batch = 100 of 283duraation = 2.0851648092269897\n",
      "epoch = 225batch = 200 of 283duraation = 4.10904506444931\n",
      "..Overrun....no improvement\n",
      "Epoch: 225, Train Loss: 5.53139073, overrun_counter 67\n",
      "epoch = 226batch = 0 of 283duraation = 0.05667071342468262\n",
      "epoch = 226batch = 100 of 283duraation = 2.0823936263720193\n",
      "epoch = 226batch = 200 of 283duraation = 4.106388421853383\n",
      "Epoch: 226, Train Loss: 5.53063681, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e226_2023_05_30_17_12_14.pth\n",
      "Current LR = 0.00001038\n",
      "epoch = 227batch = 0 of 283duraation = 0.055964183807373044\n",
      "epoch = 227batch = 100 of 283duraation = 2.083647529284159\n",
      "epoch = 227batch = 200 of 283duraation = 4.106565364201864\n",
      "..Overrun....no improvement\n",
      "Epoch: 227, Train Loss: 5.53119049, overrun_counter 0\n",
      "epoch = 228batch = 0 of 283duraation = 0.061027499039967854\n",
      "epoch = 228batch = 100 of 283duraation = 2.0880308628082274\n",
      "epoch = 228batch = 200 of 283duraation = 4.112241725126903\n",
      "..Overrun....no improvement\n",
      "Epoch: 228, Train Loss: 5.53066131, overrun_counter 1\n",
      "epoch = 229batch = 0 of 283duraation = 0.0594019889831543\n",
      "epoch = 229batch = 100 of 283duraation = 2.086218253771464\n",
      "epoch = 229batch = 200 of 283duraation = 4.10944344997406\n",
      "Epoch: 229, Train Loss: 5.53034537, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e229_2023_05_30_17_29_34.pth\n",
      "Current LR = 0.00001027\n",
      "epoch = 230batch = 0 of 283duraation = 0.05451620022455851\n",
      "epoch = 230batch = 100 of 283duraation = 2.0805875579516093\n",
      "epoch = 230batch = 200 of 283duraation = 4.104551780223846\n",
      "..Overrun....no improvement\n",
      "Epoch: 230, Train Loss: 5.53050541, overrun_counter 0\n",
      "epoch = 231batch = 0 of 283duraation = 0.051836768786112465\n",
      "epoch = 231batch = 100 of 283duraation = 2.0787742932637534\n",
      "epoch = 231batch = 200 of 283duraation = 4.10205751657486\n",
      "Epoch: 231, Train Loss: 5.53021247, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e231_2023_05_30_17_41_07.pth\n",
      "Current LR = 0.00001020\n",
      "epoch = 232batch = 0 of 283duraation = 0.05501750310262044\n",
      "epoch = 232batch = 100 of 283duraation = 2.080770413080851\n",
      "epoch = 232batch = 200 of 283duraation = 4.105825916926066\n",
      "..Overrun....no improvement\n",
      "Epoch: 232, Train Loss: 5.53044103, overrun_counter 0\n",
      "epoch = 233batch = 0 of 283duraation = 0.05560235579808553\n",
      "epoch = 233batch = 100 of 283duraation = 2.0835928678512574\n",
      "epoch = 233batch = 200 of 283duraation = 4.108186360200246\n",
      "..Overrun....no improvement\n",
      "Epoch: 233, Train Loss: 5.53087199, overrun_counter 1\n",
      "epoch = 234batch = 0 of 283duraation = 0.06025426785151164\n",
      "epoch = 234batch = 100 of 283duraation = 2.0874508460362753\n",
      "epoch = 234batch = 200 of 283duraation = 4.111475495497386\n",
      "..Overrun....no improvement\n",
      "Epoch: 234, Train Loss: 5.53022144, overrun_counter 2\n",
      "epoch = 235batch = 0 of 283duraation = 0.05318676233291626\n",
      "epoch = 235batch = 100 of 283duraation = 2.0806229869524637\n",
      "epoch = 235batch = 200 of 283duraation = 4.103249828020732\n",
      "..Overrun....no improvement\n",
      "Epoch: 235, Train Loss: 5.53030087, overrun_counter 3\n",
      "epoch = 236batch = 0 of 283duraation = 0.056151787439982094\n",
      "epoch = 236batch = 100 of 283duraation = 2.082212559382121\n",
      "epoch = 236batch = 200 of 283duraation = 4.106668559710185\n",
      "Epoch: 236, Train Loss: 5.52942482, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e236_2023_05_30_18_09_59.pth\n",
      "Current LR = 0.00001002\n",
      "epoch = 237batch = 0 of 283duraation = 0.06200878222783406\n",
      "epoch = 237batch = 100 of 283duraation = 2.0886205712954204\n",
      "epoch = 237batch = 200 of 283duraation = 4.112087619304657\n",
      "..Overrun....no improvement\n",
      "Epoch: 237, Train Loss: 5.53012617, overrun_counter 0\n",
      "epoch = 238batch = 0 of 283duraation = 0.05657573143641154\n",
      "epoch = 238batch = 100 of 283duraation = 2.084625256061554\n",
      "epoch = 238batch = 200 of 283duraation = 4.108181150754293\n",
      "..Overrun....no improvement\n",
      "Epoch: 238, Train Loss: 5.53102010, overrun_counter 1\n",
      "epoch = 239batch = 0 of 283duraation = 0.061150376001993814\n",
      "epoch = 239batch = 100 of 283duraation = 2.086616373062134\n",
      "epoch = 239batch = 200 of 283duraation = 4.110373671849569\n",
      "..Overrun....no improvement\n",
      "Epoch: 239, Train Loss: 5.53064103, overrun_counter 2\n",
      "epoch = 240batch = 0 of 283duraation = 0.061016209920247394\n",
      "epoch = 240batch = 100 of 283duraation = 2.087367085615794\n",
      "epoch = 240batch = 200 of 283duraation = 4.112643202145894\n",
      "Epoch: 240, Train Loss: 5.52929146, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e240_2023_05_30_18_33_06.pth\n",
      "Current LR = 0.00000987\n",
      "epoch = 241batch = 0 of 283duraation = 0.05882457892100016\n",
      "epoch = 241batch = 100 of 283duraation = 2.084934457143148\n",
      "epoch = 241batch = 200 of 283duraation = 4.108965194225311\n",
      "..Overrun....no improvement\n",
      "Epoch: 241, Train Loss: 5.53011732, overrun_counter 0\n",
      "epoch = 242batch = 0 of 283duraation = 0.05574152866999308\n",
      "epoch = 242batch = 100 of 283duraation = 2.0821412483851116\n",
      "epoch = 242batch = 200 of 283duraation = 4.107308332125346\n",
      "..Overrun....no improvement\n",
      "Epoch: 242, Train Loss: 5.52944647, overrun_counter 1\n",
      "epoch = 243batch = 0 of 283duraation = 0.05147789319356282\n",
      "epoch = 243batch = 100 of 283duraation = 2.078530470530192\n",
      "epoch = 243batch = 200 of 283duraation = 4.105315395196279\n",
      "..Overrun....no improvement\n",
      "Epoch: 243, Train Loss: 5.52929951, overrun_counter 2\n",
      "epoch = 244batch = 0 of 283duraation = 0.06046375036239624\n",
      "epoch = 244batch = 100 of 283duraation = 2.087817597389221\n",
      "epoch = 244batch = 200 of 283duraation = 4.111266974608103\n",
      "..Overrun....no improvement\n",
      "Epoch: 244, Train Loss: 5.52974478, overrun_counter 3\n",
      "epoch = 245batch = 0 of 283duraation = 0.054139697551727296\n",
      "epoch = 245batch = 100 of 283duraation = 2.0816654125849405\n",
      "epoch = 245batch = 200 of 283duraation = 4.1065933068593345\n",
      "..Overrun....no improvement\n",
      "Epoch: 245, Train Loss: 5.53015573, overrun_counter 4\n",
      "epoch = 246batch = 0 of 283duraation = 0.056621678670247394\n",
      "epoch = 246batch = 100 of 283duraation = 2.0835296551386517\n",
      "epoch = 246batch = 200 of 283duraation = 4.108464086055756\n",
      "..Overrun....no improvement\n",
      "Epoch: 246, Train Loss: 5.52970729, overrun_counter 5\n",
      "epoch = 247batch = 0 of 283duraation = 0.05792097250620524\n",
      "epoch = 247batch = 100 of 283duraation = 2.084805234273275\n",
      "epoch = 247batch = 200 of 283duraation = 4.109406598409017\n",
      "..Overrun....no improvement\n",
      "Epoch: 247, Train Loss: 5.53109210, overrun_counter 6\n",
      "epoch = 248batch = 0 of 283duraation = 0.060716331005096436\n",
      "epoch = 248batch = 100 of 283duraation = 2.0882937908172607\n",
      "epoch = 248batch = 200 of 283duraation = 4.114057223002116\n",
      "..Overrun....no improvement\n",
      "Epoch: 248, Train Loss: 5.52986970, overrun_counter 7\n",
      "epoch = 249batch = 0 of 283duraation = 0.05983729362487793\n",
      "epoch = 249batch = 100 of 283duraation = 2.0870779355367026\n",
      "epoch = 249batch = 200 of 283duraation = 4.112047576904297\n",
      "..Overrun....no improvement\n",
      "Epoch: 249, Train Loss: 5.53201440, overrun_counter 8\n",
      "epoch = 250batch = 0 of 283duraation = 0.05130271514256795\n",
      "epoch = 250batch = 100 of 283duraation = 2.0799244324366253\n",
      "epoch = 250batch = 200 of 283duraation = 4.104021843274435\n",
      "..Overrun....no improvement\n",
      "Epoch: 250, Train Loss: 5.53121389, overrun_counter 9\n",
      "epoch = 251batch = 0 of 283duraation = 0.05922274986902873\n",
      "epoch = 251batch = 100 of 283duraation = 2.085354804992676\n",
      "epoch = 251batch = 200 of 283duraation = 4.110115377108256\n",
      "..Overrun....no improvement\n",
      "Epoch: 251, Train Loss: 5.53049430, overrun_counter 10\n",
      "epoch = 252batch = 0 of 283duraation = 0.05969889561335246\n",
      "epoch = 252batch = 100 of 283duraation = 2.0868740042050677\n",
      "epoch = 252batch = 200 of 283duraation = 4.112538131078084\n",
      "..Overrun....no improvement\n",
      "Epoch: 252, Train Loss: 5.53363787, overrun_counter 11\n",
      "epoch = 253batch = 0 of 283duraation = 0.05698202053705851\n",
      "epoch = 253batch = 100 of 283duraation = 2.084573769569397\n",
      "epoch = 253batch = 200 of 283duraation = 4.108304723103841\n",
      "..Overrun....no improvement\n",
      "Epoch: 253, Train Loss: 5.53037433, overrun_counter 12\n",
      "epoch = 254batch = 0 of 283duraation = 0.05530902942021688\n",
      "epoch = 254batch = 100 of 283duraation = 2.082086988290151\n",
      "epoch = 254batch = 200 of 283duraation = 4.106571547190348\n",
      "..Overrun....no improvement\n",
      "Epoch: 254, Train Loss: 5.53137408, overrun_counter 13\n",
      "epoch = 255batch = 0 of 283duraation = 0.05324010848999024\n",
      "epoch = 255batch = 100 of 283duraation = 2.080312160650889\n",
      "epoch = 255batch = 200 of 283duraation = 4.1038426915804544\n",
      "..Overrun....no improvement\n",
      "Epoch: 255, Train Loss: 5.53132995, overrun_counter 14\n",
      "epoch = 256batch = 0 of 283duraation = 0.05213913917541504\n",
      "epoch = 256batch = 100 of 283duraation = 2.0776582320531207\n",
      "epoch = 256batch = 200 of 283duraation = 4.102355027198792\n",
      "..Overrun....no improvement\n",
      "Epoch: 256, Train Loss: 5.53093896, overrun_counter 15\n",
      "epoch = 257batch = 0 of 283duraation = 0.05418890714645386\n",
      "epoch = 257batch = 100 of 283duraation = 2.082568649450938\n",
      "epoch = 257batch = 200 of 283duraation = 4.107101972897848\n",
      "..Overrun....no improvement\n",
      "Epoch: 257, Train Loss: 5.53105560, overrun_counter 16\n",
      "epoch = 258batch = 0 of 283duraation = 0.060805344581604005\n",
      "epoch = 258batch = 100 of 283duraation = 2.0875023007392883\n",
      "epoch = 258batch = 200 of 283duraation = 4.112840569019317\n",
      "..Overrun....no improvement\n",
      "Epoch: 258, Train Loss: 5.53058118, overrun_counter 17\n",
      "epoch = 259batch = 0 of 283duraation = 0.05845377445220947\n",
      "epoch = 259batch = 100 of 283duraation = 2.087924432754517\n",
      "epoch = 259batch = 200 of 283duraation = 4.113273787498474\n",
      "..Overrun....no improvement\n",
      "Epoch: 259, Train Loss: 5.53076544, overrun_counter 18\n",
      "epoch = 260batch = 0 of 283duraation = 0.051974260807037355\n",
      "epoch = 260batch = 100 of 283duraation = 2.0800654451052347\n",
      "epoch = 260batch = 200 of 283duraation = 4.105293782552083\n",
      "..Overrun....no improvement\n",
      "Epoch: 260, Train Loss: 5.53101935, overrun_counter 19\n",
      "epoch = 261batch = 0 of 283duraation = 0.0590753714243571\n",
      "epoch = 261batch = 100 of 283duraation = 2.089218831062317\n",
      "epoch = 261batch = 200 of 283duraation = 4.114444696903229\n",
      "..Overrun....no improvement\n",
      "Epoch: 261, Train Loss: 5.53146066, overrun_counter 20\n",
      "epoch = 262batch = 0 of 283duraation = 0.05132506688435872\n",
      "epoch = 262batch = 100 of 283duraation = 2.0771348079045615\n",
      "epoch = 262batch = 200 of 283duraation = 4.101750878492991\n",
      "..Overrun....no improvement\n",
      "Epoch: 262, Train Loss: 5.53320741, overrun_counter 21\n",
      "epoch = 263batch = 0 of 283duraation = 0.05612938404083252\n",
      "epoch = 263batch = 100 of 283duraation = 2.083218777179718\n",
      "epoch = 263batch = 200 of 283duraation = 4.110973314444224\n",
      "..Overrun....no improvement\n",
      "Epoch: 263, Train Loss: 5.53171401, overrun_counter 22\n",
      "epoch = 264batch = 0 of 283duraation = 0.05621749957402547\n",
      "epoch = 264batch = 100 of 283duraation = 2.083340418338776\n",
      "epoch = 264batch = 200 of 283duraation = 4.112225564320882\n",
      "..Overrun....no improvement\n",
      "Epoch: 264, Train Loss: 5.53227899, overrun_counter 23\n",
      "epoch = 265batch = 0 of 283duraation = 0.05418194929758708\n",
      "epoch = 265batch = 100 of 283duraation = 2.0826349298159283\n",
      "epoch = 265batch = 200 of 283duraation = 4.109865824381511\n",
      "..Overrun....no improvement\n",
      "Epoch: 265, Train Loss: 5.53252927, overrun_counter 24\n",
      "epoch = 266batch = 0 of 283duraation = 0.06300870180130005\n",
      "epoch = 266batch = 100 of 283duraation = 2.091716265678406\n",
      "epoch = 266batch = 200 of 283duraation = 4.117543903986613\n",
      "..Overrun....no improvement\n",
      "Epoch: 266, Train Loss: 5.53269845, overrun_counter 25\n",
      "epoch = 267batch = 0 of 283duraation = 0.0662342111269633\n",
      "epoch = 267batch = 100 of 283duraation = 2.09221750497818\n",
      "epoch = 267batch = 200 of 283duraation = 4.118434739112854\n",
      "..Overrun....no improvement\n",
      "Epoch: 267, Train Loss: 5.53233427, overrun_counter 26\n",
      "epoch = 268batch = 0 of 283duraation = 0.060229047139485674\n",
      "epoch = 268batch = 100 of 283duraation = 2.087952963511149\n",
      "epoch = 268batch = 200 of 283duraation = 4.110370914141337\n",
      "..Overrun....no improvement\n",
      "Epoch: 268, Train Loss: 5.53143402, overrun_counter 27\n",
      "epoch = 269batch = 0 of 283duraation = 0.05789476235707601\n",
      "epoch = 269batch = 100 of 283duraation = 2.0836925466855365\n",
      "epoch = 269batch = 200 of 283duraation = 4.106610159079234\n",
      "..Overrun....no improvement\n",
      "Epoch: 269, Train Loss: 5.53270626, overrun_counter 28\n",
      "epoch = 270batch = 0 of 283duraation = 0.05373822848002116\n",
      "epoch = 270batch = 100 of 283duraation = 2.0825311144193015\n",
      "epoch = 270batch = 200 of 283duraation = 4.106460718313853\n",
      "..Overrun....no improvement\n",
      "Epoch: 270, Train Loss: 5.53201773, overrun_counter 29\n",
      "epoch = 271batch = 0 of 283duraation = 0.05905667940775553\n",
      "epoch = 271batch = 100 of 283duraation = 2.0851855278015137\n",
      "epoch = 271batch = 200 of 283duraation = 4.111340443293254\n",
      "..Overrun....no improvement\n",
      "Epoch: 271, Train Loss: 5.53154992, overrun_counter 30\n",
      "epoch = 272batch = 0 of 283duraation = 0.054645172754923504\n",
      "epoch = 272batch = 100 of 283duraation = 2.082290029525757\n",
      "epoch = 272batch = 200 of 283duraation = 4.104841045538584\n",
      "..Overrun....no improvement\n",
      "Epoch: 272, Train Loss: 5.53205631, overrun_counter 31\n",
      "epoch = 273batch = 0 of 283duraation = 0.05612535874048869\n",
      "epoch = 273batch = 100 of 283duraation = 2.082655974229177\n",
      "epoch = 273batch = 200 of 283duraation = 4.10750074783961\n",
      "..Overrun....no improvement\n",
      "Epoch: 273, Train Loss: 5.53241884, overrun_counter 32\n",
      "epoch = 274batch = 0 of 283duraation = 0.059648529688517256\n",
      "epoch = 274batch = 100 of 283duraation = 2.086569782098134\n",
      "epoch = 274batch = 200 of 283duraation = 4.1106546719868975\n",
      "..Overrun....no improvement\n",
      "Epoch: 274, Train Loss: 5.53158699, overrun_counter 33\n",
      "epoch = 275batch = 0 of 283duraation = 0.05743484894434611\n",
      "epoch = 275batch = 100 of 283duraation = 2.0835906863212585\n",
      "epoch = 275batch = 200 of 283duraation = 4.106573617458343\n",
      "..Overrun....no improvement\n",
      "Epoch: 275, Train Loss: 5.53219962, overrun_counter 34\n",
      "epoch = 276batch = 0 of 283duraation = 0.05803909699122111\n",
      "epoch = 276batch = 100 of 283duraation = 2.08456959327062\n",
      "epoch = 276batch = 200 of 283duraation = 4.109644349416097\n",
      "..Overrun....no improvement\n",
      "Epoch: 276, Train Loss: 5.52985100, overrun_counter 35\n",
      "epoch = 277batch = 0 of 283duraation = 0.06152100165685018\n",
      "epoch = 277batch = 100 of 283duraation = 2.088532801469167\n",
      "epoch = 277batch = 200 of 283duraation = 4.112250510851542\n",
      "..Overrun....no improvement\n",
      "Epoch: 277, Train Loss: 5.53226650, overrun_counter 36\n",
      "epoch = 278batch = 0 of 283duraation = 0.05394569238026937\n",
      "epoch = 278batch = 100 of 283duraation = 2.0803746501604716\n",
      "epoch = 278batch = 200 of 283duraation = 4.1050788839658106\n",
      "..Overrun....no improvement\n",
      "Epoch: 278, Train Loss: 5.53175847, overrun_counter 37\n",
      "epoch = 279batch = 0 of 283duraation = 0.05852384567260742\n",
      "epoch = 279batch = 100 of 283duraation = 2.0851799408594767\n",
      "epoch = 279batch = 200 of 283duraation = 4.110770527521769\n",
      "..Overrun....no improvement\n",
      "Epoch: 279, Train Loss: 5.53250326, overrun_counter 38\n",
      "epoch = 280batch = 0 of 283duraation = 0.056929461161295575\n",
      "epoch = 280batch = 100 of 283duraation = 2.0834731101989745\n",
      "epoch = 280batch = 200 of 283duraation = 4.10773808558782\n",
      "..Overrun....no improvement\n",
      "Epoch: 280, Train Loss: 5.52984496, overrun_counter 39\n",
      "epoch = 281batch = 0 of 283duraation = 0.05526250998179118\n",
      "epoch = 281batch = 100 of 283duraation = 2.082124110062917\n",
      "epoch = 281batch = 200 of 283duraation = 4.107666099071503\n",
      "..Overrun....no improvement\n",
      "Epoch: 281, Train Loss: 5.53108340, overrun_counter 40\n",
      "epoch = 282batch = 0 of 283duraation = 0.057223864396413165\n",
      "epoch = 282batch = 100 of 283duraation = 2.0837236205736795\n",
      "epoch = 282batch = 200 of 283duraation = 4.109765370686849\n",
      "..Overrun....no improvement\n",
      "Epoch: 282, Train Loss: 5.53100871, overrun_counter 41\n",
      "epoch = 283batch = 0 of 283duraation = 0.05643613338470459\n",
      "epoch = 283batch = 100 of 283duraation = 2.087065780162811\n",
      "epoch = 283batch = 200 of 283duraation = 4.112564611434936\n",
      "..Overrun....no improvement\n",
      "Epoch: 283, Train Loss: 5.53098769, overrun_counter 42\n",
      "epoch = 284batch = 0 of 283duraation = 0.057082609335581465\n",
      "epoch = 284batch = 100 of 283duraation = 2.0834291179974875\n",
      "epoch = 284batch = 200 of 283duraation = 4.108655993143717\n",
      "..Overrun....no improvement\n",
      "Epoch: 284, Train Loss: 5.53001529, overrun_counter 43\n",
      "epoch = 285batch = 0 of 283duraation = 0.05414551496505737\n",
      "epoch = 285batch = 100 of 283duraation = 2.081107715765635\n",
      "epoch = 285batch = 200 of 283duraation = 4.106239048639933\n",
      "..Overrun....no improvement\n",
      "Epoch: 285, Train Loss: 5.53095799, overrun_counter 44\n",
      "epoch = 286batch = 0 of 283duraation = 0.06334628264109293\n",
      "epoch = 286batch = 100 of 283duraation = 2.0870485941569012\n",
      "epoch = 286batch = 200 of 283duraation = 4.109513473510742\n",
      "..Overrun....no improvement\n",
      "Epoch: 286, Train Loss: 5.53088211, overrun_counter 45\n",
      "epoch = 287batch = 0 of 283duraation = 0.06353161732355754\n",
      "epoch = 287batch = 100 of 283duraation = 2.089232810338338\n",
      "epoch = 287batch = 200 of 283duraation = 4.113134471575419\n",
      "..Overrun....no improvement\n",
      "Epoch: 287, Train Loss: 5.53079797, overrun_counter 46\n",
      "epoch = 288batch = 0 of 283duraation = 0.055702730019887285\n",
      "epoch = 288batch = 100 of 283duraation = 2.082924528916677\n",
      "epoch = 288batch = 200 of 283duraation = 4.1085311571757\n",
      "..Overrun....no improvement\n",
      "Epoch: 288, Train Loss: 5.53120214, overrun_counter 47\n",
      "epoch = 289batch = 0 of 283duraation = 0.061858598391215006\n",
      "epoch = 289batch = 100 of 283duraation = 2.0891804893811545\n",
      "epoch = 289batch = 200 of 283duraation = 4.11354722181956\n",
      "..Overrun....no improvement\n",
      "Epoch: 289, Train Loss: 5.53173485, overrun_counter 48\n",
      "epoch = 290batch = 0 of 283duraation = 0.056016568342844644\n",
      "epoch = 290batch = 100 of 283duraation = 2.084254213174184\n",
      "epoch = 290batch = 200 of 283duraation = 4.109872488180796\n",
      "..Overrun....no improvement\n",
      "Epoch: 290, Train Loss: 5.53176363, overrun_counter 49\n",
      "epoch = 291batch = 0 of 283duraation = 0.05652236143747966\n",
      "epoch = 291batch = 100 of 283duraation = 2.0837170759836834\n",
      "epoch = 291batch = 200 of 283duraation = 4.108914295832316\n",
      "..Overrun....no improvement\n",
      "Epoch: 291, Train Loss: 5.53027567, overrun_counter 50\n",
      "epoch = 292batch = 0 of 283duraation = 0.05995675325393677\n",
      "epoch = 292batch = 100 of 283duraation = 2.0892696817715963\n",
      "epoch = 292batch = 200 of 283duraation = 4.113359312216441\n",
      "..Overrun....no improvement\n",
      "Epoch: 292, Train Loss: 5.53107720, overrun_counter 51\n",
      "epoch = 293batch = 0 of 283duraation = 0.057888372739156084\n",
      "epoch = 293batch = 100 of 283duraation = 2.082423973083496\n",
      "epoch = 293batch = 200 of 283duraation = 4.106185944875081\n",
      "..Overrun....no improvement\n",
      "Epoch: 293, Train Loss: 5.53032830, overrun_counter 52\n",
      "epoch = 294batch = 0 of 283duraation = 0.053729859987894694\n",
      "epoch = 294batch = 100 of 283duraation = 2.0810360391934712\n",
      "epoch = 294batch = 200 of 283duraation = 4.1049978574117025\n",
      "..Overrun....no improvement\n",
      "Epoch: 294, Train Loss: 5.52993042, overrun_counter 53\n",
      "epoch = 295batch = 0 of 283duraation = 0.05602820714314779\n",
      "epoch = 295batch = 100 of 283duraation = 2.0826653440793357\n",
      "epoch = 295batch = 200 of 283duraation = 4.106753695011139\n",
      "..Overrun....no improvement\n",
      "Epoch: 295, Train Loss: 5.52982245, overrun_counter 54\n",
      "epoch = 296batch = 0 of 283duraation = 0.052230278650919594\n",
      "epoch = 296batch = 100 of 283duraation = 2.0782240509986876\n",
      "epoch = 296batch = 200 of 283duraation = 4.101992253462473\n",
      "..Overrun....no improvement\n",
      "Epoch: 296, Train Loss: 5.53006661, overrun_counter 55\n",
      "epoch = 297batch = 0 of 283duraation = 0.05494391918182373\n",
      "epoch = 297batch = 100 of 283duraation = 2.081174349784851\n",
      "epoch = 297batch = 200 of 283duraation = 4.106250810623169\n",
      "..Overrun....no improvement\n",
      "Epoch: 297, Train Loss: 5.52936910, overrun_counter 56\n",
      "epoch = 298batch = 0 of 283duraation = 0.05563682715098063\n",
      "epoch = 298batch = 100 of 283duraation = 2.0828621983528137\n",
      "epoch = 298batch = 200 of 283duraation = 4.107108879089355\n",
      "..Overrun....no improvement\n",
      "Epoch: 298, Train Loss: 5.53100211, overrun_counter 57\n",
      "epoch = 299batch = 0 of 283duraation = 0.06312101682027181\n",
      "epoch = 299batch = 100 of 283duraation = 2.091004665692647\n",
      "epoch = 299batch = 200 of 283duraation = 4.1152636885643\n",
      "..Overrun....no improvement\n",
      "Epoch: 299, Train Loss: 5.52930745, overrun_counter 58\n",
      "epoch = 300batch = 0 of 283duraation = 0.059756608804066975\n",
      "epoch = 300batch = 100 of 283duraation = 2.0851242860158283\n",
      "epoch = 300batch = 200 of 283duraation = 4.1090005278587345\n",
      "..Overrun....no improvement\n",
      "Epoch: 300, Train Loss: 5.52991804, overrun_counter 59\n",
      "epoch = 301batch = 0 of 283duraation = 0.060636218388875326\n",
      "epoch = 301batch = 100 of 283duraation = 2.086887590090434\n",
      "epoch = 301batch = 200 of 283duraation = 4.110227247079213\n",
      "..Overrun....no improvement\n",
      "Epoch: 301, Train Loss: 5.53001585, overrun_counter 60\n",
      "epoch = 302batch = 0 of 283duraation = 0.05245641072591146\n",
      "epoch = 302batch = 100 of 283duraation = 2.0800552209218344\n",
      "epoch = 302batch = 200 of 283duraation = 4.103862547874451\n",
      "..Overrun....no improvement\n",
      "Epoch: 302, Train Loss: 5.53088537, overrun_counter 61\n",
      "epoch = 303batch = 0 of 283duraation = 0.057315512498219805\n",
      "epoch = 303batch = 100 of 283duraation = 2.0832003275553386\n",
      "epoch = 303batch = 200 of 283duraation = 4.107557650407156\n",
      "..Overrun....no improvement\n",
      "Epoch: 303, Train Loss: 5.52992491, overrun_counter 62\n",
      "epoch = 304batch = 0 of 283duraation = 0.05956013600031535\n",
      "epoch = 304batch = 100 of 283duraation = 2.08646111090978\n",
      "epoch = 304batch = 200 of 283duraation = 4.112464133898417\n",
      "Epoch: 304, Train Loss: 5.52901404, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e304_2023_05_31_00_42_39.pth\n",
      "Current LR = 0.00000742\n",
      "epoch = 305batch = 0 of 283duraation = 0.05684600671132405\n",
      "epoch = 305batch = 100 of 283duraation = 2.084959840774536\n",
      "epoch = 305batch = 200 of 283duraation = 4.109443895022074\n",
      "Epoch: 305, Train Loss: 5.52881632, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e305_2023_05_31_00_48_26.pth\n",
      "Current LR = 0.00000738\n",
      "epoch = 306batch = 0 of 283duraation = 0.05596485535303752\n",
      "epoch = 306batch = 100 of 283duraation = 2.083579703172048\n",
      "epoch = 306batch = 200 of 283duraation = 4.107048960526784\n",
      "Epoch: 306, Train Loss: 5.52863477, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e306_2023_05_31_00_54_12.pth\n",
      "Current LR = 0.00000734\n",
      "epoch = 307batch = 0 of 283duraation = 0.05552505652109782\n",
      "epoch = 307batch = 100 of 283duraation = 2.082312321662903\n",
      "epoch = 307batch = 200 of 283duraation = 4.104990597565969\n",
      "..Overrun....no improvement\n",
      "Epoch: 307, Train Loss: 5.52934312, overrun_counter 0\n",
      "epoch = 308batch = 0 of 283duraation = 0.059294561545054116\n",
      "epoch = 308batch = 100 of 283duraation = 2.0857461214065554\n",
      "epoch = 308batch = 200 of 283duraation = 4.1108437021573385\n",
      "Epoch: 308, Train Loss: 5.52798975, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e308_2023_05_31_01_05_45.pth\n",
      "Current LR = 0.00000726\n",
      "epoch = 309batch = 0 of 283duraation = 0.05618819793065389\n",
      "epoch = 309batch = 100 of 283duraation = 2.0824642340342203\n",
      "epoch = 309batch = 200 of 283duraation = 4.107793351014455\n",
      "..Overrun....no improvement\n",
      "Epoch: 309, Train Loss: 5.52946806, overrun_counter 0\n",
      "epoch = 310batch = 0 of 283duraation = 0.05814660390218099\n",
      "epoch = 310batch = 100 of 283duraation = 2.085022870699565\n",
      "epoch = 310batch = 200 of 283duraation = 4.109348515669505\n",
      "..Overrun....no improvement\n",
      "Epoch: 310, Train Loss: 5.53009451, overrun_counter 1\n",
      "epoch = 311batch = 0 of 283duraation = 0.059773524602254234\n",
      "epoch = 311batch = 100 of 283duraation = 2.086083209514618\n",
      "epoch = 311batch = 200 of 283duraation = 4.11176229317983\n",
      "..Overrun....no improvement\n",
      "Epoch: 311, Train Loss: 5.52820428, overrun_counter 2\n",
      "epoch = 312batch = 0 of 283duraation = 0.05837229490280151\n",
      "epoch = 312batch = 100 of 283duraation = 2.0848435004552206\n",
      "epoch = 312batch = 200 of 283duraation = 4.108296235402425\n",
      "..Overrun....no improvement\n",
      "Epoch: 312, Train Loss: 5.52834972, overrun_counter 3\n",
      "epoch = 313batch = 0 of 283duraation = 0.051232163111368814\n",
      "epoch = 313batch = 100 of 283duraation = 2.077826186021169\n",
      "epoch = 313batch = 200 of 283duraation = 4.102251660823822\n",
      "..Overrun....no improvement\n",
      "Epoch: 313, Train Loss: 5.52834691, overrun_counter 4\n",
      "epoch = 314batch = 0 of 283duraation = 0.06183179616928101\n",
      "epoch = 314batch = 100 of 283duraation = 2.0880497256914774\n",
      "epoch = 314batch = 200 of 283duraation = 4.110035574436187\n",
      "..Overrun....no improvement\n",
      "Epoch: 314, Train Loss: 5.53019483, overrun_counter 5\n",
      "epoch = 315batch = 0 of 283duraation = 0.05918252865473429\n",
      "epoch = 315batch = 100 of 283duraation = 2.08684534629186\n",
      "epoch = 315batch = 200 of 283duraation = 4.1129618167877195\n",
      "Epoch: 315, Train Loss: 5.52791116, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e315_2023_05_31_01_46_10.pth\n",
      "Current LR = 0.00000699\n",
      "epoch = 316batch = 0 of 283duraation = 0.052406811714172365\n",
      "epoch = 316batch = 100 of 283duraation = 2.079170290629069\n",
      "epoch = 316batch = 200 of 283duraation = 4.101906418800354\n",
      "..Overrun....no improvement\n",
      "Epoch: 316, Train Loss: 5.52964555, overrun_counter 0\n",
      "epoch = 317batch = 0 of 283duraation = 0.05598094463348389\n",
      "epoch = 317batch = 100 of 283duraation = 2.0836705565452576\n",
      "epoch = 317batch = 200 of 283duraation = 4.107014397780101\n",
      "..Overrun....no improvement\n",
      "Epoch: 317, Train Loss: 5.52999943, overrun_counter 1\n",
      "epoch = 318batch = 0 of 283duraation = 0.05520646174748738\n",
      "epoch = 318batch = 100 of 283duraation = 2.0844530701637267\n",
      "epoch = 318batch = 200 of 283duraation = 4.107795385519664\n",
      "..Overrun....no improvement\n",
      "Epoch: 318, Train Loss: 5.52915381, overrun_counter 2\n",
      "epoch = 319batch = 0 of 283duraation = 0.05825705925623576\n",
      "epoch = 319batch = 100 of 283duraation = 2.0837634603182473\n",
      "epoch = 319batch = 200 of 283duraation = 4.1087579528490705\n",
      "..Overrun....no improvement\n",
      "Epoch: 319, Train Loss: 5.52980411, overrun_counter 3\n",
      "epoch = 320batch = 0 of 283duraation = 0.05738900502522786\n",
      "epoch = 320batch = 100 of 283duraation = 2.082421310742696\n",
      "epoch = 320batch = 200 of 283duraation = 4.106949683030447\n",
      "..Overrun....no improvement\n",
      "Epoch: 320, Train Loss: 5.52968515, overrun_counter 4\n",
      "epoch = 321batch = 0 of 283duraation = 0.05468080441157023\n",
      "epoch = 321batch = 100 of 283duraation = 2.0816801031430563\n",
      "epoch = 321batch = 200 of 283duraation = 4.10521901845932\n",
      "..Overrun....no improvement\n",
      "Epoch: 321, Train Loss: 5.53031302, overrun_counter 5\n",
      "epoch = 322batch = 0 of 283duraation = 0.05486534436543782\n",
      "epoch = 322batch = 100 of 283duraation = 2.0809744795163474\n",
      "epoch = 322batch = 200 of 283duraation = 4.103557900587718\n",
      "..Overrun....no improvement\n",
      "Epoch: 322, Train Loss: 5.53100535, overrun_counter 6\n",
      "epoch = 323batch = 0 of 283duraation = 0.055598803361256915\n",
      "epoch = 323batch = 100 of 283duraation = 2.0798024614651998\n",
      "epoch = 323batch = 200 of 283duraation = 4.105368320147196\n",
      "..Overrun....no improvement\n",
      "Epoch: 323, Train Loss: 5.52982364, overrun_counter 7\n",
      "epoch = 324batch = 0 of 283duraation = 0.0546324888865153\n",
      "epoch = 324batch = 100 of 283duraation = 2.081154771645864\n",
      "epoch = 324batch = 200 of 283duraation = 4.105061693986257\n",
      "..Overrun....no improvement\n",
      "Epoch: 324, Train Loss: 5.53037991, overrun_counter 8\n",
      "epoch = 325batch = 0 of 283duraation = 0.055967899163564046\n",
      "epoch = 325batch = 100 of 283duraation = 2.0835542996724445\n",
      "epoch = 325batch = 200 of 283duraation = 4.108120961983999\n",
      "..Overrun....no improvement\n",
      "Epoch: 325, Train Loss: 5.53162066, overrun_counter 9\n",
      "epoch = 326batch = 0 of 283duraation = 0.05488119125366211\n",
      "epoch = 326batch = 100 of 283duraation = 2.083523217837016\n",
      "epoch = 326batch = 200 of 283duraation = 4.106931670506795\n",
      "..Overrun....no improvement\n",
      "Epoch: 326, Train Loss: 5.53137595, overrun_counter 10\n",
      "epoch = 327batch = 0 of 283duraation = 0.05569313367207845\n",
      "epoch = 327batch = 100 of 283duraation = 2.0826457182566327\n",
      "epoch = 327batch = 200 of 283duraation = 4.1084330479304\n",
      "..Overrun....no improvement\n",
      "Epoch: 327, Train Loss: 5.53065808, overrun_counter 11\n",
      "epoch = 328batch = 0 of 283duraation = 0.056876798470815025\n",
      "epoch = 328batch = 100 of 283duraation = 2.0833048105239866\n",
      "epoch = 328batch = 200 of 283duraation = 4.107329897085825\n",
      "..Overrun....no improvement\n",
      "Epoch: 328, Train Loss: 5.53047187, overrun_counter 12\n",
      "epoch = 329batch = 0 of 283duraation = 0.05986870527267456\n",
      "epoch = 329batch = 100 of 283duraation = 2.0848281423250836\n",
      "epoch = 329batch = 200 of 283duraation = 4.109761476516724\n",
      "..Overrun....no improvement\n",
      "Epoch: 329, Train Loss: 5.53016057, overrun_counter 13\n",
      "epoch = 330batch = 0 of 283duraation = 0.054672094186147054\n",
      "epoch = 330batch = 100 of 283duraation = 2.082866112391154\n",
      "epoch = 330batch = 200 of 283duraation = 4.105643765131632\n",
      "..Overrun....no improvement\n",
      "Epoch: 330, Train Loss: 5.53049931, overrun_counter 14\n",
      "epoch = 331batch = 0 of 283duraation = 0.05504811604817708\n",
      "epoch = 331batch = 100 of 283duraation = 2.081374255816142\n",
      "epoch = 331batch = 200 of 283duraation = 4.105145661036174\n",
      "..Overrun....no improvement\n",
      "Epoch: 331, Train Loss: 5.53035497, overrun_counter 15\n",
      "epoch = 332batch = 0 of 283duraation = 0.05164630015691121\n",
      "epoch = 332batch = 100 of 283duraation = 2.078819493452708\n",
      "epoch = 332batch = 200 of 283duraation = 4.102591911951701\n",
      "..Overrun....no improvement\n",
      "Epoch: 332, Train Loss: 5.52948474, overrun_counter 16\n",
      "epoch = 333batch = 0 of 283duraation = 0.057681167125701906\n",
      "epoch = 333batch = 100 of 283duraation = 2.085256497065226\n",
      "epoch = 333batch = 200 of 283duraation = 4.10949068069458\n",
      "..Overrun....no improvement\n",
      "Epoch: 333, Train Loss: 5.52940232, overrun_counter 17\n",
      "epoch = 334batch = 0 of 283duraation = 0.0600263237953186\n",
      "epoch = 334batch = 100 of 283duraation = 2.086245906352997\n",
      "epoch = 334batch = 200 of 283duraation = 4.111063679059346\n",
      "..Overrun....no improvement\n",
      "Epoch: 334, Train Loss: 5.53014324, overrun_counter 18\n",
      "epoch = 335batch = 0 of 283duraation = 0.05227129062016805\n",
      "epoch = 335batch = 100 of 283duraation = 2.0791011810302735\n",
      "epoch = 335batch = 200 of 283duraation = 4.1022547324498495\n",
      "..Overrun....no improvement\n",
      "Epoch: 335, Train Loss: 5.52882687, overrun_counter 19\n",
      "epoch = 336batch = 0 of 283duraation = 0.05715017318725586\n",
      "epoch = 336batch = 100 of 283duraation = 2.0847156763076784\n",
      "epoch = 336batch = 200 of 283duraation = 4.108992842833201\n",
      "..Overrun....no improvement\n",
      "Epoch: 336, Train Loss: 5.53004256, overrun_counter 20\n",
      "epoch = 337batch = 0 of 283duraation = 0.059087395668029785\n",
      "epoch = 337batch = 100 of 283duraation = 2.085928245385488\n",
      "epoch = 337batch = 200 of 283duraation = 4.111330540974935\n",
      "..Overrun....no improvement\n",
      "Epoch: 337, Train Loss: 5.52962757, overrun_counter 21\n",
      "epoch = 338batch = 0 of 283duraation = 0.0592530886332194\n",
      "epoch = 338batch = 100 of 283duraation = 2.0845110058784484\n",
      "epoch = 338batch = 200 of 283duraation = 4.109210201104482\n",
      "..Overrun....no improvement\n",
      "Epoch: 338, Train Loss: 5.52865190, overrun_counter 22\n",
      "epoch = 339batch = 0 of 283duraation = 0.056805121898651126\n",
      "epoch = 339batch = 100 of 283duraation = 2.0824548562367755\n",
      "epoch = 339batch = 200 of 283duraation = 4.106508044401805\n",
      "Epoch: 339, Train Loss: 5.52775235, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e339_2023_05_31_04_04_42.pth\n",
      "Current LR = 0.00000607\n",
      "epoch = 340batch = 0 of 283duraation = 0.05733573039372762\n",
      "epoch = 340batch = 100 of 283duraation = 2.0854551712671916\n",
      "epoch = 340batch = 200 of 283duraation = 4.108585155010223\n",
      "Epoch: 340, Train Loss: 5.52699174, overrun_counter -1\n",
      "Saving model to: /dli/task/ComParE2022_VecNet/models/encoder/model_e340_2023_05_31_04_10_29.pth\n",
      "Current LR = 0.00000603\n",
      "epoch = 341batch = 0 of 283duraation = 0.055568544069925944\n",
      "epoch = 341batch = 100 of 283duraation = 2.083493689695994\n",
      "epoch = 341batch = 200 of 283duraation = 4.107125532627106\n",
      "..Overrun....no improvement\n",
      "Epoch: 341, Train Loss: 5.52821684, overrun_counter 0\n",
      "epoch = 342batch = 0 of 283duraation = 0.057990658283233645\n",
      "epoch = 342batch = 100 of 283duraation = 2.084084101517995\n",
      "epoch = 342batch = 200 of 283duraation = 4.107572623093923\n",
      "..Overrun....no improvement\n",
      "Epoch: 342, Train Loss: 5.52946802, overrun_counter 1\n",
      "epoch = 343batch = 0 of 283duraation = 0.057340017954508465\n",
      "epoch = 343batch = 100 of 283duraation = 2.0832757393519086\n",
      "epoch = 343batch = 200 of 283duraation = 4.108356340726217\n",
      "..Overrun....no improvement\n",
      "Epoch: 343, Train Loss: 5.52878828, overrun_counter 2\n",
      "epoch = 344batch = 0 of 283duraation = 0.0642524520556132\n",
      "epoch = 344batch = 100 of 283duraation = 2.091132060686747\n",
      "epoch = 344batch = 200 of 283duraation = 4.115411241849263\n",
      "..Overrun....no improvement\n",
      "Epoch: 344, Train Loss: 5.52770831, overrun_counter 3\n",
      "epoch = 345batch = 0 of 283duraation = 0.058516331513722736\n",
      "epoch = 345batch = 100 of 283duraation = 2.0855171004931132\n"
     ]
    }
   ],
   "source": [
    "csv_loc = os.path.join(\"..\",\"..\",\"data\",\"metadata\",\"neurips_2021_zenodo_0_0_1.csv\")\n",
    "df = prepare_df(classes = classes,csv_loc = csv_loc)\n",
    "plot_df(df)\n",
    "df_train ,df_val ,df_test = train_test_split(df)\n",
    "print(\"now validating the split post loading and keeping TZ data\")\n",
    "validate_split(df_train ,df_val)\n",
    "validate_split(df_train ,df_test)\n",
    "validate_split(df_test ,df_val)\n",
    "df_train_offset = get_offsets_df(df_train, short_audio=True)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=True)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=True)\n",
    "df_train_offset.reset_index(inplace = True , drop = True)\n",
    "df_test_offset.reset_index(inplace = True , drop = True)\n",
    "df_val_offset.reset_index(inplace = True , drop = True)\n",
    "print(\"now validating the split post offset_creation\")\n",
    "validate_split(df_train_offset ,df_val_offset)\n",
    "validate_split(df_train_offset ,df_test_offset)\n",
    "validate_split(df_test_offset ,df_val_offset)\n",
    "\n",
    "class_weights = get_class_weights(df_train_offset)\n",
    "print(\"inside main. class_weigths type = \", type(class_weights))\n",
    "model =MyModel('convnext_xlarge_in22k',224)\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "\n",
    "train_dataset = MozDataset(df_train_offset,  config.data_dir, min_length)\n",
    "val_dataset = MozDataset(df_val_offset,  config.data_dir, min_length)\n",
    "test_dataset = MozDataset(df_test_offset,  config.data_dir, min_length)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True, pin_memory=True )\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory, shuffle = True )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,shuffle = True)\n",
    "\n",
    "encoder = SupConResNet()\n",
    "\n",
    "#train_loader, val_loader,test_loader, model ,class_weights, classes = classes, num_epochs = num_epochs ,n_channels = 1\n",
    "#train_loader, val_loader,test_loader, model, classes ,df,num_epochs = num_epochs ,n_channels = 1\n",
    "tr_model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1 = train_model(train_loader, val_loader, test_loader,model,classes,class_weights ,num_epochs,encoder )\n",
    "\n",
    "print(\"ALL DONE!!!!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b44129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b36eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28097a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2ad8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
