{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed4974f",
   "metadata": {},
   "source": [
    "### Mofified the initial code to include SpecAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26286d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyterlab in /opt/conda/lib/python3.8/site-packages (4.0.9)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (21.3)\n",
      "Requirement already satisfied: traitlets in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (5.14.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (5.4.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (5.5.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (2.25.2)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (0.2.3)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (2.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (6.8.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (2.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (2.11.1)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (6.3.3)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (3.0.3)\n",
      "Requirement already satisfied: tomli in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (1.2.2)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyterlab) (6.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from async-lru>=1.0.0->jupyterlab) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyterlab) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=3.0.3->jupyterlab) (2.0.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (4.1.0)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.11.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.2)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.6.4)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.4.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (5.9.2)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (25.1.1)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.4.9)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.9.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.12.1)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.4.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (21.1.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab) (3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.8.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core->jupyterlab) (4.0.0)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.7)\n",
      "Requirement already satisfied: jsonschema[format-nongpl]>=4.18.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (4.20.0)\n",
      "Requirement already satisfied: referencing in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.31.0)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.4)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.13.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.10)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (2023.11.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (20.11.0)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.1)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.13)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.4)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (2.13.1)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (2.31.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (0.9.6)\n",
      "Requirement already satisfied: pytz>=2015.7 in /opt/conda/lib/python3.8/site-packages (from babel>=2.10->jupyterlab-server<3,>=2.19.0->jupyterlab) (2021.3)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.2.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.9)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (3.0.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.1.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.10.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.1)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.8/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.19.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->jupyterlab) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab) (2.0.8)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.8/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (2.21)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyterlab) (1.8.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyterlab) (7.30.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyterlab) (5.8.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyterlab) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyterlab) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (3.0.41)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (59.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyterlab) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyterlab) (0.2.5)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.8/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.8/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.8.19.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.1)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (from jupyter) (8.1.1)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.27.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (7.11.0)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.3.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.4.9)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.30.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.14.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (25.1.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (0.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (3.0.41)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (59.4.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyter) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->jupyter) (3.0.9)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->jupyter) (4.0.9)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (6.8.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jinja2>=3.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.10.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.9.2)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=3.6->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.19.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert->jupyter) (4.20.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (0.31.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (2023.11.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (0.13.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (23.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.3)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.2)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.2.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.41)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: audiomentations in /opt/conda/lib/python3.8/site-packages (0.34.1)\n",
      "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from audiomentations) (0.3.7)\n",
      "Requirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from audiomentations) (0.8.1)\n",
      "Requirement already satisfied: scipy<2,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from audiomentations) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.8/site-packages (from audiomentations) (1.21.4)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (5.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (21.3)\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.53.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.10.3.post1)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.1.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.1.9)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.2.2)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.24.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (59.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.31.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in /opt/conda/lib/python3.8/site-packages (from resampy>=0.2.2->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.21)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.0.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jupyterlab\n",
    "!pip install --upgrade jupyter\n",
    "!pip install ipywidgets\n",
    "!pip install audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c9f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/nnAudio/Spectrogram.py:4: Warning: importing Spectrogram subpackage will be deprecated soon. You should import the feature extractor from the feature subpackage. See actual documentation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "#sys.path.append(0, os.path.abspath('ComParE2022_VecNet/src'))\n",
    "#sys.path.append('../../src')\n",
    "\n",
    "import config \n",
    "#from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "# import IPython.display as ipd\n",
    "\n",
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "## nnAudio\n",
    "from nnAudio import features , Spectrogram\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "# import torch.multiprocessing as mp\n",
    "# mp.set_start_method('spawn')\n",
    "from audiomentations import AddBackgroundNoise, PolarityInversion , TimeStretch,TimeMask\n",
    "import torchvision.models.quantization as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5d5b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True, progress=True, quantize=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.train()\n",
    "model.fuse_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f4102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8091add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_prep(model_ft):\n",
    "    model_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n",
    "# Step 3\n",
    "    model_ft = torch.quantization.prepare_qat(model_ft, inplace=True)\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = True\n",
    "    return(model_ft)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d9fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def create_combined_model(model_fe):\n",
    "    # Step 1. Isolate the feature extractor.\n",
    "    model_fe_features = nn.Sequential(\n",
    "    model_fe.quant,  # Quantize the input\n",
    "    model_fe.conv1,\n",
    "    model_fe.bn1,\n",
    "    model_fe.relu,\n",
    "    model_fe.maxpool,\n",
    "    model_fe.layer1,\n",
    "    model_fe.layer2,\n",
    "    model_fe.layer3,\n",
    "    model_fe.layer4,\n",
    "    model_fe.avgpool,\n",
    "    model_fe.dequant,  # Dequantize the output\n",
    "  )\n",
    "\n",
    "    # Step 2. Create a new \"head\"\n",
    "    new_head = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(num_ftrs, 2),\n",
    "   )\n",
    "\n",
    "  # Step 3. Combine, and don't forget the quant stubs.\n",
    "    new_model = nn.Sequential(\n",
    "    model_fe_features,\n",
    "    nn.Flatten(1),\n",
    "    new_head,)\n",
    "    \n",
    "    model_ft  = quant_prep(new_model)\n",
    "    \n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a8ca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_ft = create_combined_model(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e63495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(3,224,224)\n",
    "# y_t = model_ft(x)\n",
    "# print(y_t)\n",
    "# torch.argmax(y_t, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1d1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    #This is same as defined in config -min_duration = win_size * frame_duration\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    stride = step_frac*min_length\n",
    "#     print(\"min_length = \" +str(min_length))\n",
    "#     print(\"step_frac = \" +str(step_frac))\n",
    "#     print(\"stride = \" +str(stride))\n",
    "    for _,row in df.iterrows():\n",
    "        #processed_data keeps track of the tensor_values processed thus far\n",
    "        if row['length'] > min_length:\n",
    "            processed_data = 0\n",
    "            #total_data is the total tensor present in the audio\n",
    "            total_data = config.rate*row['length']\n",
    "            label_ind = row['sound_type']\n",
    "            if label_ind == 'mosquito':\n",
    "                label_ind =1\n",
    "            else:\n",
    "                label_ind = 0\n",
    "            #print(\"********\")\n",
    "            count = 0\n",
    "            #print(\"count = \" +str(count))\n",
    "            #print(\"id = \" + str(row['id']) + \" duration = \" +str(row['length']) + \"total x vals = \" + str(total_data))\n",
    "            inner_loop_flag = False\n",
    "            #print(\"going into the inner loop to offset....\")\n",
    "            while(processed_data < total_data):\n",
    "                #print(\"inside inner loop.....\")\n",
    "                start = count*stride*config.rate\n",
    "                #now find out the row_len\n",
    "                if total_data - (start + min_length*config.rate) >= 0:\n",
    "                    #print(\"full chunk \")\n",
    "                    row_len = min_length\n",
    "                    end = start + row_len*config.rate\n",
    "                    audio_offsets.append({'id':row['id'], 'offset':count, 'length': row_len,'specie_ind': label_ind,'start':start,'end':end})\n",
    "                    #print(\"count = \" +str(count) + \"offset = \" +str(count) + \"start = \" +str(start) + \"end = \" +str(end))\n",
    "                    #print(\"for count.... = \" + str(count) + \"processed data = \" +str(processed_data))\n",
    "                    count+=1\n",
    "                    processed_data = (count*stride)*config.rate\n",
    "                    \n",
    "                else:\n",
    "                    inner_loop_flag = True\n",
    "                    break\n",
    "                    \n",
    "                                                       \n",
    "            #for processing residual data\n",
    "            if(inner_loop_flag):\n",
    "                #print(\"processing residual ....processed \" +str(processed_data) + \" of \" + str(total_data))\n",
    "                start = count*stride*config.rate\n",
    "                resid_durn = round((total_data - processed_data)/config.rate,2)\n",
    "                end = total_data\n",
    "                #print(\"for...\" + str(row['id']) + \" adding the residual data in the data frame with duration = \" + str(resid_durn))\n",
    "                audio_offsets.append({'id':row['id'], 'offset':count, 'length':resid_durn ,'specie_ind': label_ind,'start':start,'end':end})\n",
    "            \n",
    "        elif short_audio:\n",
    "            label_ind = row['sound_type']\n",
    "            if label_ind == 'mosquito':\n",
    "                label_ind =1\n",
    "            else:\n",
    "                label_ind = 0\n",
    "            start = 0\n",
    "            end = row['length']*config.rate\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': label_ind,'start':0 , 'end':end})\n",
    "    return pd.DataFrame(audio_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3a807a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(csv_loc = config.data_df  ):\n",
    "    \"\"\"This function reads a csv and creates a dataframe for further processing.\"\"\"\n",
    "    df = pd.read_csv(csv_loc)\n",
    "    idx_test_A = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'field')\n",
    "    idx_test_B = np.logical_and(df['country'] == 'UK', df['location_type'] == 'culture')\n",
    "    idx_train = np.logical_not(np.logical_or(idx_test_A, idx_test_B))\n",
    "    #df_test_A = df[idx_test_A]\n",
    "    #df_test_B = df[idx_test_B]\n",
    "    #df = df.loc[df['Grade'].notnull()]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e69a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    import seaborn as sns\n",
    "    sns.countplot(x = 'species', data = df , ax = ax , hue = 'gender',palette='dark')\n",
    "    #ax.bar_label(ax.containers[0])\n",
    "    #ax.bar_label(ax.containers[-1], fmt='Count:\\n%.2f', label_type='center')\n",
    "    plt.xticks(rotation=90 )\n",
    "    plt.title(\"Distribution of Species \")\n",
    "    plt.rc('xtick', labelsize=12)\n",
    "    plt.rc('xtick', labelsize=12)\n",
    "    plt.rc('axes', labelsize=15)\n",
    "    plt.rc('figure', titlesize=15)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88268c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df_all):\n",
    "    np.random.seed(42)\n",
    "    msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "    df_test = df_all[msk_test]\n",
    "    df_train_temp  = df_all[~msk_test]\n",
    "    msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "    df_val = df_train_temp[msk_train]\n",
    "    df_train  = df_train_temp[~msk_train]\n",
    "    return df_train ,df_val ,df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e289a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_split(df1 , df2):\n",
    "    df_temp = pd.merge(df1,df2, on = 'id', how = 'inner')\n",
    "    #print(df_temp)\n",
    "    common_elem = len(df_temp)\n",
    "    #print(\"common_elem = \",common_elem)\n",
    "    con = (common_elem == 0)\n",
    "    #print(\"condition = \",con)\n",
    "    assert (con), \"Split has issues\"\n",
    "    print(\"split is a success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42352fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b76bbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(df):\n",
    "    np.array(df_train_offset.specie_ind)\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df.specie_ind)),y=np.array(np.array(df.specie_ind)))\n",
    "    print(type(class_weights))\n",
    "    print(class_weights.shape)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9504b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53402a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7286f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normalize_batch, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_mean = torch.mean(x, dim=0, keepdim=True)\n",
    "        batch_std = torch.std(x, dim=0, keepdim=True)\n",
    "        epsilon = 1e-8\n",
    "        batch_std = torch.sqrt(batch_std ** 2 + epsilon)\n",
    "        batch_normalized = (x - batch_mean) / batch_std\n",
    "        return batch_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "602d7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self,model = model,image_size = 224):\n",
    "#         super().__init__()\n",
    "#         # num_classes=0 removes the pretrained head\n",
    "#         self.backbone = create_combined_model(model)\n",
    "#         #####  This section is model specific\n",
    "#         #### It freezes some fo the layers by name\n",
    "#         #### you'll have to inspect the model to see the names\n",
    "#                 #### end layer freezing\n",
    "#         #self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "#         self.sizer = VT.Resize((image_size,image_size),antialias = True)\n",
    "#         self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "#                               window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "#                            sr=config.rate, output_format=\"Magnitude\", trainable=True,verbose = False).to('cuda')\n",
    "#         self.batch_norm = nn.BatchNorm2d(num_features= 1)\n",
    "#         #self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "#         self.quant = torch.ao.quantization.QuantStub()\n",
    "#         self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        \n",
    "        \n",
    "#     def forward(self, x,train = True):\n",
    "#         # first compute spectrogram\n",
    "#         #x = self.quant(x)\n",
    "#         spec_gram = self.spec_layer(x)\n",
    "#         #print(\"post spec gram shape = \",spec_gram.shape)\n",
    "#         spec_gram = self.batch_norm(spec_gram.unsqueeze(dim = 1))\n",
    "#         #print(\"post norm shape = \",spec_gram.shape)\n",
    "#         spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "#         assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after spec gram creation.\"\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             if train == True:\n",
    "#                 #generate a random number and if condition is met apply aug\n",
    "#                 ta_transformations_rndm_choice = VT.RandomChoice([AT.FrequencyMasking(freq_mask_param=100),AT.TimeMasking(time_mask_param=50)], p=[.4, .4])\n",
    "#                 ta_transformations_rndm_apply = VT.RandomApply([AT.FrequencyMasking(freq_mask_param=50),AT.TimeMasking(time_mask_param=25)],p = .15)\n",
    "#                 spec_gram = ta_transformations_rndm_choice(spec_gram)\n",
    "#                 spec_gram = ta_transformations_rndm_apply(spec_gram)\n",
    "#                 spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "#                 assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after augmentations  \"\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "#         x = self.sizer(spec_gram.squeeze(dim = 1))\n",
    "#         #print(\"post sizer shape = \",x.shape)\n",
    "#         x = x.unsqueeze(dim = 1)\n",
    "#         #print(\"post unsqueeze shape = \",x.shape)\n",
    "        \n",
    "#         # then repeat channels\n",
    "#         del spec_gram,spec_gram_nan_check\n",
    "#         if DEBUG:\n",
    "#             print(\"Final shape that goes to backbone = \" + str(x.shape))\n",
    "                \n",
    "#         x = x.expand(-1, 3, -1, -1)\n",
    "#         #print(\"post expansion x device = \",x.device)\n",
    "#         x = self.backbone(x)\n",
    "#         backbone_op_nan_check = torch.isnan(x).any().item()\n",
    "#         assert not (backbone_op_nan_check) ,\"Tensor contains NaN values in the backbone OP \"\n",
    "#         #print(\"x shape after backbone  = \" + str(x.shape))\n",
    "#         #print(\"output of model = \" +str(x))\n",
    "#         #pred = nn.Softmax(x)\n",
    "#         #x = self.dequant(x)\n",
    "#         pred = x\n",
    "#         #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "#         #print(pred)\n",
    "#         output = {\"prediction\": pred }\n",
    "#         #print(output)\n",
    "#         del x , backbone_op_nan_check\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fceb8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specgram_apply_aug(x , train = True , image_size = 224):\n",
    "    \n",
    "    sizer = VT.Resize((image_size,image_size),antialias = True)\n",
    "    spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=False,verbose = False).to('cuda')\n",
    "    batch_norm = nn.BatchNorm2d(num_features= 1).to('cuda')\n",
    "    x.to('cuda')\n",
    "    #print(\"x device before spec gram = \" , x.device)\n",
    "    spec_gram = spec_layer(x)\n",
    "    #print(\"post spec gram generation device  = \" , spec_gram.device)\n",
    "    spec_gram.to('cuda')\n",
    "    spec_gram = spec_gram.unsqueeze(dim = 1).to('cuda')\n",
    "    #print(\"post unsqueeze device  = \" , spec_gram.device)\n",
    "    spec_gram = batch_norm(spec_gram)\n",
    "    #print(\"post norm device  = \" , spec_gram.device)\n",
    "    \n",
    "    if train == True:\n",
    "        #generate a random number and if condition is met apply aug\n",
    "        ta_transformations_rndm_choice = VT.RandomChoice([AT.FrequencyMasking(freq_mask_param=100),AT.TimeMasking(time_mask_param=50)], p=[.4, .4])\n",
    "        ta_transformations_rndm_apply = VT.RandomApply([AT.FrequencyMasking(freq_mask_param=50),AT.TimeMasking(time_mask_param=25)],p = .15)\n",
    "        spec_gram = ta_transformations_rndm_choice(spec_gram)\n",
    "        spec_gram = ta_transformations_rndm_apply(spec_gram)\n",
    "        spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "        assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after augmentations  \"\n",
    "    \n",
    "    spec_gram = sizer(spec_gram.squeeze(dim = 1))\n",
    "    #converting to 3 channels as required by resnet.\n",
    "    spec_gram = spec_gram.unsqueeze(dim = 1)\n",
    "    spec_gram = spec_gram.expand(-1, 3, -1, -1)\n",
    "    #print(\"shape of spec gram being returned = \", spec_gram.shape)\n",
    "    \n",
    "    return spec_gram\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2dd474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(64,15360, device = 'cuda')\n",
    "# t = get_specgram_apply_aug(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0a2ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  device=None , call = \"val\"):\n",
    "    softmax = nn.Softmax()\n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            y_pred = model(x)\n",
    "            #y_pred_smax = softmax(y_pred)\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred\n",
    "## Train_model ####\n",
    "#train_loader, val_loader, test_loader,model,classes,df_train_offset ,num_epochs = num_epochs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9529eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader,test_loader, model ,class_weights,num_epochs ):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = BinaryCrossEntropy(weight=weights_adj)\n",
    "    lr = .000015\n",
    "    base_optimiser = timm.optim.AdamP(model.parameters(), lr= lr)\n",
    "    look_optimiser = timm.optim.Lookahead(base_optimiser)\n",
    "    cooldown_epoch = 50\n",
    "    \n",
    "    #optimiser = timm.optim.AdamW(model.parameters(), lr=config_pytorch.lr)\n",
    "    #timm.optim.Lookahead(optimiser, alpha=0.5, k=6)\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(base_optimiser, t_initial= num_epochs,lr_min= lr/100,warmup_t = 5,warmup_lr_init= lr/10,noise_std=.075)\n",
    "    \n",
    "    \n",
    "    #optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    softmax = nn.Softmax()\n",
    "    all_train_f1 = []\n",
    "    all_val_f1 = []\n",
    "    accumulation_steps = 4\n",
    "    lr_log = []\n",
    "    for e in range(num_epochs + cooldown_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        #tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(train_loader):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 20 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            #create a transformation here\n",
    "            # create a spectrogram and call for the randomization\n",
    "            #spec = get_specgram_apply_aug(x, train = True)\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. x device = \" +str(x.device))\n",
    "                \n",
    "            \n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "                                             \n",
    "            with autocast():\n",
    "                x.to(device)\n",
    "                y_pred = model(x)\n",
    "                #y_pred_smax = softmax(y_pred)\n",
    "                preds = torch.argmax(y_pred, axis = 1)\n",
    "                loss = criterion(y_pred, y)\n",
    "                            \n",
    "            if DEBUG:\n",
    "                    print(\"y_pred  = \" +str(y_pred))\n",
    "                    print(\"preds = \" +str(preds))\n",
    "                   \n",
    "            train_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            #preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "                \n",
    "            loss.backward()\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            base_optimiser.step()\n",
    "            base_optimiser.zero_grad()\n",
    "            #scheduler.step(e)\n",
    "                \n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),error_if_nonfinite=False ,max_norm = 1.0 )\n",
    "            base_optimiser.step()\n",
    "            del x\n",
    "            del y\n",
    "            del y_pred,preds\n",
    "        \n",
    "        #lr_log.append(lr)\n",
    "        look_optimiser.sync_lookahead()\n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        all_train_f1.append(train_f1)\n",
    "        if DEBUG:\n",
    "            print(\"train acc = \" +str(train_acc))\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = criterion, device=device, call = \"val\")\n",
    "        all_val_f1.append(val_f1)\n",
    "        all_val_loss.append(val_loss)\n",
    "        if DEBUG:\n",
    "            print(\"val F1 = \" + str(val_f1))\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_med{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir,  checkpoint_name))\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.model_dir,  checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"********************************\")\n",
    "            current_lr = base_optimiser.param_groups[0]['lr']\n",
    "            print(\"Current LR = \" + '{0:.8f}'.format(current_lr))\n",
    "            from sklearn.metrics import classification_report\n",
    "            _, _ , all_y_test,all_y_pred_test = test_model(model, val_loader, criterion = criterion, device=device, call = \"test\")\n",
    "            # at times output is not getting printed. Could be due to multi threading and hence adding sleep\n",
    "            time.sleep(2)\n",
    "            sys.stdout.flush()\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= [\"0\",\"1\"]))\n",
    "            print(\"********************************\")\n",
    "            time.sleep(2)\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , [\"0\",\"1\"])\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            sys.stdout.flush()\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        scheduler.step(e+1)\n",
    "        if overrun_counter > 50:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1\n",
    "\n",
    "\n",
    "\n",
    "#### Dataste class #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6554c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozDataset_train(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        temp_id = int(self.audio_df.loc[idx]['id'])\n",
    "        file_path = os.path.join(\"..\",\"data\",\"audio\")\n",
    "        path_var = file_path +\"/\" +str(temp_id)+ str(\".wav\")\n",
    "        entire_aud, inp_rate = torchaudio.load(path_var)\n",
    "        #print(\"processsing file on \" +str(path_var))\n",
    "        if inp_rate != config.rate:\n",
    "            #print(\" Original sample rate = \" +str(inp_rate)+ \" resampling ...\")\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=entire_aud.dtype)\n",
    "            entire_aud = resampler(entire_aud)\n",
    "            #print(\"processsing file on \" +str(path_var) + \"Post resample shape =  \" + str(entire_aud.shape))\n",
    "        \n",
    "        aud_len = self.audio_df.loc[idx]['length']\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        #print(\"sliced val = \" +str(int((offset+config.min_duration)*config.rate)))\n",
    "        start_pos = int(round(self.audio_df.loc[idx]['start']))\n",
    "        #print(\"start_pos = \" +str(start_pos))\n",
    "        end_pos =  int(round(self.audio_df.loc[idx]['end']))\n",
    "        #print(\"end_pos = \" +str(end_pos))\n",
    "        x = entire_aud[:,start_pos:end_pos]\n",
    "        #print(\"extracted x = \" +str(x))\n",
    "        #print(\"x shape = \" +str(x.shape))\n",
    "        if (aud_len < config.min_duration) or (x.shape[1] < 15360):\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            #print(\"padding on \" +str(path_var))\n",
    "            f_out = pad_mean(x)\n",
    "            #print(\"returning from padding  SHape = \" +str(f_out.shape))\n",
    "        else:\n",
    "            f_out = x[0]\n",
    "            f_out = f_out.unsqueeze(0)\n",
    "            #print(\"inside else, returned  SHape = \" +str(f_out.shape))\n",
    "            \n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            #print(\"offset = \" + str(offset))\n",
    "            #print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(f_out.shape))\n",
    "        \n",
    "        #x_val = x[:,start:end]\n",
    "        #now that we have final x- let's create specgram and add augmentations.\n",
    "        from audiomentations import AddGaussianNoise\n",
    "\n",
    "        transform_gauss_noise = AddGaussianNoise(min_amplitude=0.001,max_amplitude=0.015, p= .25 )\n",
    "        transform_timemask = TimeMask(min_band_part=0.1, max_band_part=0.15,fade=True, p= .25 )\n",
    "        transform_stretch = TimeStretch(min_rate=0.8, max_rate=1.25,leave_length_unchanged=True, p= .10)\n",
    "        #augmented_sound = transform_audio_with_noise(my_waveform_ndarray, sample_rate=config.rate)\n",
    "        f_out = f_out.squeeze(dim = 0)\n",
    "        f_out = transform_gauss_noise(f_out,sample_rate=config.rate)\n",
    "#         f_out.numpy()\n",
    "#         f_out = transform_timemask(f_out,sample_rate=config.rate)\n",
    "#         f_out = transform_stretch(f_out,sample_rate=config.rate)\n",
    "#         f_out = torch.tensor(f_out)      \n",
    "        f_out = f_out.unsqueeze(dim = 0)\n",
    "        \n",
    "        \n",
    "        return (f_out,self.audio_df.loc[idx]['specie_ind'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ef5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozDataset_test(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        temp_id = int(self.audio_df.loc[idx]['id'])\n",
    "        file_path = os.path.join(\"..\",\"data\",\"audio\")\n",
    "        path_var = file_path +\"/\" +str(temp_id)+ str(\".wav\")\n",
    "        entire_aud, inp_rate = torchaudio.load(path_var)\n",
    "        #print(\"processsing file on \" +str(path_var))\n",
    "        if inp_rate != config.rate:\n",
    "            #print(\" Original sample rate = \" +str(inp_rate)+ \" resampling ...\")\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=entire_aud.dtype)\n",
    "            entire_aud = resampler(entire_aud)\n",
    "            #print(\"processsing file on \" +str(path_var) + \"Post resample shape =  \" + str(entire_aud.shape))\n",
    "        \n",
    "        aud_len = self.audio_df.loc[idx]['length']\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        #print(\"sliced val = \" +str(int((offset+config.min_duration)*config.rate)))\n",
    "        start_pos = int(round(self.audio_df.loc[idx]['start']))\n",
    "        #print(\"start_pos = \" +str(start_pos))\n",
    "        end_pos =  int(round(self.audio_df.loc[idx]['end']))\n",
    "        #print(\"end_pos = \" +str(end_pos))\n",
    "        x = entire_aud[:,start_pos:end_pos]\n",
    "        #print(\"extracted x = \" +str(x))\n",
    "        #print(\"x shape = \" +str(x.shape))\n",
    "        if (aud_len < config.min_duration) or (x.shape[1] < 15360):\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            #print(\"padding on \" +str(path_var))\n",
    "            f_out = pad_mean(x)\n",
    "            #print(\"returning from padding  SHape = \" +str(f_out.shape))\n",
    "        else:\n",
    "            f_out = x[0]\n",
    "            f_out = f_out.unsqueeze(0)\n",
    "            #print(\"inside else, returned  SHape = \" +str(f_out.shape))\n",
    "            \n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            #print(\"offset = \" + str(offset))\n",
    "            #print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(f_out.shape))\n",
    "        \n",
    "        #x_val = x[:,start:end]\n",
    "        #now that we have final x- let's create specgram and add augmentations.\n",
    "                 \n",
    "        return (f_out,self.audio_df.loc[idx]['specie_ind'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a823cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_indices(num_values ,df ,classes = classes):\n",
    "#     new_df = pd.DataFrame()\n",
    "#     for ind in range(len(classes)):\n",
    "#         #print(\"ind = \", ind)\n",
    "#         op = df[df['specie_ind'] == ind]\n",
    "#         #print(\"len op = \", len(op))\n",
    "#         op_new = op.sample(n = 1)\n",
    "#         #print(\"rand_ind = \" , rand_ind)\n",
    "#         #([df1, df2], axis=1)\n",
    "#         new_df = pd.concat([op_new,new_df],axis = 0)\n",
    "#         #print(\"elem = \" , elem)\n",
    "#         #new_list.append(elem)\n",
    "#     if len(new_df) < num_values:\n",
    "#         diff =  num_values - len(new_df)\n",
    "#         #print(\"diff = \", diff)\n",
    "#         remaining_elems= df.sample(n = diff)\n",
    "#         #print(\"len of remaining elems = \", len(remaining_elems))\n",
    "#         new_df = pd.concat([remaining_elems,new_df],axis = 0)\n",
    "        \n",
    "#     #print(\"new_df = \", new_df)    \n",
    "#     new_df_1 = new_df.reset_index(drop = True)\n",
    "#     return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb9d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(filepath, model=MyModel('convnext_xlarge_in22k')):\n",
    "#     # Instantiate model to inspect\n",
    "#     print(\"Filepath = \" + str(filepath))\n",
    "#     print(\"model = \" +str(model))\n",
    "#     device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "#     print(f'Training on {device}')\n",
    "        \n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         print(\"Using data parallel\")\n",
    "#         model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "#     model = model.to(device)\n",
    "#     # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         map_location=lambda storage, loc: storage.cuda()\n",
    "#     else:\n",
    "#         map_location='cpu'\n",
    "        \n",
    "#     checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691c3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "pin_memory = True\n",
    "num_workers = 8\n",
    "num_epochs = 200\n",
    "short_audio=True\n",
    "DEBUG = False\n",
    "resume_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ca09ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside main.....\n",
      "The current working directory is  /dli/task/notebooks\n",
      "now validating the split post loading and keeping TZ data\n",
      "split is a success\n",
      "split is a success\n",
      "split is a success\n",
      "now validating the split post offset_creation\n",
      "split is a success\n",
      "split is a success\n",
      "split is a success\n"
     ]
    }
   ],
   "source": [
    "print(\"inside main.....\")\n",
    "print(\"The current working directory is \", os.getcwd())\n",
    "csv_loc = os.path.join(\"..\",\"data\",\"metadata\",\"neurips_2021_zenodo_0_0_1.csv\")\n",
    "df = prepare_df(csv_loc = csv_loc)\n",
    "#plot_df(df)\n",
    "df_train ,df_val ,df_test = train_test_split(df)\n",
    "print(\"now validating the split post loading and keeping TZ data\")\n",
    "validate_split(df_train ,df_val)\n",
    "validate_split(df_train ,df_test)\n",
    "validate_split(df_test ,df_val)\n",
    "df_train_offset = get_offsets_df(df_train, short_audio=True)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=True)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=True)\n",
    "df_train_offset.reset_index(inplace = True , drop = True)\n",
    "df_test_offset.reset_index(inplace = True , drop = True)\n",
    "df_val_offset.reset_index(inplace = True , drop = True)\n",
    "print(\"now validating the split post offset_creation\")\n",
    "validate_split(df_train_offset ,df_val_offset)\n",
    "validate_split(df_train_offset ,df_test_offset)\n",
    "validate_split(df_test_offset ,df_val_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cb99e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_file,model = model):\n",
    "    model = model\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5acffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d95e7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c976450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_med45_2023_11_09_05_31_52\n",
    "if resume_train == True:\n",
    "    model_file = os.path.join(\"..\",\"models\",\"model_med45_2023_11_09_05_31_52.pth\")\n",
    "    model = load_model(model_file , model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84b35ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self ,image_size = 224 ):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = create_combined_model(model)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "                #### end layer freezing\n",
    "        #self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size ,image_size ),antialias = True)\n",
    "        self.spec_layer = AT.Spectrogram(n_fft = int(config.NFFT), return_complex= False,).to('cuda')\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features= 1).to('cuda')\n",
    "        #self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first compute spectrogram\n",
    "        \n",
    "        spec_gram = self.spec_layer(x)\n",
    "        #print(\"post spec gram shape = \",spec_gram.shape)\n",
    "        #spec_gram = spec_gram.unsqueeze(dim = 1).to('cuda')\n",
    "        spec_gram = self.batch_norm(spec_gram)\n",
    "        #print(\"post norm shape = \",spec_gram.shape)\n",
    "        spec_gram_nan_check = torch.isnan(spec_gram).any().item()\n",
    "        assert not (spec_gram_nan_check) ,\"Tensor contains NaN values after spec gram creation.\"\n",
    "             \n",
    "        \n",
    "        x = self.sizer(spec_gram.squeeze(dim = 1))\n",
    "        #print(\"post sizer shape = \",x.shape)\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "        #print(\"post unsqueeze shape = \",x.shape)\n",
    "        \n",
    "        # then repeat channels\n",
    "        del spec_gram,spec_gram_nan_check\n",
    "        x = x.expand(-1, 3, -1, -1)\n",
    "        #print(\"post expand shape = \",x.shape)\n",
    "        x = self.backbone(x)\n",
    "        #backbone_op_nan_check = torch.isnan(x).any().item()\n",
    "        #assert not (backbone_op_nan_check) ,\"Tensor contains NaN values in the backbone OP \"\n",
    "        #print(\"x shape = \" + str(x.shape))\n",
    "        #print(\"x = \" +str(x))\n",
    "        #pred = nn.Softmax(x)\n",
    "        output = x\n",
    "        #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "        #print(pred)\n",
    "        #print(output)\n",
    "        del x \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92c4495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2,)\n",
      "inside main. class_weigths type =  <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchaudio/transforms/_transforms.py:94: UserWarning: `return_complex` argument is now deprecated and is not effective.`torchaudio.transforms.Spectrogram(power=None)` always returns a tensor with complex dtype. Please remove the argument in the function call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class_weights = get_class_weights(df_train_offset)\n",
    "print(\"inside main. class_weigths type = \", type(class_weights))\n",
    "model = MyModel()\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "\n",
    "train_dataset = MozDataset_train(df_train_offset,  config.data_dir, min_length)\n",
    "val_dataset = MozDataset_test(df_val_offset,  config.data_dir, min_length)\n",
    "test_dataset = MozDataset_test(df_test_offset,  config.data_dir, min_length)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True, pin_memory=True )\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory, shuffle = True )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers= num_workers, pin_memory=pin_memory,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fd39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "epoch = 0batch = 0 of 55duraation = 0.9357369224230448\n",
      "epoch = 0batch = 20 of 55duraation = 2.9684248050053914\n",
      "epoch = 0batch = 40 of 55duraation = 5.672177890936534\n",
      "Epoch: 0, Train Loss: 0.70762763, Train f1: 0.36884596, Val Loss: 0.03694551, Val f1: 0.36044547, overrun_counter -1\n",
      "Saving model to: /dli/task/models/model_med0_2023_11_29_01_45_17.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "Current LR = 0.00000150\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.96      0.63     13006\n",
      "           1       0.58      0.05      0.10     14596\n",
      "\n",
      "    accuracy                           0.48     27602\n",
      "   macro avg       0.52      0.50      0.37     27602\n",
      "weighted avg       0.53      0.48      0.35     27602\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1batch = 0 of 55duraation = 0.9549750169118245\n",
      "epoch = 1batch = 20 of 55duraation = 2.9400944550832113\n",
      "epoch = 1batch = 40 of 55duraation = 5.71318830649058\n",
      "Epoch: 1, Train Loss: 0.69856990, Train f1: 0.35217880, Val Loss: 0.03659889, Val f1: 0.38988399, overrun_counter -1\n",
      "Saving model to: /dli/task/models/model_med1_2023_11_29_01_55_02.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "Current LR = 0.00000420\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.99      0.65     13006\n",
      "           1       0.87      0.07      0.13     14596\n",
      "\n",
      "    accuracy                           0.50     27602\n",
      "   macro avg       0.68      0.53      0.39     27602\n",
      "weighted avg       0.69      0.50      0.37     27602\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD1CAYAAABZXyJ5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdHElEQVR4nO3debxV8/7H8dfnNNB8mkvRQLjqGqOuIREpU3Hpl+tHurn5ETJdQ1FuQi7pxg/XoShT5ltcIilzKepWR9L5RXW6DWhwaTx7f35/7FW2tGvvM+29z3o/Pdbj7P1Zw/6shx77s7/DWsvcHRERCZ+cdCcgIiLpoQIgIhJSKgAiIiGlAiAiElIqACIiIaUCICISUpXTnUCqtiycpnmr8gs5DVukOwXJQFUatLaSHmPbd0uS/r4pjc8rb1lXAEREyk00ku4MypQKgIhIIh5NdwZlSgVARCQBjxSlO4UypQIgIpJIVC0AEZFwUheQiEhIaRBYRCSk1AIQEQknDQKLiISVBoFFREJKXUAiIiGlQWARkZBSC0BEJKQ0BiAiElKaBSQiEk7uGgMQEQknjQGIiISUxgBEREJKLQARkZDSdQAiIiGlWUAiIiFVwbuActKdgIhIxopGk1/2wMzGmtkaM1sQF7vXzL40s3lm9qqZ5catu8XMCsxskZmdFhfvFsQKzOzmuHgrM5sZxJ83s6p7ykkFQEQkkVIsAMCTQLedYlOAdu5+KPAVcAuAmR0C9AbaBvs8bGaVzKwS8BDQHTgEuCDYFuAeYJS7HwCsA/rtKSEVABGRBNwjSS97Ppa/D6zdKfa2u28faJgBNA9e9wAmuPsWd/8aKACOCZYCd1/i7luBCUAPMzPgZOClYP9xQM895aQxABGRRMp3EPiPwPPB62bECsJ2hUEMYPlO8Q5AfWB9XDGJ3z4hFQARkURSuBDMzPoD/eNCee6el+S+g4Ei4JmU8ishFQARkURSmAUUfNkn9YUfz8wuAc4Euri7B+EVwL5xmzUPYiSIfw/kmlnloBUQv31CGgMQEUmkdAeBf8XMugE3Ame7+8a4VZOA3ma2l5m1AtoAnwKzgDbBjJ+qxAaKJwWFYxpwXrB/H2Dinj5fLQARkURK8ToAM3sO6Aw0MLNCYCixWT97AVNi47jMcPf/cfd8M3sB+IJY19AAD0aazexK4C2gEjDW3fODj7gJmGBmw4E5wJg95vRziyM7bFk4LbsSljKX07BFulOQDFSlQWsr6TE2vflA0t831bpfXeLPK29qAYiIJKJbQYiIhJRuBy0iElIV/F5AKgAiIomoBSAiElJqAYiIhJRaACIiIRXRE8FERMJJLQARkZBSARARCSkNAouIhJRaACIiIaVBYBGRkFILQEQkpDQGICISTh6t2HefVwEQEUlEXUAiIiGlLiARkZAq0iwgEZFwUhdQ2TKzbsBoYg84ftzdR6Q5pXI15MHxvDd7PvXq1OLVB4YAMPLJl3lv1jyqVK7Mvk0aMOyqPtSuWZ1tRRFuf+gpFv7fMiLRKGd17sil53UDoNufBlG92t5UysmhUqUcJowc9IvPGfePKbHjjr+PurVrlvt5Smpuvet+3v/oU+rVzeUfT/8dgAfzxvPuh5+QYznUq1uHOwdfT6OG9QH49PN53DP6UYqKiqibW5snH7oXgA9nzGbE3/5OJBrl92d149KLegEwY/YcRj40hmjUqV59b+4cfD37Nd8nPSebybLsmempSmsBMLNKwEPAqUAhMMvMJrn7F+nMqzydffLv6H16ZwaPfnJH7HeH/YaBF/WkcqVKjBr3CmNensy1fc7l7Y8+Y9u2Il55YAibtmzlnCtvp/sJ7WnWuAEAY4Zft8sv91XfruWTuQtp2rBeeZ2WlFDP00/lD78/m0F33Lcj1vfC33NV/4sBePrFiTzyxLMMvfEqfvjPjwwf+b88OnI4TZs04vt16wGIRCIMH/kQj/3tLpo0asB/XTqQk47vwP6tWnDHfQ/xwIgh7N9yPya88jqPPvkcd956fTpONbNV8BZATpo//xigwN2XuPtWYALQI805lav2bdtQp2b1X8SOPeIQKleqBMChB7Vi9ffrADAzNm7eQlEkwpYtW6lSpTI1q1fb42f8deyLXNvnXKz005cy0v7w31Kndq1fxGrWqLHj9aZNm7Hgf+gbU6ZzyonH0bRJIwDq180FYP7Cr9iv+T7s26wpVapUoXuXE3n3gxkAGPDTTxsB+M+PP9GwQf2yPaFsFfXklyyU7i6gZsDyuPeFQIc05ZKRXn3nY7od3x6AU489kumf/osufW9i05at3PjH86lTK/hSMOOy20djGOefdgLnnXYCANNmzqVR/VwOatU8XacgpWj0o08yafJUatWowdgHY72l3ywrpCgS4ZIrb2Tjxk1ceH4PenQ/hTXffkeTRg137Nu4UQPm5y8C4C83X8PlNwxh772qUqNGdZ7NG5WW88l4FfxWEOluASTFzPqb2Wwzm/34C6+nO51yk/fiG1SulMMZJx4DwILFX5OTY7wz9h7efHQ44ya+Q+GqbwEYd/cNvHD/YB4eciUT3pzO7PzFbNqylcdemsyAC85O52lIKRp42SVMffUpzuh6Es++/BoAkUiUL75czMP3DuPR+4fz6JPP8c2ywt0eZ/zzr/LIfcOY+o+n6Xl6V/76wGPlkX7W8Wg06SUbpbsArAD2jXvfPIj9grvnuXt7d29/aa8zyy25dJo49WPenz2fu6/rhwVt/Tfen8VxR7SlSuVK1M+tzRG/2Z/8gqUANK5fF4D6ubU5ucPhLFj8NctXfsuKNd9z/jV30O1Pg1j9/Xr+67o7+W7dhrSdl5SOM7uexDvTPwJiv+yP7XAU1avtTd3cOhx1eDsWFXxNo4YNWLXm2x37rF7zHY0a1mftuvUsKljCoW0PBqB7l07MXRCaYbfUlGIXkJmNNbM1ZrYgLlbPzKaY2eLgb90gbmb2gJkVmNk8Mzsybp8+wfaLzaxPXPwoM5sf7POAbf/i2I10F4BZQBsza2VmVYHewKQ055R2H36ezxOvvs0Dg66g2l5Vd8SbNqzHp/NjTfiNm7cwb9ESWjVvwsbNW/hp0+Yd8U/mLuSA/ZpxYMtmvDfuXiY/dheTH7uLxvVzef7+wTSoWyct5yUls3T5z7+N3v3gE1q1iHXrnXRCR+bMy6eoKMKmzZuZn7+I1i33pd3BB7Ks8N8U/nsV27Zt482p73HS8R2pXasWP/60cUcr4eNZc2jdYr+0nFPG82jyy549CXTbKXYzMNXd2wBTg/cA3YE2wdIfeARiBQMYSqyr/Bhg6PaiEWzzp7j9dv6sX0nrGIC7F5nZlcBbxKaBjnX3/HTmVN5uHPk4sxd8xfoffuSUfjdzRe+zGPPyZLZuK+KyoaOB2EDwbZdfSO/uJ3Lbg+M556q/4O706HIsB7ZsTuGqb7lmRGyqYCQSpXunozn+yLbpPC0poT8PHcGsOfNYv/4HuvT8b67odxEffDKLb5YVYjnGPk0aMeTPVwGwf8v9OK5De87tczk5lsPvzzqNNq1bAjDo2su57LpbiUQinHNmVw5o3QKA22+6mmsH34nlGLVr1eSOW65N16lmtlIc3HX3982s5U7hHkDn4PU4YDpwUxAf7+4OzDCzXDNrGmw7xd3XApjZFKCbmU0Harv7jCA+HugJvLm7nMyzbJ7rloXTsithKXM5DVukOwXJQFUatC7xxLefbr8g6e+bmn+ZcBmxX+vb5bl7Xvw2QQF43d3bBe/Xu3tu8NqAde6ea2avAyPc/cNg3VRihaEzsLe7Dw/itwGbiBWOEe5+ShA/AbjJ3XfbZ57uWUAiIpkrhVlAwZd93h43TLy/m1m5/sBN9xiAiEjmKvvrAFYHXTsEf9cE8UQTZHYXb76L+G6pAIiIJFAO00AnAdtn8vQBJsbFLw5mA3UENrj7SmLjpV3NrG4w+NsVeCtY94OZdQy6ki6OO1ZC6gISEUmkFAeBzew5Yn34DcyskNhsnhHAC2bWD1gK9Ao2fwM4HSgANgJ9Adx9rZndQWwGJcCw7QPCwBXEZhpVIzb4u9sBYFABEBFJrHRnAV2QYFWXXWzrwIAExxkLjN1FfDbQLpWcVABERBLRA2FERMLJi1QARETCKUvv8pksFQARkUSy9CZvyVIBEBFJRC0AEZGQUgEQEQknj6gLSEQknNQCEBEJJ1cBEBEJKRUAEZGQqthDACoAIiKJqAtIRCSsilQARERCSS0AEZGw0hiAiEg4qQUgIhJWagGIiIRTBX8ejAqAiEgiXpTuDMqWCoCISCJqAYiIhJO6gEREQkoFQEQkpCp6AchJdwIiIpnKI5b0kgwzu9bM8s1sgZk9Z2Z7m1krM5tpZgVm9ryZVQ223St4XxCsbxl3nFuC+CIzO62456cCICKSgEct6WVPzKwZcDXQ3t3bAZWA3sA9wCh3PwBYB/QLdukHrAvio4LtMLNDgv3aAt2Ah82sUnHOL+UCYGaHmtkIM5toZu/ExVuaWS8zq1ucREREMo1Hk1+SVBmoZmaVgerASuBk4KVg/TigZ/C6R/CeYH0XM7MgPsHdt7j710ABcExxzi+lMQAzGwYM4ufCEX+ddA7wHHAN8GBxkhERySTuyXXtJHcsX2Fm9wHLgE3A28BnwHr3HVccFALNgtfNgOXBvkVmtgGoH8RnxB06fp+UJN0CMLPewK3AFOBw4O749e6+BJgNnF2cREREMk0qLQAz629ms+OW/vHHCnpHegCtgH2AGsS6cNImlRbA1cSaGj3cfauZnbOLbRYCnUsjMRGRdEumb3/Htu55QN5uNjkF+NrdvwUws1eA44BcM6sctAKaAyuC7VcA+wKFQZdRHeD7uPh28fukJJUxgN8Cb7n71t1s82+gcXESERHJNNGIJb0kYRnQ0cyqB335XYAvgGnAecE2fYCJwetJwXuC9e+6uwfx3sEsoVZAG+DT4pxfKi0AY88XRjcGNhcnERGRTJNKC2CPx3KfaWYvAZ8DRcAcYi2GfwITzGx4EBsT7DIGeMrMCoC1xGb+4O75ZvYCseJRBAxw90hxckqlACwGjk200sxygOOB/OIkIiKSabyUHwfg7kOBoTuFl7CLWTzuvhk4P8Fx7gTuLGk+qXQBvQAcaWbXJ1g/CDgAeLakSYmIZILSvA4gE6XSAvgbsWr0VzPrRTAFNJjWdALQntjUpN0NgoiIZI3SnAaaiZIuAO6+ycxOAkYDFxK7ig3gOmJjA08DV8bNZxURyWoV/V5AKV0I5u4bgEvM7DrgaGIXJWwAPt0+tUlEpKKIRCv23XKKdTdQd18LvFXKuYiIZJRs7dtPlm4HLSKSQGnPAso0SRcAMxub5Kbu7v32vJmISGZTC+Bnl+xhvRO7WMz5+XamIiJZK6pZQDu0ShDPJTYgfBvwMXBzCXMSEckIUbUAYtx9aYJVS4F/mdlbwDzgHX6+lFlEJGtV9BZAqc1xcvflwGvAwNI6pohIOrlb0ks2Ku1ZQKuJ3ZlORCTraRZQkoJnUp5M7MIwEZGsV9G7gFKZBtppN8fYF+hL7Elhj5c8rcRqHPbfZXl4yUINqtdOdwqSgVatX1jiY2Rr106yUmkBTOeXzwDemQHvA38uSUIiIpkiogKwwzB2XQCiwDpi9wMq1lNpREQykbqAAu5+exnmISKScSp6F1DS00DNbKyZXVuWyYiIZJJoCks2SuU6gD8AjcoqERGRTONY0ks2SmUM4BtUAEQkRKIV/DqAVFoAzwLdzaxuWSUjIpJJIuQkvWSjVLK+G5gNTDOzM82scRnlJCKSESr6GMBuu4DM7GJgrrvPAzZvDwMTg/W72s3dXQ+aEZGsl619+8na0xf1k8BQYnf5/IDdXwgmIlKhlPYvezPLJXa3hHbEvk//CCwCngdaEhtr7eXu6yz2C3s0cDqwEbjE3T8PjtMHuDU47HB3H1ecfJL5pW4A7t65OB8gIpKtyqBrZzQw2d3PM7OqQHVgEDDV3UeY2c3EnqlyE9Cd2M012wAdgEeADmZWj9gP8/bEishnZjbJ3delmkx2jlyIiJSDiFnSy56YWR2gE8HzUtx9q7uvB3oA23/BjwN6Bq97AOM9ZgaQa2ZNgdOAKe6+NvjSnwJ0K875qQCIiCQQxZJektAK+BZ4wszmmNnjZlYDaOzuK4NtVgHbJ9g0A5bH7V8YxBLFU5ZMF1Cume2XykHdfVlxkhERySSpDHqaWX+gf1woz93z4t5XBo4ErnL3mWY2mp0eoevubmblNtaaTAEYSGpP+fIkjysiktFSGQMIvuzzdrNJIVDo7jOD9y8RKwCrzaypu68MunjWBOtXELvV/nbNg9gKoPNO8ekppLpDMl/UPwDri3NwEZFsFk2ibz9Z7r7KzJab2UHuvgjoAnwRLH2AEcHficEuk4ArzWwCsUHgDUGReAu4K+6i3K7ALcXJKZkCMMrdhxXn4CIi2awM+mKuAp4JZgAtIfYgrRzgBTPrBywFegXbvkFsCmgBsWmgfQHcfa2Z3QHMCrYb5u5ri5OMumpERBIoKuXrwNx9LrHpmzvrsottHRiQ4DhjgbElzUcFQEQkgSRn92QtFQARkQQq+q0PVABERBKIVuwGwO4LgLvrQjERCa1svctnstQCEBFJIBLmFoCISJipBSAiElIqACIiIeXqAhIRCSe1AEREQkoFQEQkpDQLSEQkpNQCEBEJKRUAEZGQ0r2ARERCKtT3AhIRCTN1AYmIhFSkgncCqQCIiCSgFoCISEhV7N//KgAiIgmpBSAiElKaBSQiElIaBBYRCamK3gWkZ/6KiCQQxZNekmFmlcxsjpm9HrxvZWYzzazAzJ43s6pBfK/gfUGwvmXcMW4J4ovM7LSSnJ8KgIhIAp7CkqSBwMK49/cAo9z9AGAd0C+I9wPWBfFRwXaY2SFAb6At0A142MwqFevkUAEQEUkomsKyJ2bWHDgDeDx4b8DJwEvBJuOAnsHrHsF7gvVdgu17ABPcfYu7fw0UAMcU9/xUAEREEijlLqC/ATfyc72oD6x396LgfSHQLHjdDFgOEKzfEGy/I76LfVKmAiAikkAkhcXM+pvZ7Lil//bjmNmZwBp3/6y8z2F3NAtIRCQBT6F3393zgLwEq48Dzjaz04G9gdrAaCDXzCoHv/KbAyuC7VcA+wKFZlYZqAN8HxffLn6flKkFICKSQGmNAbj7Le7e3N1bEhvEfdfdLwSmAecFm/UBJgavJwXvCda/6+4exHsHs4RaAW2AT4t7fmktAGY21szWmNmCdOaRiR7LG8m/C//F3DlTd8SG3HYdS7+ezexZbzN71tt073YyAKd0OYGZM95kzufvMHPGm5zU+bh0pS3l4NL/uYjpH0/ivU9e40+XXwxA298ezD+nTOCdD17hrWkvcsSRvwWgVu2ajJ/wMFM/fJX3PnmN3heek87Us05pTwPdhZuA68ysgFgf/5ggPgaoH8SvA24GcPd84AXgC2AyMMDdI8X9cIsVlfQws07Aj8B4d2+XzD6Vqzar2JfmBU44vgM//vgTTzwxmsOP6ALECsCPP/7E/aMe/cW2hx/eltWrv2PlytW0bXsQb7z+DC1atU9H2mnRoHrtdKdQbg7+TRv+PmYk3bv0YuvWbTz38mPceO3tjBg5hLyHx/HuOx/Q5dRODBjYj3PP7MPV1/Wndu1aDL99JPXr1+XD2W9w6IGd2LZtW7pPpcytWr+wxDdyuLxlr6S/bx755oWsu3FEWscA3P39+Asc5GcffDiTFi2aJ7Xt3Ln5O17n5y+iWrW9qVq1Klu3bi2r9CRN2hzYms8/m8emTZsB+OSjWZxx1qm4O7Vq1QRiv/pXrVwDgLtTs2YNAGrUrM76dRsoKira9cHlV0rwyz4raAwgy1xxeV8+/2wKj+WNJDe3zq/Wn3vuGcyZs0Bf/hXUlwsX0+F3R1G3bi7Vqu1Nl1M7sU/zJgy55W5uG3YDny14l6F33Mhdw0YBMPaxZ2hzUGv+9eX7TPtoIrfdfDfpbPVnmwie9JKNVACyyN8fHc+BBx/LUe27smrVGu7965BfrD/kkAO5+85BXD7gpjRlKGVt8VdL+N/RjzPh1cd59uXHyJ//JZFIlD79ejN08AiOancyQweN4P4HhwNw0snHs2D+lxx2cCe6nHAud917KzVr1UjzWWSP0rwQLBNlRQGIn18bjf6U7nTSZs2a74hGo7g7j495hqOPPnzHumbNmvLSi2Po+8eBLFmyNH1JSpl77qmXOa3zeZxz+kWsX7+BJQXf0Kt3T/45aQoAk/4xeccgcO8Lz+WN12Lxb75exrKlhbRp0zptuWcbT+G/bJQVBcDd89y9vbu3z8kJ76+XJk0a7Xjds0d38vMXAVCnTm0mTRzPoMF38fEns9OVnpSTBg3qAdCseVNOP+tUXnnpdVatWsOxxx8NwPGdOu74EbCicCUnnNgxtl/D+ux/QCuWfrN81weWX6noLYC0DgKb2XNAZ6CBmRUCQ919zO73Coenn3qIEzv9jgYN6vHNktn8Zdh9nHjisRx22CG4O0uXFnL5FbGungFX9OWA/Vty6+BruXXwtQB0P/0Cvv32+3SegpSRx8ePpl69XLYVFXHLDXfww4b/cMPAIdwxYhCVK1diy+Yt/HlgrHvw/nsfZvTDdzPto4mYGcNvH8natevTewJZJFrBx0vSOg20OMIyDVSSF6ZpoJK80pgG+ocW5yT9ffPs0lc1DVREpKLI1r79ZKkAiIgkkK19+8lSARARSaCiXwimAiAikoC6gEREQkpdQCIiIRXxil0CVABERBKo2F//KgAiIglpDEBEJKQ0C0hEJKSy7U4JqVIBEBFJQGMAIiIhFangJUAFQEQkAXUBiYiElAaBRURCStNARURCqqI/EEYFQEQkgUgFbwFkxTOBRUTSIYonveyJme1rZtPM7AszyzezgUG8nplNMbPFwd+6QdzM7AEzKzCzeWZ2ZNyx+gTbLzazPsU9PxUAEZEE3D3pJQlFwPXufgjQERhgZocANwNT3b0NMDV4D9AdaBMs/YFHIFYwgKFAB+AYYOj2opEqFQARkQRKswXg7ivd/fPg9X+AhUAzoAcwLthsHNAzeN0DGO8xM4BcM2sKnAZMcfe17r4OmAJ0K875aQxARCSBspoFZGYtgSOAmUBjd18ZrFoFNA5eNwOWx+1WGMQSxVOmFoCISAKpdAGZWX8zmx239N/VMc2sJvAycI27/7DT5zmU38izWgAiIgmk8kAYd88D8na3jZlVIfbl/4y7vxKEV5tZU3dfGXTxrAniK4B943ZvHsRWAJ13ik9POtE4agGIiCRQyrOADBgDLHT3++NWTQK2z+TpA0yMi18czAbqCGwIuoreArqaWd1g8LdrEEuZWgAiIgmU8hjAccBFwHwzmxvEBgEjgBfMrB+wFOgVrHsDOB0oADYCfQHcfa2Z3QHMCrYb5u5ri5OQZdvNjipXbZZdCUuZa1C9drpTkAy0av1CK+kx2jXumPT3zYLVM0r8eeVNLQARkQR0LyARkZBKZRA4G6kAiIgkoJvBiYiElLqARERCSi0AEZGQUgtARCSkXIPAIiLhpFlAIiIhpYfCi4iEVLbdKSFVKgAiIgloFpCISEhpFpCISEipC0hEJKQ0C0hEJKQ0BiAiElLqAhIRCSldByAiElJqAYiIhJQGgUVEQkqDwCIiIaUuIBGRkNKVwCIiIaUWgIhISFX0AmAV/QQrMjPr7+556c5DMof+TUgqctKdgJRI/3QnIBlH/yYkaSoAIiIhpQIgIhJSKgDZTX29sjP9m5CkaRBYRCSk1AIQEQkpFQARkZBSARARCSldCZwlzOxgoAfQLAitACa5+8L0ZSUi2UwtgCxgZjcBEwADPg0WA54zs5vTmZtkLjPrm+4cJLNpFlAWMLOvgLbuvm2neFUg393bpCczyWRmtszd90t3HpK51AWUHaLAPsDSneJNg3USUmY2L9EqoHF55iLZRwUgO1wDTDWzxcDyILYfcABwZbqSkozQGDgNWLdT3ICPyz8dySYqAFnA3Seb2YHAMfxyEHiWu0fSl5lkgNeBmu4+d+cVZja93LORrKIxABGRkNIsIBGRkFIBEBEJKRUAEZGQUgEQEQkpFQARkZD6f8JHOtqelXsMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2batch = 0 of 55duraation = 0.9420462449391683\n",
      "epoch = 2batch = 20 of 55duraation = 2.9705753604571026\n",
      "epoch = 2batch = 40 of 55duraation = 5.86395263671875\n",
      "Epoch: 2, Train Loss: 0.68234228, Train f1: 0.38005611, Val Loss: 0.03549304, Val f1: 0.41086843, overrun_counter -1\n",
      "Saving model to: /dli/task/models/model_med2_2023_11_29_02_04_49.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "Current LR = 0.00000690\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.99      0.66     13006\n",
      "           1       0.92      0.09      0.16     14596\n",
      "\n",
      "    accuracy                           0.51     27602\n",
      "   macro avg       0.70      0.54      0.41     27602\n",
      "weighted avg       0.72      0.51      0.39     27602\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3batch = 0 of 55duraation = 0.9490363041559855\n",
      "epoch = 3batch = 20 of 55duraation = 2.962321762243907\n",
      "epoch = 3batch = 40 of 55duraation = 5.764342455069224\n",
      "..Overrun....no improvement\n",
      "Epoch: 3, Train Loss: 0.66769244, Train f1: 0.38458261, Val Loss: 0.03516945, Val f1: 0.39060553, overrun_counter 0\n",
      "epoch = 4batch = 0 of 55duraation = 1.0110384384791056\n",
      "epoch = 4batch = 20 of 55duraation = 3.079846390088399\n",
      "epoch = 4batch = 40 of 55duraation = 5.939183568954467\n",
      "..Overrun....no improvement\n",
      "Epoch: 4, Train Loss: 0.65755894, Train f1: 0.40179336, Val Loss: 0.03484491, Val f1: 0.40595491, overrun_counter 1\n",
      "epoch = 5batch = 0 of 55duraation = 0.9993768930435181\n",
      "epoch = 5batch = 20 of 55duraation = 3.0488924781481423\n",
      "epoch = 5batch = 40 of 55duraation = 7.682337693373362\n",
      "..Overrun....no improvement\n",
      "Epoch: 5, Train Loss: 0.65732776, Train f1: 0.40077601, Val Loss: 0.03506639, Val f1: 0.39574438, overrun_counter 2\n",
      "epoch = 6batch = 0 of 55duraation = 0.9446373701095581\n",
      "epoch = 6batch = 20 of 55duraation = 2.9935125430425007\n",
      "epoch = 6batch = 40 of 55duraation = 5.797600384553274\n",
      "Epoch: 6, Train Loss: 0.65071903, Train f1: 0.49500362, Val Loss: 0.03459235, Val f1: 0.53743909, overrun_counter -1\n",
      "Saving model to: /dli/task/models/model_med6_2023_11_29_02_41_21.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "Current LR = 0.00001497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.19      0.30     13006\n",
      "           1       0.56      0.90      0.69     14596\n",
      "\n",
      "    accuracy                           0.57     27602\n",
      "   macro avg       0.60      0.55      0.49     27602\n",
      "weighted avg       0.59      0.57      0.50     27602\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7batch = 0 of 55duraation = 0.9300821185111999\n",
      "epoch = 7batch = 20 of 55duraation = 3.04554469982783\n",
      "epoch = 7batch = 40 of 55duraation = 5.902831649780273\n",
      "..Overrun....no improvement\n",
      "Epoch: 7, Train Loss: 0.65727803, Train f1: 0.42564473, Val Loss: 0.03528313, Val f1: 0.39135488, overrun_counter 0\n",
      "epoch = 8batch = 0 of 55duraation = 0.8559835910797119\n",
      "epoch = 8batch = 20 of 55duraation = 2.9677175203959147\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_loader, val_loader,test_loader, model ,class_weights, classes = classes, num_epochs = args.num_epochs ,n_channels = 1\n",
    "#train_loader, val_loader,test_loader, model, classes ,df,num_epochs = args.num_epochs ,n_channels = 1\n",
    "tr_model, lr_log,all_train_f1,all_train_loss,all_val_loss,all_val_f1 = train_model(train_loader, val_loader, test_loader,model,class_weights ,num_epochs = num_epochs )\n",
    "\n",
    "print(\"ALL DONE!!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd757bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
